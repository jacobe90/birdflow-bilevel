{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "078856ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import jvp, grad, jacobian\n",
    "from jax.scipy.sparse.linalg import cg as conjugate_gradient\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c123d74",
   "metadata": {},
   "source": [
    "## Implicit Differentiation Toy Example\n",
    "\n",
    "$$ \\mathcal{L}^{in}(w, \\theta) = (w-\\theta^2)^2$$\n",
    "$$ \\mathcal{L}^{out}(w^*, \\theta) = 2 w^*$$\n",
    "Want to find $\\min_{\\theta} \\mathcal{L}^{out}(w^*, \\theta)$, where $w^* \\in \\text{argmin}_{w} \\mathcal{L}^{in}(w, \\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "022f759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_objective(w, theta):\n",
    "    return (w-theta**2)**2\n",
    "\n",
    "def outer_objective(w_opt):\n",
    "    return 2 * w_opt\n",
    "\n",
    "grad_outer = grad(outer_objective)\n",
    "grad_inner = grad(inner_objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e550619b",
   "metadata": {},
   "source": [
    "## Solve the inner problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e73a27d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_inner(theta, w_init):\n",
    "    optimizer = optax.adam(learning_rate=1e-2)\n",
    "    opt_state = optimizer.init(w_init)\n",
    "    w = w_init\n",
    "    for i in range(50):\n",
    "        grad_w = grad_inner(w, theta)\n",
    "        updates, opt_state = optimizer.update(grad_w, opt_state, w)\n",
    "        w = optax.apply_updates(w, updates)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f413b5e3",
   "metadata": {},
   "source": [
    "## Implicit Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccc84b2",
   "metadata": {},
   "source": [
    "$f(\\theta) = \\theta^2 = w^*$ (recall $\\mathcal{L}^{in}(w, \\theta) = (w-\\theta^2)^2$)\n",
    "\n",
    "$Af'(\\theta) = B$\n",
    "- $A = -\\left[\\partial^{2}_{w}\\mathcal{L}^{in}(w^*, \\theta)\\right]$\n",
    "\n",
    "- $B = \\partial_{\\theta w} \\mathcal{L}^{in}(w^*, \\theta)$\n",
    "\n",
    "\n",
    "Let $v = \\partial_{w}\\mathcal{L}^{out}(f(\\theta), \\theta)$, then $\\nabla_{\\theta} = v^T f'(\\theta)$. We directly compute $v^T f'(\\theta)$ as follows:\n",
    "- solve the linear system $Au = v$ for $u$ using conjugate gradient\n",
    "- compute $u^TB$ \n",
    "    - [$u^T B = u^T A f'(\\theta) = u^T A^T f'(\\theta) = v^T f'(\\theta)$]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "224f734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_outer(theta_init, n_steps=500, step_size=1e-3, learning_rate=1e-2):\n",
    "    theta = theta_init\n",
    "    optimizer = optax.adam(learning_rate)\n",
    "    opt_state = optimizer.init(theta)\n",
    "    w_init = 1.0\n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        # solve inner problem\n",
    "        w_star = solve_inner(theta, w_init)\n",
    "        w_init = w_star # get initial parameters for next iteration\n",
    "        \n",
    "        # solve for hyperparameter gradient with conjugate gradient\n",
    "        v = grad_outer(w_star)\n",
    "        B = jacobian(grad_inner, argnums=1)(w_star, theta)\n",
    "        \n",
    "        def matvec_A(u):\n",
    "            return -jvp(lambda w: grad_inner(w, theta), (w_star,), (u,))[1]\n",
    "        \n",
    "        u = conjugate_gradient(matvec_A, v.T)[0]\n",
    "        grad_theta = jnp.dot(u.T, B)\n",
    "  \n",
    "        # gradient descent with ADAM\n",
    "        print(f'Iteration {i} of {n_steps} theta: {theta}, grad theta: {grad_theta}')\n",
    "        updates, opt_state = optimizer.update(grad_theta, opt_state)\n",
    "        theta = optax.apply_updates(theta, updates)\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19f8e0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 of 100 theta: 1.0, grad theta: 4.0\n",
      "Iteration 1 of 100 theta: 0.9200005531311035, grad theta: 3.680002212524414\n",
      "Iteration 2 of 100 theta: 0.8402443528175354, grad theta: 3.3609774112701416\n",
      "Iteration 3 of 100 theta: 0.7609260082244873, grad theta: 3.043704032897949\n",
      "Iteration 4 of 100 theta: 0.6822706460952759, grad theta: 2.7290825843811035\n",
      "Iteration 5 of 100 theta: 0.604533314704895, grad theta: 2.41813325881958\n",
      "Iteration 6 of 100 theta: 0.5280033349990845, grad theta: 2.112013339996338\n",
      "Iteration 7 of 100 theta: 0.4530062675476074, grad theta: 1.8120250701904297\n",
      "Iteration 8 of 100 theta: 0.3799028992652893, grad theta: 1.5196115970611572\n",
      "Iteration 9 of 100 theta: 0.309088796377182, grad theta: 1.236355185508728\n",
      "Iteration 10 of 100 theta: 0.24098969995975494, grad theta: 0.9639587998390198\n",
      "Iteration 11 of 100 theta: 0.17605474591255188, grad theta: 0.7042189836502075\n",
      "Iteration 12 of 100 theta: 0.11474530398845673, grad theta: 0.4589812159538269\n",
      "Iteration 13 of 100 theta: 0.057520270347595215, grad theta: 0.23008108139038086\n",
      "Iteration 14 of 100 theta: 0.004818487912416458, grad theta: 0.019273951649665833\n",
      "Iteration 15 of 100 theta: -0.042961444705724716, grad theta: -0.17184577882289886\n",
      "Iteration 16 of 100 theta: -0.0854811817407608, grad theta: -0.3419247269630432\n",
      "Iteration 17 of 100 theta: -0.1224811002612114, grad theta: -0.4899244010448456\n",
      "Iteration 18 of 100 theta: -0.15379434823989868, grad theta: -0.6151773929595947\n",
      "Iteration 19 of 100 theta: -0.17935553193092346, grad theta: -0.7174221277236938\n",
      "Iteration 20 of 100 theta: -0.19920291006565094, grad theta: -0.7968116402626038\n",
      "Iteration 21 of 100 theta: -0.21347443759441376, grad theta: -0.853897750377655\n",
      "Iteration 22 of 100 theta: -0.22239898145198822, grad theta: -0.8895959258079529\n",
      "Iteration 23 of 100 theta: -0.22628414630889893, grad theta: -0.9051365852355957\n",
      "Iteration 24 of 100 theta: -0.22550247609615326, grad theta: -0.902009904384613\n",
      "Iteration 25 of 100 theta: -0.22047743201255798, grad theta: -0.8819097280502319\n",
      "Iteration 26 of 100 theta: -0.2116701751947403, grad theta: -0.8466807007789612\n",
      "Iteration 27 of 100 theta: -0.1995677649974823, grad theta: -0.7982710599899292\n",
      "Iteration 28 of 100 theta: -0.18467281758785248, grad theta: -0.7386912703514099\n",
      "Iteration 29 of 100 theta: -0.1674947440624237, grad theta: -0.6699789762496948\n",
      "Iteration 30 of 100 theta: -0.1485421359539032, grad theta: -0.5941685438156128\n",
      "Iteration 31 of 100 theta: -0.12831595540046692, grad theta: -0.5132638216018677\n",
      "Iteration 32 of 100 theta: -0.10730345547199249, grad theta: -0.42921382188796997\n",
      "Iteration 33 of 100 theta: -0.08597219735383987, grad theta: -0.3438887894153595\n",
      "Iteration 34 of 100 theta: -0.06476448476314545, grad theta: -0.2590579390525818\n",
      "Iteration 35 of 100 theta: -0.04409172385931015, grad theta: -0.1763668954372406\n",
      "Iteration 36 of 100 theta: -0.024329086765646935, grad theta: -0.09731634706258774\n",
      "Iteration 37 of 100 theta: -0.005810409784317017, grad theta: -0.023241639137268066\n",
      "Iteration 38 of 100 theta: 0.01117624156177044, grad theta: 0.04470496624708176\n",
      "Iteration 39 of 100 theta: 0.026392333209514618, grad theta: 0.10556933283805847\n",
      "Iteration 40 of 100 theta: 0.039651550352573395, grad theta: 0.15860620141029358\n",
      "Iteration 41 of 100 theta: 0.05082131177186966, grad theta: 0.20328524708747864\n",
      "Iteration 42 of 100 theta: 0.05982296168804169, grad theta: 0.23929184675216675\n",
      "Iteration 43 of 100 theta: 0.06663061678409576, grad theta: 0.26652246713638306\n",
      "Iteration 44 of 100 theta: 0.07126876711845398, grad theta: 0.2850750684738159\n",
      "Iteration 45 of 100 theta: 0.07380867004394531, grad theta: 0.29523468017578125\n",
      "Iteration 46 of 100 theta: 0.07436379790306091, grad theta: 0.29745519161224365\n",
      "Iteration 47 of 100 theta: 0.0730845257639885, grad theta: 0.292338103055954\n",
      "Iteration 48 of 100 theta: 0.07015224546194077, grad theta: 0.28060898184776306\n",
      "Iteration 49 of 100 theta: 0.06577315181493759, grad theta: 0.26309260725975037\n",
      "Iteration 50 of 100 theta: 0.060171838849782944, grad theta: 0.24068735539913177\n",
      "Iteration 51 of 100 theta: 0.053584929555654526, grad theta: 0.2143397182226181\n",
      "Iteration 52 of 100 theta: 0.04625481739640236, grad theta: 0.18501926958560944\n",
      "Iteration 53 of 100 theta: 0.03842364624142647, grad theta: 0.15369458496570587\n",
      "Iteration 54 of 100 theta: 0.030327674001455307, grad theta: 0.12131069600582123\n",
      "Iteration 55 of 100 theta: 0.022192027419805527, grad theta: 0.0887681096792221\n",
      "Iteration 56 of 100 theta: 0.014226039871573448, grad theta: 0.05690415948629379\n",
      "Iteration 57 of 100 theta: 0.006619175896048546, grad theta: 0.026476703584194183\n",
      "Iteration 58 of 100 theta: -0.0004623616114258766, grad theta: -0.0018494464457035065\n",
      "Iteration 59 of 100 theta: -0.0068782782182097435, grad theta: -0.027513112872838974\n",
      "Iteration 60 of 100 theta: -0.012516045942902565, grad theta: -0.05006418377161026\n",
      "Iteration 61 of 100 theta: -0.017291955649852753, grad theta: -0.06916782259941101\n",
      "Iteration 62 of 100 theta: -0.021151352673768997, grad theta: -0.08460541069507599\n",
      "Iteration 63 of 100 theta: -0.024068104103207588, grad theta: -0.09627241641283035\n",
      "Iteration 64 of 100 theta: -0.026043325662612915, grad theta: -0.10417330265045166\n",
      "Iteration 65 of 100 theta: -0.02710345759987831, grad theta: -0.10841383039951324\n",
      "Iteration 66 of 100 theta: -0.027297763153910637, grad theta: -0.10919105261564255\n",
      "Iteration 67 of 100 theta: -0.026695379987359047, grad theta: -0.10678151994943619\n",
      "Iteration 68 of 100 theta: -0.02538200467824936, grad theta: -0.10152801871299744\n",
      "Iteration 69 of 100 theta: -0.02345634251832962, grad theta: -0.09382537007331848\n",
      "Iteration 70 of 100 theta: -0.021026434376835823, grad theta: -0.08410573750734329\n",
      "Iteration 71 of 100 theta: -0.01820596493780613, grad theta: -0.07282385975122452\n",
      "Iteration 72 of 100 theta: -0.01511065661907196, grad theta: -0.06044262647628784\n",
      "Iteration 73 of 100 theta: -0.011854847893118858, grad theta: -0.04741939157247543\n",
      "Iteration 74 of 100 theta: -0.008548332378268242, grad theta: -0.03419332951307297\n",
      "Iteration 75 of 100 theta: -0.0052935341373085976, grad theta: -0.02117413654923439\n",
      "Iteration 76 of 100 theta: -0.0021830860059708357, grad theta: -0.008732344023883343\n",
      "Iteration 77 of 100 theta: 0.0007021557539701462, grad theta: 0.0028086230158805847\n",
      "Iteration 78 of 100 theta: 0.0032946153078228235, grad theta: 0.013178461231291294\n",
      "Iteration 79 of 100 theta: 0.005541013088077307, grad theta: 0.022164052352309227\n",
      "Iteration 80 of 100 theta: 0.0074028875678777695, grad theta: 0.029611550271511078\n",
      "Iteration 81 of 100 theta: 0.008856639266014099, grad theta: 0.035426557064056396\n",
      "Iteration 82 of 100 theta: 0.009893123060464859, grad theta: 0.039572492241859436\n",
      "Iteration 83 of 100 theta: 0.010516829788684845, grad theta: 0.04206731915473938\n",
      "Iteration 84 of 100 theta: 0.010744704864919186, grad theta: 0.04297881945967674\n",
      "Iteration 85 of 100 theta: 0.010604673996567726, grad theta: 0.042418695986270905\n",
      "Iteration 86 of 100 theta: 0.010133924894034863, grad theta: 0.04053569957613945\n",
      "Iteration 87 of 100 theta: 0.009377028793096542, grad theta: 0.03750811517238617\n",
      "Iteration 88 of 100 theta: 0.008383963257074356, grad theta: 0.033535853028297424\n",
      "Iteration 89 of 100 theta: 0.007208103779703379, grad theta: 0.028832415118813515\n",
      "Iteration 90 of 100 theta: 0.005904256831854582, grad theta: 0.023617027327418327\n",
      "Iteration 91 of 100 theta: 0.00452678557485342, grad theta: 0.01810714229941368\n",
      "Iteration 92 of 100 theta: 0.003127888310700655, grad theta: 0.01251155324280262\n",
      "Iteration 93 of 100 theta: 0.0017560686683282256, grad theta: 0.0070242746733129025\n",
      "Iteration 94 of 100 theta: 0.0004548415308818221, grad theta: 0.0018193661235272884\n",
      "Iteration 95 of 100 theta: -0.0007383078336715698, grad theta: -0.0029532313346862793\n",
      "Iteration 96 of 100 theta: -0.0017926868749782443, grad theta: -0.007170747499912977\n",
      "Iteration 97 of 100 theta: -0.0026848798152059317, grad theta: -0.010739519260823727\n",
      "Iteration 98 of 100 theta: -0.00339895230717957, grad theta: -0.01359580922871828\n",
      "Iteration 99 of 100 theta: -0.003926392644643784, grad theta: -0.015705570578575134\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(-0.0042658, dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solve_outer(jnp.array(1.0), n_steps=100, learning_rate=8e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6b3d97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
