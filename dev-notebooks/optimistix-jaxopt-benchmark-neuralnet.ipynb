{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "874f9c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import jax\n",
    "from jaxtyping import Array, Float, Int, PyTree  # https://github.com/google/jaxtyping\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import optimistix as optx\n",
    "import jaxopt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a236a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', version=1, as_frame=True)\n",
    "\n",
    "X = mnist.data.to_numpy().astype(np.float64)\n",
    "y = mnist.target.to_numpy().astype(np.int32)\n",
    "#y = pd.get_dummies(mnist.target).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01adcef3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_X = X[1000].reshape(28, 28)\n",
    "test_y = y[1000]\n",
    "plt.imshow(test_X, cmap='grey')\n",
    "plt.title(f\"{test_y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c45c30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b787cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTNet(eqx.Module): # ~10^4 parameters\n",
    "    layers: list\n",
    "    \n",
    "    def __init__(self, key):\n",
    "        key1, key2, key3, key4 = jax.random.split(key, 4)\n",
    "        self.layers = [\n",
    "            eqx.nn.Linear(784, 20, key=key1),\n",
    "            jax.nn.relu,\n",
    "            eqx.nn.Linear(20, 10, key=key3),\n",
    "            jax.nn.softmax,\n",
    "        ]\n",
    "    def __call__(self, x: Float[Array, \" 1 784\"]) -> Float[Array, \" 10\"]:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "03ca54be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y, pred_y):\n",
    "    pred_y = jnp.take_along_axis(pred_y, jnp.expand_dims(y, 1), axis=1)\n",
    "    return -jnp.mean(pred_y)\n",
    "\n",
    "def l2reg(model):\n",
    "    total = 0\n",
    "    params = list(filter(lambda x: x != None, eqx.filter(model, eqx.is_array).layers)) # filter model to contain only the linear layers\n",
    "    for layer in params:\n",
    "        total += (jnp.sum(layer.weight ** 2) + jnp.sum(layer.bias ** 2))\n",
    "    return total\n",
    "\n",
    "def l2reg2(params):\n",
    "    params_list = list(filter(lambda x: x != None, params.layers))\n",
    "    total = 0\n",
    "    for layer in params_list:\n",
    "        total += (jnp.sum(layer.weight ** 2) + jnp.sum(layer.bias ** 2))\n",
    "    return total\n",
    "\n",
    "def loss(model, lam, X: Float[Array, \" batch 1 784\"], y: Int[Array, \" batch\"]):\n",
    "    pred_y = jax.vmap(model)(X)\n",
    "    return cross_entropy(y, pred_y) + lam * l2reg(model)\n",
    "\n",
    "def loss2(params, static, lam, X: Float[Array, \" batch 1 784\"], y: Int[Array, \" batch\"]):\n",
    "    model = eqx.combine(params, static)\n",
    "    return loss(model, lam, X, y)\n",
    "\n",
    "def val_loss2(params, static, X, y):\n",
    "    model = eqx.combine(params, static)\n",
    "    return val_loss(model, X, y)\n",
    "\n",
    "def val_loss(model, X, y):\n",
    "    pred_y = jax.vmap(model)(X)\n",
    "    return cross_entropy(y, pred_y)\n",
    "\n",
    "def loss_optx(params, args):\n",
    "    static, lam, X, y = args\n",
    "    model = eqx.combine(params, static)\n",
    "    return loss(model, lam, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f3304e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(arrays, batch_size):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = np.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = np.random.permutation(indices)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end <= dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size\n",
    "\n",
    "def train_model(lam,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    learning_rate=1e-4,\n",
    "    batch_size=100,\n",
    "    seed=42,\n",
    "    steps=30):           \n",
    "    \n",
    "    _, model_key = jax.random.split(jax.random.PRNGKey(seed), 2)\n",
    "    model = MNISTNet(model_key)\n",
    "    \n",
    "    optim = optax.adam(learning_rate)\n",
    "    \n",
    "    @eqx.filter_jit\n",
    "    def make_step(model, x, y, opt_state):\n",
    "        loss_val, grads = eqx.filter_value_and_grad(loss)(model, lam, x, y)\n",
    "        updates, opt_state = optim.update(grads, opt_state)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return loss_val, model, opt_state\n",
    "    \n",
    "    iter_data = dataloader((X_train, y_train), batch_size)\n",
    "    \n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_array))\n",
    "    for step, (x, y) in zip(range(steps), iter_data):\n",
    "        loss_val, model, opt_state = make_step(model, x, y, opt_state)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\"\"\"\n",
    "Using optimistix minimiser to train the model\n",
    "\"\"\"\n",
    "def train_model_optx_and_evaluate_acc(lam, args,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    learning_rate=1e-4,\n",
    "    batch_size=100,\n",
    "    seed=42,\n",
    "    steps=2):           \n",
    "    \n",
    "    _, model_key = jax.random.split(jax.random.PRNGKey(seed), 2)\n",
    "    model = MNISTNet(model_key)\n",
    "    params, static = eqx.partition(model, eqx.is_array)\n",
    "    solver = optx.OptaxMinimiser(optax.adam(learning_rate), rtol=1e-3, atol=1e-3)\n",
    "\n",
    "    sol = optx.minimise(loss_optx, solver, y0=params, args=(static, lam, X_train[:batch_size], y_train[:batch_size]), max_steps=steps)\n",
    "    params = sol.value\n",
    "    acc = model_acc2(params, static, X_test, y_test)\n",
    "    return lam * l2reg2(params)\n",
    "\n",
    "def test_accuracy(model, X, y):\n",
    "    y_pred = jax.vmap(model)(X)\n",
    "    y_pred = jnp.argmax(y_pred, axis=1)\n",
    "    return jnp.mean(y_pred == y)\n",
    "\n",
    "def test_accuracy2(params, static, X, y):\n",
    "    model = eqx.combine(params, static)\n",
    "    return test_accuracy(model, X, y)\n",
    "\n",
    "def train_and_evaluate_test_accuracy(lam, args):\n",
    "    params, static = train_model(lam, X_train=X_train,\n",
    "                        y_train=y_train,\n",
    "                        learning_rate=1e-4,\n",
    "                        batch_size=100,\n",
    "                        seed=42,\n",
    "                        steps=100,\n",
    "                   )\n",
    "    acc = test_accuracy2(params, static)\n",
    "    print(f\"acc = {acc}\")\n",
    "    return\n",
    "\n",
    "def train_using_optx_solver_and_evaluate_test_accuracy(lam, args):\n",
    "    params, static = train_model_optx(lam, X_train=X_train,\n",
    "                        y_train=y_train,\n",
    "                        learning_rate=1e-4,\n",
    "                        batch_size=100,\n",
    "                        seed=42,\n",
    "                        steps=1000,\n",
    "                   )\n",
    "    acc = test_accuracy2(params, static)\n",
    "    print(f\"acc = {acc}, type = {type(acc)}\")\n",
    "    return acc\n",
    "\n",
    "def train_and_evaluate_test_accuracy_jaxopt(params_init, static_init, lam, X_train, y_train, seed=42):\n",
    "    params_end = train_model_jaxopt(params_init, static_init, lam, X_train, y_train)\n",
    "    model = eqx.combine(params_end, static_init)\n",
    "    acc = test_accuracy(model)\n",
    "    print(acc)\n",
    "    return test_accuracy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3ed05f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use optax minimiser to train the model\n",
    "\"\"\"\n",
    "#@jaxopt.implicit_diff.custom_root(jax.grad(loss2))\n",
    "def train_model_optax_and_evaluate_acc(params_init, static_init, lam, X_train, y_train):\n",
    "    \n",
    "    learning_rate=1e-4\n",
    "    batch_size=100\n",
    "    seed=42\n",
    "    steps=100\n",
    "    \n",
    "    params = params_init\n",
    "    static = static_init\n",
    "    optim = optax.adam(learning_rate)\n",
    "    \n",
    "    @eqx.filter_jit\n",
    "    def make_step(params, x, y, opt_state):\n",
    "        loss_val, grads = jax.value_and_grad(loss2)(params, static, lam, x, y)\n",
    "        updates, opt_state = optim.update(grads, opt_state)\n",
    "        params = eqx.apply_updates(params, updates)\n",
    "        return loss_val, params, opt_state\n",
    "    \n",
    "    iter_data = dataloader((X_train, y_train), batch_size)\n",
    "    \n",
    "    opt_state = optim.init(params_init)\n",
    "    for step, (x, y) in zip(range(steps), iter_data):\n",
    "        loss_val, params, opt_state = make_step(params, x, y, opt_state)\n",
    "    \n",
    "    val = val_loss2(params, static, X_test, y_test)\n",
    "    return val\n",
    "\n",
    "\"\"\"\n",
    "Using optimistix minimiser to train the model\n",
    "\"\"\"\n",
    "def train_model_optx_and_compute_acc(lam, args,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,                                 \n",
    "    learning_rate=1e-4,\n",
    "    batch_size=10,\n",
    "    seed=42,\n",
    "    steps=1000):\n",
    "    \n",
    "    # initialize model\n",
    "    _, model_key = jax.random.split(jax.random.PRNGKey(seed), 2)\n",
    "    model = MNISTNet(model_key)\n",
    "    \n",
    "    params, static = eqx.partition(model, eqx.is_array)\n",
    "    \n",
    "    # initialize optax adam solver (with optimistix)\n",
    "    solver = optx.OptaxMinimiser(optax.adam(learning_rate), rtol=1e-3, atol=1e-3)\n",
    "    \n",
    "    # minimise loss to find the optimal params, compute model accuracy\n",
    "    # TODO - iterative solver\n",
    "    sol = optx.minimise(loss_optx, solver, y0=params, adjoint=optx.ImplicitAdjoint(), args=(static, lam, X_train[:batch_size], y_train[:batch_size]), max_steps=steps)\n",
    "    params = sol.value\n",
    "    val = val_loss2(params, static, X_test, y_test)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e141169c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(-0.29180443, dtype=float32),\n",
       " Array(-14.953585, dtype=float32, weak_type=True))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, model_key = jax.random.split(jax.random.PRNGKey(42), 2)\n",
    "model = MNISTNet(model_key)\n",
    "params_init, static_init = eqx.partition(model, eqx.is_array)\n",
    "acc1 = train_model_optax_and_evaluate_acc(params_init, static_init, 0.1, X_train, y_train)\n",
    "jax.value_and_grad(train_model_optax_and_evaluate_acc, argnums=2)(params_init, static_init, 0.1, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b6368509",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: -0.22727148234844208, y: 1.0\n",
      "Step: 1, Loss: -0.19062981009483337, y: 1.000100016593933\n",
      "Step: 2, Loss: -0.2192959487438202, y: 1.0001883506774902\n",
      "Step: 3, Loss: -0.24245917797088623, y: 1.000274896621704\n",
      "Step: 4, Loss: -0.2098533809185028, y: 1.0003430843353271\n",
      "Step: 5, Loss: -0.22246181964874268, y: 1.0004117488861084\n",
      "Step: 6, Loss: -0.228701651096344, y: 1.000480055809021\n",
      "Step: 7, Loss: -0.2302989512681961, y: 1.0005396604537964\n",
      "Step: 8, Loss: -0.2313893884420395, y: 1.0005924701690674\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/armenv/lib/python3.13/site-packages/equinox/_jit.py:248\u001b[0m, in \u001b[0;36m_JitWrapper._call\u001b[0;34m(self, is_lower, args, kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m jitting:\n\u001b[0;32m--> 248\u001b[0m         \u001b[43mmarker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock_until_ready\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JaxRuntimeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# Catch Equinox's runtime errors, and re-raise them with actually useful\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;66;03m# information. (By default XlaRuntimeError produces a lot of terrifying\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;66;03m# but useless information.)\u001b[39;00m\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: FAILED_PRECONDITION: Buffer Definition Event: CpuCallback error: Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/armenv/lib/python3.13/site-packages/jax/_src/interpreters/mlir.py\", line 2768, in _wrapped_callback\n  File \"/opt/anaconda3/envs/armenv/lib/python3.13/site-packages/jax/_src/callback.py\", line 269, in _callback\n  File \"/opt/anaconda3/envs/armenv/lib/python3.13/site-packages/jax/_src/callback.py\", line 97, in pure_callback_impl\n  File \"/opt/anaconda3/envs/armenv/lib/python3.13/site-packages/jax/_src/callback.py\", line 71, in __call__\n  File \"/opt/anaconda3/envs/armenv/lib/python3.13/site-packages/equinox/_errors.py\", line 89, in raises\n_EquinoxRuntimeError: The maximum number of steps was reached in the nonlinear solver. The problem may not be solveable (e.g., a root-find on a function that has no roots), or you may need to increase `max_steps`.\n\n\n--------------------\nAn error occurred during the runtime of your JAX program! Unfortunately you do not appear to be using `equinox.filter_jit` (perhaps you are using `jax.jit` instead?) and so further information about the error cannot be displayed. (Probably you are seeing a very large but uninformative error message right now.) Please wrap your program with `equinox.filter_jit`.\n--------------------\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# implicit diff on train_model_optx_and_compute_acc\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     sol \u001b[38;5;241m=\u001b[39m optx\u001b[38;5;241m.\u001b[39mminimise(train_model_optx_and_compute_acc, optim, adjoint\u001b[38;5;241m=\u001b[39moptx\u001b[38;5;241m.\u001b[39mImplicitAdjoint(), y0\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, args\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, max_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mfind_optimal_reg_implicit_optx\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[94], line 6\u001b[0m, in \u001b[0;36mfind_optimal_reg_implicit_optx\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m optim \u001b[38;5;241m=\u001b[39m optx\u001b[38;5;241m.\u001b[39mOptaxMinimiser(optax\u001b[38;5;241m.\u001b[39madam(\u001b[38;5;241m1e-4\u001b[39m), rtol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfrozenset\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m}))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# implicit diff on train_model_optx_and_compute_acc\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m sol \u001b[38;5;241m=\u001b[39m \u001b[43moptx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_model_optx_and_compute_acc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImplicitAdjoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/armenv/lib/python3.13/site-packages/equinox/_jit.py:256\u001b[0m, in \u001b[0;36m_JitWrapper._call\u001b[0;34m(self, is_lower, args, kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m         marker\u001b[38;5;241m.\u001b[39mblock_until_ready()\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JaxRuntimeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# Catch Equinox's runtime errors, and re-raise them with actually useful\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;66;03m# information. (By default XlaRuntimeError produces a lot of terrifying\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;66;03m# but useless information.)\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    254\u001b[0m         last_msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m last_stack \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_EquinoxRuntimeError: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     ):\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;66;03m# We check `last_msg` and `last_stack` just in case. I'm not sure\u001b[39;00m\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;66;03m# what happens in distributed/multiprocess environments. Is the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;66;03m# callback necessarily executed in the same interpreter as we are in\u001b[39;00m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;66;03m# here?\u001b[39;00m\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m EquinoxRuntimeError(\n\u001b[1;32m    263\u001b[0m             _on_error_msg\u001b[38;5;241m.\u001b[39mformat(msg\u001b[38;5;241m=\u001b[39mlast_msg, stack\u001b[38;5;241m=\u001b[39mlast_stack)\n\u001b[1;32m    264\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;66;03m# `from None` to hide the large but uninformative XlaRuntimeError.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def find_optimal_reg_implicit_optx():\n",
    "    # initialize adam optimisizer\n",
    "    optim = optx.OptaxMinimiser(optax.adam(1e-4), rtol=1e-4, atol=1e-4, verbose=frozenset({\"step\", \"y\", \"loss\"}))\n",
    "    \n",
    "    # implicit diff on train_model_optx_and_compute_acc\n",
    "    sol = optx.minimise(train_model_optx_and_compute_acc, optim, adjoint=optx.ImplicitAdjoint(), y0=1.0, args=0, max_steps=10)\n",
    "\n",
    "find_optimal_reg_implicit_optx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a0ef4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jax.grad(train_model_optx_test, argnums=0)(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a48cad0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training the model\n",
      "done training\n",
      "0.3347143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(0., dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, model_key = jax.random.split(jax.random.PRNGKey(42), 2)\n",
    "model = MNISTNet(model_key)\n",
    "params_init, static_init = eqx.partition(model, eqx.is_array)\n",
    "#jax.grad(loss2, argnums=2)(params_init, static_init, 1e-3, X_train, y_train)\n",
    "jax.grad(train_and_evaluate_test_accuracy_jaxopt, argnums=2)(params_init, static_init, 1e-3, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4c404a07",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m ys \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lam \u001b[38;5;129;01min\u001b[39;00m lams:\n\u001b[0;32m----> 4\u001b[0m     acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_test_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlam\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     ys\u001b[38;5;241m.\u001b[39mappend(acc)\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(lams, ys)\n",
      "Cell \u001b[0;32mIn[42], line 80\u001b[0m, in \u001b[0;36mtrain_and_evaluate_test_accuracy\u001b[0;34m(lam, args)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain_and_evaluate_test_accuracy\u001b[39m(lam, args):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m test_accuracy(\n\u001b[0;32m---> 80\u001b[0m         \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m                        \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m                        \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m                   \u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[42], line 69\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(lam, X_train, y_train, learning_rate, batch_size, seed, steps)\u001b[0m\n\u001b[1;32m     67\u001b[0m opt_state \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39minit(eqx\u001b[38;5;241m.\u001b[39mfilter(model, eqx\u001b[38;5;241m.\u001b[39mis_array))\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mrange\u001b[39m(steps), iter_data):\n\u001b[0;32m---> 69\u001b[0m     loss_val, model, opt_state \u001b[38;5;241m=\u001b[39m \u001b[43mmake_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "    \u001b[0;31m[... skipping hidden 4 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/armenv/lib/python3.13/site-packages/jax/_src/pjit.py:338\u001b[0m, in \u001b[0;36m_cpp_pjit.<locals>.cache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mno_tracing\u001b[38;5;241m.\u001b[39mvalue:\n\u001b[1;32m    336\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mre-tracing function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjit_info\u001b[38;5;241m.\u001b[39mfun_sourceinfo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    337\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`jit`, but \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno_tracing\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 338\u001b[0m outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked \u001b[38;5;241m=\u001b[39m \u001b[43m_python_pjit_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m executable \u001b[38;5;241m=\u001b[39m _read_most_recent_pjit_call_executable(jaxpr)\n\u001b[1;32m    341\u001b[0m pgle_profiler \u001b[38;5;241m=\u001b[39m _read_pgle_profiler(jaxpr)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/armenv/lib/python3.13/site-packages/jax/_src/pjit.py:188\u001b[0m, in \u001b[0;36m_python_pjit_helper\u001b[0;34m(fun, jit_info, *args, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m   args_flat \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39minit_states, \u001b[38;5;241m*\u001b[39margs_flat]\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 188\u001b[0m   out_flat \u001b[38;5;241m=\u001b[39m \u001b[43mpjit_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pxla\u001b[38;5;241m.\u001b[39mDeviceAssignmentMismatchError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    190\u001b[0m   fails, \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39margs\n",
      "File \u001b[0;32m/opt/anaconda3/envs/armenv/lib/python3.13/site-packages/jax/_src/core.py:2803\u001b[0m, in \u001b[0;36mAxisPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m   2799\u001b[0m axis_main \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m((axis_frame(a)\u001b[38;5;241m.\u001b[39mmain_trace \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m used_axis_names(\u001b[38;5;28mself\u001b[39m, params)),\n\u001b[1;32m   2800\u001b[0m                 default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[38;5;28mgetattr\u001b[39m(t, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   2801\u001b[0m top_trace \u001b[38;5;241m=\u001b[39m (top_trace \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m axis_main \u001b[38;5;129;01mor\u001b[39;00m axis_main\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m<\u001b[39m top_trace\u001b[38;5;241m.\u001b[39mlevel\n\u001b[1;32m   2802\u001b[0m              \u001b[38;5;28;01melse\u001b[39;00m axis_main\u001b[38;5;241m.\u001b[39mwith_cur_sublevel())\n\u001b[0;32m-> 2803\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/armenv/lib/python3.13/site-packages/jax/_src/core.py:442\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[1;32m    441\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m pop_level(trace\u001b[38;5;241m.\u001b[39mlevel):\n\u001b[0;32m--> 442\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_raise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/armenv/lib/python3.13/site-packages/jax/_src/core.py:955\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    953\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m call_impl_with_key_reuse_checks(primitive, primitive\u001b[38;5;241m.\u001b[39mimpl, \u001b[38;5;241m*\u001b[39mtracers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 955\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/armenv/lib/python3.13/site-packages/jax/_src/pjit.py:1738\u001b[0m, in \u001b[0;36m_pjit_call_impl\u001b[0;34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1729\u001b[0m donated_argnums \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(i \u001b[38;5;28;01mfor\u001b[39;00m i, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(donated_invars) \u001b[38;5;28;01mif\u001b[39;00m d)\n\u001b[1;32m   1730\u001b[0m cache_key \u001b[38;5;241m=\u001b[39m pxla\u001b[38;5;241m.\u001b[39mJitGlobalCppCacheKeys(\n\u001b[1;32m   1731\u001b[0m     donate_argnums\u001b[38;5;241m=\u001b[39mdonated_argnums, donate_argnames\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1732\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1736\u001b[0m     out_layouts_treedef\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out_layouts_leaves\u001b[38;5;241m=\u001b[39mout_layouts,\n\u001b[1;32m   1737\u001b[0m     use_resource_env\u001b[38;5;241m=\u001b[39mresource_env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1738\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_xla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpjit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall_impl_cache_miss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtree_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_registry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpxla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcc_shard_arg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_get_cpp_global_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_key\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontains_explicit_attributes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/armenv/lib/python3.13/site-packages/jax/_src/pjit.py:1714\u001b[0m, in \u001b[0;36m_pjit_call_impl.<locals>.call_impl_cache_miss\u001b[0;34m(*args_, **kwargs_)\u001b[0m\n\u001b[1;32m   1713\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_impl_cache_miss\u001b[39m(\u001b[38;5;241m*\u001b[39margs_, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_):\n\u001b[0;32m-> 1714\u001b[0m   out_flat, compiled \u001b[38;5;241m=\u001b[39m \u001b[43m_pjit_call_impl_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_shardings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[43m      \u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_layouts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_layouts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1717\u001b[0m \u001b[43m      \u001b[49m\u001b[43mout_layouts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_layouts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresource_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1718\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdonated_invars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1719\u001b[0m \u001b[43m      \u001b[49m\u001b[43minline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1720\u001b[0m   pgle_profiler \u001b[38;5;241m=\u001b[39m _read_pgle_profiler(jaxpr)\n\u001b[1;32m   1721\u001b[0m   fastpath_data \u001b[38;5;241m=\u001b[39m _get_fastpath_data(\n\u001b[1;32m   1722\u001b[0m       compiled, tree_structure(out_flat), args, out_flat, [], jaxpr\u001b[38;5;241m.\u001b[39meffects,\n\u001b[1;32m   1723\u001b[0m       jaxpr\u001b[38;5;241m.\u001b[39mconsts, \u001b[38;5;28;01mNone\u001b[39;00m, pgle_profiler)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/armenv/lib/python3.13/site-packages/jax/_src/pjit.py:1644\u001b[0m, in \u001b[0;36m_pjit_call_impl_python\u001b[0;34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1631\u001b[0m     compile_options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfdo_profile\u001b[39m\u001b[38;5;124m'\u001b[39m: fdo_profile}\n\u001b[1;32m   1633\u001b[0m \u001b[38;5;66;03m# TODO(patrios): Do not pass mutable profile session through cached lowering\u001b[39;00m\n\u001b[1;32m   1634\u001b[0m \u001b[38;5;66;03m# chain. Instead we need to move profilers dictionary to pxla module and use\u001b[39;00m\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;66;03m# module as key. Right now we can't do that since there is no way to evict _pjit_lower_cached cache for in PGLE mode.\u001b[39;00m\n\u001b[1;32m   1636\u001b[0m compiled \u001b[38;5;241m=\u001b[39m \u001b[43m_resolve_and_lower\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1637\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_shardings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_layouts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_layouts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1639\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_layouts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_layouts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresource_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1640\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdonated_invars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1641\u001b[0m \u001b[43m    \u001b[49m\u001b[43minline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlowering_platforms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlowering_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlir\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoweringParameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpgle_profiler\u001b[49m\n\u001b[0;32m-> 1644\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1646\u001b[0m _most_recent_pjit_call_executable\u001b[38;5;241m.\u001b[39mweak_key_dict[jaxpr] \u001b[38;5;241m=\u001b[39m compiled\n\u001b[1;32m   1647\u001b[0m \u001b[38;5;66;03m# This check is expensive so only do it if enable_checks is on.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/armenv/lib/python3.13/site-packages/jax/_src/interpreters/pxla.py:2345\u001b[0m, in \u001b[0;36mMeshComputation.compile\u001b[0;34m(self, compiler_options)\u001b[0m\n\u001b[1;32m   2343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompile\u001b[39m(\u001b[38;5;28mself\u001b[39m, compiler_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m MeshExecutable:\n\u001b[1;32m   2344\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m compiler_options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2345\u001b[0m     executable \u001b[38;5;241m=\u001b[39m \u001b[43mUnloadedMeshExecutable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_hlo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2346\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompiler_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompiler_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compiler_options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2349\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executable \u001b[38;5;241m=\u001b[39m executable\n",
      "File \u001b[0;32m/opt/anaconda3/envs/armenv/lib/python3.13/site-packages/jax/_src/interpreters/pxla.py:2854\u001b[0m, in \u001b[0;36mUnloadedMeshExecutable.from_hlo\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   2851\u001b[0m       mesh \u001b[38;5;241m=\u001b[39m i\u001b[38;5;241m.\u001b[39mmesh  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   2852\u001b[0m       \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 2854\u001b[0m xla_executable \u001b[38;5;241m=\u001b[39m \u001b[43m_cached_compilation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspmd_lowering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtuple_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_spmd_lowering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_prop_to_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_prop_to_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpmap_nreps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiler_options_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompiler_options_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2860\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m auto_spmd_lowering:\n\u001b[1;32m   2861\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m mesh \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/armenv/lib/python3.13/site-packages/jax/_src/interpreters/pxla.py:2666\u001b[0m, in \u001b[0;36m_cached_compilation\u001b[0;34m(computation, name, mesh, spmd_lowering, tuple_args, auto_spmd_lowering, allow_prop_to_inputs, allow_prop_to_outputs, host_callbacks, backend, da, pmap_nreps, compiler_options_keys, compiler_options_values, pgle_profiler)\u001b[0m\n\u001b[1;32m   2658\u001b[0m compile_options \u001b[38;5;241m=\u001b[39m create_compile_options(\n\u001b[1;32m   2659\u001b[0m     computation, mesh, spmd_lowering, tuple_args, auto_spmd_lowering,\n\u001b[1;32m   2660\u001b[0m     allow_prop_to_inputs, allow_prop_to_outputs, backend,\n\u001b[1;32m   2661\u001b[0m     dev, pmap_nreps, compiler_options)\n\u001b[1;32m   2663\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dispatch\u001b[38;5;241m.\u001b[39mlog_elapsed_time(\n\u001b[1;32m   2664\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished XLA compilation of \u001b[39m\u001b[38;5;132;01m{fun_name}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{elapsed_time:.9f}\u001b[39;00m\u001b[38;5;124m sec\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2665\u001b[0m     fun_name\u001b[38;5;241m=\u001b[39mname, event\u001b[38;5;241m=\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mBACKEND_COMPILE_EVENT):\n\u001b[0;32m-> 2666\u001b[0m   xla_executable \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_or_get_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2667\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2668\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2669\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xla_executable\n",
      "File \u001b[0;32m/opt/anaconda3/envs/armenv/lib/python3.13/site-packages/jax/_src/compiler.py:434\u001b[0m, in \u001b[0;36mcompile_or_get_cached\u001b[0;34m(backend, computation, devices, compile_options, host_callbacks, pgle_profiler)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m   log_persistent_cache_miss(module_name, cache_key)\n\u001b[0;32m--> 434\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_and_write_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m      \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/armenv/lib/python3.13/site-packages/jax/_src/compiler.py:662\u001b[0m, in \u001b[0;36m_compile_and_write_cache\u001b[0;34m(backend, computation, compile_options, host_callbacks, module_name, cache_key)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_compile_and_write_cache\u001b[39m(\n\u001b[1;32m    654\u001b[0m     backend: xc\u001b[38;5;241m.\u001b[39mClient,\n\u001b[1;32m    655\u001b[0m     computation: ir\u001b[38;5;241m.\u001b[39mModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    659\u001b[0m     cache_key: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    660\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m xc\u001b[38;5;241m.\u001b[39mLoadedExecutable:\n\u001b[1;32m    661\u001b[0m   start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 662\u001b[0m   executable \u001b[38;5;241m=\u001b[39m \u001b[43mbackend_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    665\u001b[0m   compile_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    666\u001b[0m   _cache_write(\n\u001b[1;32m    667\u001b[0m       cache_key, compile_time, module_name, backend, executable, host_callbacks\n\u001b[1;32m    668\u001b[0m   )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/armenv/lib/python3.13/site-packages/jax/_src/profiler.py:333\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    332\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m/opt/anaconda3/envs/armenv/lib/python3.13/site-packages/jax/_src/compiler.py:267\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m    262\u001b[0m         built_c, compile_options\u001b[38;5;241m=\u001b[39moptions, host_callbacks\u001b[38;5;241m=\u001b[39mhost_callbacks\n\u001b[1;32m    263\u001b[0m     )\n\u001b[1;32m    264\u001b[0m   \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    265\u001b[0m   \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    266\u001b[0m   \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m xc\u001b[38;5;241m.\u001b[39mXlaRuntimeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    269\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m error_handler \u001b[38;5;129;01min\u001b[39;00m _XLA_RUNTIME_ERROR_HANDLERS:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lams = jnp.arange(-5, -2, 0.1)\n",
    "ys = []\n",
    "for lam in lams:\n",
    "    acc = train_and_evaluate_test_accuracy(jnp.exp(lam), None)\n",
    "    ys.append(acc)\n",
    "plt.plot(lams, ys)\n",
    "plt.title(\"accuracy on MNIST\")\n",
    "plt.xlabel(\"log reg weight\")\n",
    "plt.ylabel(\"model accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb61638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_jit\n",
    "def find_optimal_reg_implicit_optx(learning_rate=1e-4,\n",
    "                                   reg_init = 0):\n",
    "    optim = optx.OptaxMinimiser(optax.adam(learning_rate), rtol=1e-4, atol=1e-4, verbose=frozenset({\"step\", \"loss\"}))\n",
    "    sol = optx.minimise(train_and_evaluate_test_accuracy, optim, adjoint=optx.ImplicitAdjoint(), y0=reg_init, args=0, max_steps=1)\n",
    "    return sol\n",
    "\n",
    "@eqx.filter_jit\n",
    "def find_optimal_reg_implicit_optx2(learning_rate=1e-4,\n",
    "                                   reg_init=0.1):\n",
    "    optim = optx.OptaxMinimiser(optax.adam(learning_rate), rtol=1e-4, atol=1e-4, verbose=frozenset({\"step\", \"y\", \"loss\"}))\n",
    "    sol = optx.minimise(train_using_optx_solver_and_evaluate_test_accuracy, optim, adjoint=optx.ImplicitAdjoint(), y0=reg_init, args=0, max_steps=10)\n",
    "    return sol\n",
    "\n",
    "def find_optimal_reg_implicit_jaxopt(): # todo\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f67548e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = 0.692642867565155, type = <class 'jaxlib.xla_extension.ArrayImpl'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(0.69264287, dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_using_optx_solver_and_evaluate_test_accuracy(0.1, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "965b48ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=3/0)>, type = <class 'jax._src.interpreters.partial_eval.DynamicJaxprTracer'>\n",
      "Step: 0, Loss: 0.692642867565155, y: 0.10000000149011612\n",
      "Step: 1, Loss: 0.692642867565155, y: 0.10000000149011612\n",
      "solving with optimistix solver: 2.745856761932373\n"
     ]
    }
   ],
   "source": [
    "# t1 = time.time()\n",
    "# find_optimal_reg_implicit_optx()\n",
    "# t2 = time.time() - t1\n",
    "# print(f\"solving with optax solver: {t2}\")\n",
    "\n",
    "t1 = time.time()\n",
    "find_optimal_reg_implicit_optx2()\n",
    "t2 = time.time() - t1\n",
    "print(f\"solving with optimistix solver: {t2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7979655",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(1e-4)\n",
    "print(test_accuracy(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f733b8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution(\n",
      "  value=MLP(\n",
      "    layers=(\n",
      "      Linear(\n",
      "        weight=f64[4,8],\n",
      "        bias=f64[4],\n",
      "        in_features=8,\n",
      "        out_features=4,\n",
      "        use_bias=True\n",
      "      ),\n",
      "      Linear(\n",
      "        weight=f64[4,4],\n",
      "        bias=f64[4],\n",
      "        in_features=4,\n",
      "        out_features=4,\n",
      "        use_bias=True\n",
      "      ),\n",
      "      Linear(\n",
      "        weight=f64[1,4],\n",
      "        bias=f64[1],\n",
      "        in_features=4,\n",
      "        out_features=1,\n",
      "        use_bias=True\n",
      "      )\n",
      "    ),\n",
      "    activation=None,\n",
      "    final_activation=None,\n",
      "    use_bias=True,\n",
      "    use_final_bias=True,\n",
      "    in_size=8,\n",
      "    out_size=1,\n",
      "    width_size=4,\n",
      "    depth=2\n",
      "  ),\n",
      "  result=EnumerationItem(\n",
      "    _value=i32[],\n",
      "    _enumeration=<class 'optimistix._solution.RESULTS'>\n",
      "  ),\n",
      "  aux=None,\n",
      "  stats={'max_steps': 16384, 'num_steps': weak_i64[]},\n",
      "  state=_BFGSState(\n",
      "    first_step=bool[],\n",
      "    y_eval=MLP(\n",
      "      layers=(\n",
      "        Linear(\n",
      "          weight=f64[4,8],\n",
      "          bias=f64[4],\n",
      "          in_features=8,\n",
      "          out_features=4,\n",
      "          use_bias=True\n",
      "        ),\n",
      "        Linear(\n",
      "          weight=f64[4,4],\n",
      "          bias=f64[4],\n",
      "          in_features=4,\n",
      "          out_features=4,\n",
      "          use_bias=True\n",
      "        ),\n",
      "        Linear(\n",
      "          weight=f64[1,4],\n",
      "          bias=f64[1],\n",
      "          in_features=4,\n",
      "          out_features=1,\n",
      "          use_bias=True\n",
      "        )\n",
      "      ),\n",
      "      activation=None,\n",
      "      final_activation=None,\n",
      "      use_bias=True,\n",
      "      use_final_bias=True,\n",
      "      in_size=8,\n",
      "      out_size=1,\n",
      "      width_size=4,\n",
      "      depth=2\n",
      "    ),\n",
      "    search_state=_BacktrackingState(step_size=weak_f64[]),\n",
      "    f_info=EvalGradHessianInv(\n",
      "      f=f64[],\n",
      "      grad=MLP(\n",
      "        layers=(\n",
      "          Linear(\n",
      "            weight=f64[4,8],\n",
      "            bias=f64[4],\n",
      "            in_features=8,\n",
      "            out_features=4,\n",
      "            use_bias=True\n",
      "          ),\n",
      "          Linear(\n",
      "            weight=f64[4,4],\n",
      "            bias=f64[4],\n",
      "            in_features=4,\n",
      "            out_features=4,\n",
      "            use_bias=True\n",
      "          ),\n",
      "          Linear(\n",
      "            weight=f64[1,4],\n",
      "            bias=f64[1],\n",
      "            in_features=4,\n",
      "            out_features=1,\n",
      "            use_bias=True\n",
      "          )\n",
      "        ),\n",
      "        activation=None,\n",
      "        final_activation=None,\n",
      "        use_bias=True,\n",
      "        use_final_bias=True,\n",
      "        in_size=8,\n",
      "        out_size=1,\n",
      "        width_size=4,\n",
      "        depth=2\n",
      "      ),\n",
      "      hessian_inv=PyTreeLinearOperator(\n",
      "        pytree=MLP(\n",
      "          layers=(\n",
      "            Linear(\n",
      "              weight=MLP(\n",
      "                layers=(\n",
      "                  Linear(\n",
      "                    weight=f64[4,8,4,8],\n",
      "                    bias=f64[4,8,4],\n",
      "                    in_features=8,\n",
      "                    out_features=4,\n",
      "                    use_bias=True\n",
      "                  ),\n",
      "                  Linear(\n",
      "                    weight=f64[4,8,4,4],\n",
      "                    bias=f64[4,8,4],\n",
      "                    in_features=4,\n",
      "                    out_features=4,\n",
      "                    use_bias=True\n",
      "                  ),\n",
      "                  Linear(\n",
      "                    weight=f64[4,8,1,4],\n",
      "                    bias=f64[4,8,1],\n",
      "                    in_features=4,\n",
      "                    out_features=1,\n",
      "                    use_bias=True\n",
      "                  )\n",
      "                ),\n",
      "                activation=None,\n",
      "                final_activation=None,\n",
      "                use_bias=True,\n",
      "                use_final_bias=True,\n",
      "                in_size=8,\n",
      "                out_size=1,\n",
      "                width_size=4,\n",
      "                depth=2\n",
      "              ),\n",
      "              bias=MLP(\n",
      "                layers=(\n",
      "                  Linear(\n",
      "                    weight=f64[4,4,8],\n",
      "                    bias=f64[4,4],\n",
      "                    in_features=8,\n",
      "                    out_features=4,\n",
      "                    use_bias=True\n",
      "                  ),\n",
      "                  Linear(\n",
      "                    weight=f64[4,4,4],\n",
      "                    bias=f64[4,4],\n",
      "                    in_features=4,\n",
      "                    out_features=4,\n",
      "                    use_bias=True\n",
      "                  ),\n",
      "                  Linear(\n",
      "                    weight=f64[4,1,4],\n",
      "                    bias=f64[4,1],\n",
      "                    in_features=4,\n",
      "                    out_features=1,\n",
      "                    use_bias=True\n",
      "                  )\n",
      "                ),\n",
      "                activation=None,\n",
      "                final_activation=None,\n",
      "                use_bias=True,\n",
      "                use_final_bias=True,\n",
      "                in_size=8,\n",
      "                out_size=1,\n",
      "                width_size=4,\n",
      "                depth=2\n",
      "              ),\n",
      "              in_features=8,\n",
      "              out_features=4,\n",
      "              use_bias=True\n",
      "            ),\n",
      "            Linear(\n",
      "              weight=MLP(\n",
      "                layers=(\n",
      "                  Linear(\n",
      "                    weight=f64[4,4,4,8],\n",
      "                    bias=f64[4,4,4],\n",
      "                    in_features=8,\n",
      "                    out_features=4,\n",
      "                    use_bias=True\n",
      "                  ),\n",
      "                  Linear(\n",
      "                    weight=f64[4,4,4,4],\n",
      "                    bias=f64[4,4,4],\n",
      "                    in_features=4,\n",
      "                    out_features=4,\n",
      "                    use_bias=True\n",
      "                  ),\n",
      "                  Linear(\n",
      "                    weight=f64[4,4,1,4],\n",
      "                    bias=f64[4,4,1],\n",
      "                    in_features=4,\n",
      "                    out_features=1,\n",
      "                    use_bias=True\n",
      "                  )\n",
      "                ),\n",
      "                activation=None,\n",
      "                final_activation=None,\n",
      "                use_bias=True,\n",
      "                use_final_bias=True,\n",
      "                in_size=8,\n",
      "                out_size=1,\n",
      "                width_size=4,\n",
      "                depth=2\n",
      "              ),\n",
      "              bias=MLP(\n",
      "                layers=(\n",
      "                  Linear(\n",
      "                    weight=f64[4,4,8],\n",
      "                    bias=f64[4,4],\n",
      "                    in_features=8,\n",
      "                    out_features=4,\n",
      "                    use_bias=True\n",
      "                  ),\n",
      "                  Linear(\n",
      "                    weight=f64[4,4,4],\n",
      "                    bias=f64[4,4],\n",
      "                    in_features=4,\n",
      "                    out_features=4,\n",
      "                    use_bias=True\n",
      "                  ),\n",
      "                  Linear(\n",
      "                    weight=f64[4,1,4],\n",
      "                    bias=f64[4,1],\n",
      "                    in_features=4,\n",
      "                    out_features=1,\n",
      "                    use_bias=True\n",
      "                  )\n",
      "                ),\n",
      "                activation=None,\n",
      "                final_activation=None,\n",
      "                use_bias=True,\n",
      "                use_final_bias=True,\n",
      "                in_size=8,\n",
      "                out_size=1,\n",
      "                width_size=4,\n",
      "                depth=2\n",
      "              ),\n",
      "              in_features=4,\n",
      "              out_features=4,\n",
      "              use_bias=True\n",
      "            ),\n",
      "            Linear(\n",
      "              weight=MLP(\n",
      "                layers=(\n",
      "                  Linear(\n",
      "                    weight=f64[1,4,4,8],\n",
      "                    bias=f64[1,4,4],\n",
      "                    in_features=8,\n",
      "                    out_features=4,\n",
      "                    use_bias=True\n",
      "                  ),\n",
      "                  Linear(\n",
      "                    weight=f64[1,4,4,4],\n",
      "                    bias=f64[1,4,4],\n",
      "                    in_features=4,\n",
      "                    out_features=4,\n",
      "                    use_bias=True\n",
      "                  ),\n",
      "                  Linear(\n",
      "                    weight=f64[1,4,1,4],\n",
      "                    bias=f64[1,4,1],\n",
      "                    in_features=4,\n",
      "                    out_features=1,\n",
      "                    use_bias=True\n",
      "                  )\n",
      "                ),\n",
      "                activation=None,\n",
      "                final_activation=None,\n",
      "                use_bias=True,\n",
      "                use_final_bias=True,\n",
      "                in_size=8,\n",
      "                out_size=1,\n",
      "                width_size=4,\n",
      "                depth=2\n",
      "              ),\n",
      "              bias=MLP(\n",
      "                layers=(\n",
      "                  Linear(\n",
      "                    weight=f64[1,4,8],\n",
      "                    bias=f64[1,4],\n",
      "                    in_features=8,\n",
      "                    out_features=4,\n",
      "                    use_bias=True\n",
      "                  ),\n",
      "                  Linear(\n",
      "                    weight=f64[1,4,4],\n",
      "                    bias=f64[1,4],\n",
      "                    in_features=4,\n",
      "                    out_features=4,\n",
      "                    use_bias=True\n",
      "                  ),\n",
      "                  Linear(\n",
      "                    weight=f64[1,1,4],\n",
      "                    bias=f64[1,1],\n",
      "                    in_features=4,\n",
      "                    out_features=1,\n",
      "                    use_bias=True\n",
      "                  )\n",
      "                ),\n",
      "                activation=None,\n",
      "                final_activation=None,\n",
      "                use_bias=True,\n",
      "                use_final_bias=True,\n",
      "                in_size=8,\n",
      "                out_size=1,\n",
      "                width_size=4,\n",
      "                depth=2\n",
      "              ),\n",
      "              in_features=4,\n",
      "              out_features=1,\n",
      "              use_bias=True\n",
      "            )\n",
      "          ),\n",
      "          activation=None,\n",
      "          final_activation=None,\n",
      "          use_bias=True,\n",
      "          use_final_bias=True,\n",
      "          in_size=8,\n",
      "          out_size=1,\n",
      "          width_size=4,\n",
      "          depth=2\n",
      "        ),\n",
      "        output_structure=(\n",
      "          [\n",
      "            ShapeDtypeStruct(shape=(4, 8), dtype=float64),\n",
      "            ShapeDtypeStruct(shape=(4,), dtype=float64),\n",
      "            ShapeDtypeStruct(shape=(4, 4), dtype=float64),\n",
      "            ShapeDtypeStruct(shape=(4,), dtype=float64),\n",
      "            ShapeDtypeStruct(shape=(1, 4), dtype=float64),\n",
      "            ShapeDtypeStruct(shape=(1,), dtype=float64)\n",
      "          ],\n",
      "          PyTreeDef(CustomNode(MLP[('layers', 'activation', 'final_activation'), ('use_bias', 'use_final_bias', 'in_size', 'out_size', 'width_size', 'depth'), (True, True, 8, 1, 4, 2)], [(CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (8, 4, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (4, 4, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (4, 1, True)], [*, *])), None, None]))\n",
      "        ),\n",
      "        tags=frozenset({positive_semidefinite_tag}),\n",
      "        input_structure=(\n",
      "          [\n",
      "            ShapeDtypeStruct(shape=(4, 8), dtype=float64),\n",
      "            ShapeDtypeStruct(shape=(4,), dtype=float64),\n",
      "            ShapeDtypeStruct(shape=(4, 4), dtype=float64),\n",
      "            ShapeDtypeStruct(shape=(4,), dtype=float64),\n",
      "            ShapeDtypeStruct(shape=(1, 4), dtype=float64),\n",
      "            ShapeDtypeStruct(shape=(1,), dtype=float64)\n",
      "          ],\n",
      "          PyTreeDef(CustomNode(MLP[('layers', 'activation', 'final_activation'), ('use_bias', 'use_final_bias', 'in_size', 'out_size', 'width_size', 'depth'), (True, True, 8, 1, 4, 2)], [(CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (8, 4, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (4, 4, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (4, 1, True)], [*, *])), None, None]))\n",
      "        )\n",
      "      )\n",
      "    ),\n",
      "    aux=None,\n",
      "    descent_state=_NewtonDescentState(\n",
      "      newton=MLP(\n",
      "        layers=(\n",
      "          Linear(\n",
      "            weight=f64[4,8],\n",
      "            bias=f64[4],\n",
      "            in_features=8,\n",
      "            out_features=4,\n",
      "            use_bias=True\n",
      "          ),\n",
      "          Linear(\n",
      "            weight=f64[4,4],\n",
      "            bias=f64[4],\n",
      "            in_features=4,\n",
      "            out_features=4,\n",
      "            use_bias=True\n",
      "          ),\n",
      "          Linear(\n",
      "            weight=f64[1,4],\n",
      "            bias=f64[1],\n",
      "            in_features=4,\n",
      "            out_features=1,\n",
      "            use_bias=True\n",
      "          )\n",
      "        ),\n",
      "        activation=None,\n",
      "        final_activation=None,\n",
      "        use_bias=True,\n",
      "        use_final_bias=True,\n",
      "        in_size=8,\n",
      "        out_size=1,\n",
      "        width_size=4,\n",
      "        depth=2\n",
      "      ),\n",
      "      result=EnumerationItem(\n",
      "        _value=i32[],\n",
      "        _enumeration=<class 'optimistix._solution.RESULTS'>\n",
      "      )\n",
      "    ),\n",
      "    terminate=bool[],\n",
      "    result=EnumerationItem(\n",
      "      _value=i32[],\n",
      "      _enumeration=<class 'optimistix._solution.RESULTS'>\n",
      "    ),\n",
      "    num_accepted_steps=weak_i64[]\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import optimistix as optx\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# Sample data\n",
    "X = jax.random.normal(jax.random.PRNGKey(0), (2000, 8))\n",
    "\n",
    "# Define the function\n",
    "@jax.vmap\n",
    "def function(x):\n",
    "    return x[0] + x[1] ** 2 + jnp.cos(x[2]) + jnp.sin(x[3]) + x[4] * x[5] + (x[6] * x[7]) ** 3\n",
    "\n",
    "# Compute the output for y\n",
    "y = function(X).reshape(-1, 1)  # This is a column vector with shape (2000, 1)\n",
    "\n",
    "# Define the MLP model\n",
    "model = eqx.nn.MLP(in_size=8, out_size=1, width_size=4, depth=2, activation=jax.nn.silu, key=jax.random.PRNGKey(0))\n",
    "\n",
    "# Partition the parameters of the model (params) and static (non-trainable) parts\n",
    "params, static = eqx.partition(model, eqx.is_array)\n",
    "\n",
    "# Loss function\n",
    "def loss_fn(params, static):\n",
    "    model = eqx.combine(params, static)\n",
    "    model_output = jax.vmap(model)(X)\n",
    "    return jnp.sum((model_output - y) ** 2)\n",
    "\n",
    "# Set up the solver\n",
    "solver = optx.BFGS(rtol=1e-5, atol=1e-5)\n",
    "\n",
    "# Minimize the loss function\n",
    "sol = optx.minimise(loss_fn, solver, params, args=static, max_steps=2**14)\n",
    "print(sol)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
