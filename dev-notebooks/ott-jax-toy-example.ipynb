{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "bd40acbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ott.solvers import linear\n",
    "from ott.geometry import pointcloud\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from jaxtyping import Array, Float\n",
    "from ott.solvers.linear.implicit_differentiation import ImplicitDiff\n",
    "import matplotlib.pyplot as plt\n",
    "import optax\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "e92f5fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 201 artists>"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize a gaussian kernel, mu_true, on domain X\n",
    "size = 100\n",
    "sigma = 20\n",
    "X = jnp.arange(-size, size + 1, 1).astype(float)\n",
    "mu_true = jnp.exp(-((X)**2) / (2 * sigma**2)) # gaussian kernel (sigma = 1, center = 0)\n",
    "mu_true /= jnp.sum(mu_true)\n",
    "plt.bar(X, mu_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "8d2c5fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "solve_fn = jax.jit(linear.solve)\n",
    "\n",
    "\"\"\"\n",
    "Compute regularized ot cost for transporting between marginals mu and mu_true on geometry x\n",
    "\"\"\"\n",
    "def ot_marginal_loss(mu: Float[Array, \"n \"], mu_true: Float[Array, \"n \"], x: Float[Array, \"n d\"]):\n",
    "    geom = pointcloud.PointCloud(x, x)\n",
    "    ot = solve_fn(geom, implicit_diff=ImplicitDiff(), a=mu, b=mu_true)\n",
    "    return ot.reg_ot_cost\n",
    "\n",
    "\"\"\"\n",
    "1-d gaussian kernel with center c, standard deviation sigma, domain\n",
    "\"\"\"\n",
    "def gaussian_kernel(c, sigma, X):\n",
    "    mu = jnp.exp(-((X-c)**2)/(2 * sigma**2))\n",
    "    mu /= jnp.sum(mu)\n",
    "    return mu\n",
    "\n",
    "\"\"\"\n",
    "learning the center of a gaussian marginal - doesn't work\n",
    "\"\"\"\n",
    "def loss(c: Array, mu_true: Float[Array, \"n \"], sigma: Float=5.0, X: Float[Array, \"n \"] = X):\n",
    "    # chatgpt-generated code\n",
    "    log_mu = -((X - c) ** 2) / (2 * sigma ** 2)  # Log-space Gaussian kernel\n",
    "    mu = jnp.exp(log_mu - jnp.logsumexp(log_mu))  # More numerically stable normalization\n",
    "    \n",
    "    return jnp.mean(mu)\n",
    "    #return ot_marginal_loss(mu, mu_true, X.reshape(X.shape[0], 1).astype(float)) # return cost to transport to mu_true\n",
    "\n",
    "\"\"\"\n",
    "loss function for logits of a probability distribution\n",
    "\"\"\"\n",
    "def logit_loss(theta: Float[Array, \"n\"], mu_true: Float[Array, \"n \"], X: Float[Array, \"n 1 \"] = X):\n",
    "    mu = jax.nn.softmax(theta)\n",
    "    return ot_marginal_loss(mu, mu_true, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "c5ccc7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "l2 distance between logits and probability distribution\n",
    "\"\"\"\n",
    "def logit_l2_loss(theta: Float[Array, \"n \"], marginal: Float[Array, \"n \"]):\n",
    "    mu = jax.nn.softmax(theta)\n",
    "    return jnp.sqrt(jnp.sum((mu - marginal)**2))\n",
    "\n",
    "\"\"\"\n",
    "learn the logits of a probability distribution\n",
    "\"\"\"\n",
    "def learn_marginal_logits(marginal: Float[Array, \"n \"], X: Float[Array, \"n \"], seed=42, learn_with_w2=False):\n",
    "    solver = optax.adam(learning_rate=0.03)\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    key, subkey = jax.random.split(key)\n",
    "    \n",
    "    theta = jax.random.normal(key=subkey, shape=X.shape[0])\n",
    "    opt_state = solver.init(theta)\n",
    "    \n",
    "    X_reshaped = X.reshape((X.shape[0], 1)).astype(float)\n",
    "    \n",
    "    if learn_with_w2: # learn using 2-wasserstein distance\n",
    "        loss_fn = functools.partial(logit_loss, mu_true=marginal, X=X_reshaped) # make logit loss a function of theta\n",
    "    else:\n",
    "        loss_fn = functools.partial(logit_l2_loss, marginal=marginal)\n",
    "    \n",
    "    @jax.jit\n",
    "    def make_step(theta, opt_state):\n",
    "        loss_val, grads = jax.value_and_grad(loss_fn)(theta)\n",
    "        updates, opt_state = solver.update(grads, opt_state)\n",
    "        theta = optax.apply_updates(theta, updates)\n",
    "        return loss_val, theta, opt_state\n",
    "    \n",
    "    for i in range(5000):\n",
    "        loss_val, theta, opt_state = make_step(theta, opt_state) # gradient update based on W2 distance\n",
    "        l2_loss_val = logit_l2_loss(theta, marginal)\n",
    "        print(f\"iteration {i}, loss value {loss_val}, l2 loss: {l2_loss_val}\")\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "2bfb5159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss value 2000.349609375, l2 loss: 0.12075093388557434\n",
      "iteration 1, loss value 1946.9454345703125, l2 loss: 0.11954902112483978\n",
      "iteration 2, loss value 1894.2677001953125, l2 loss: 0.11841891705989838\n",
      "iteration 3, loss value 1842.3936767578125, l2 loss: 0.11735985428094864\n",
      "iteration 4, loss value 1791.4071044921875, l2 loss: 0.1163673847913742\n",
      "iteration 5, loss value 1741.388916015625, l2 loss: 0.11543344706296921\n",
      "iteration 6, loss value 1692.4056396484375, l2 loss: 0.11454766243696213\n",
      "iteration 7, loss value 1644.509033203125, l2 loss: 0.11370165646076202\n",
      "iteration 8, loss value 1597.7357177734375, l2 loss: 0.11289335787296295\n",
      "iteration 9, loss value 1552.1171875, l2 loss: 0.11212658882141113\n",
      "iteration 10, loss value 1507.6837158203125, l2 loss: 0.11140749603509903\n",
      "iteration 11, loss value 1464.4664306640625, l2 loss: 0.11074153333902359\n",
      "iteration 12, loss value 1422.4932861328125, l2 loss: 0.11013220250606537\n",
      "iteration 13, loss value 1381.78955078125, l2 loss: 0.10958104580640793\n",
      "iteration 14, loss value 1342.37060546875, l2 loss: 0.1090879738330841\n",
      "iteration 15, loss value 1304.24755859375, l2 loss: 0.10865171253681183\n",
      "iteration 16, loss value 1267.424072265625, l2 loss: 0.10827010869979858\n",
      "iteration 17, loss value 1231.89697265625, l2 loss: 0.1079404205083847\n",
      "iteration 18, loss value 1197.658447265625, l2 loss: 0.10765963047742844\n",
      "iteration 19, loss value 1164.6962890625, l2 loss: 0.10742450505495071\n",
      "iteration 20, loss value 1132.99169921875, l2 loss: 0.10723183304071426\n",
      "iteration 21, loss value 1102.5220947265625, l2 loss: 0.10707847028970718\n",
      "iteration 22, loss value 1073.2623291015625, l2 loss: 0.10696138441562653\n",
      "iteration 23, loss value 1045.1826171875, l2 loss: 0.10687768459320068\n",
      "iteration 24, loss value 1018.2521362304688, l2 loss: 0.10682468116283417\n",
      "iteration 25, loss value 992.4382934570312, l2 loss: 0.10679981857538223\n",
      "iteration 26, loss value 967.7066040039062, l2 loss: 0.10680066049098969\n",
      "iteration 27, loss value 944.0217895507812, l2 loss: 0.10682490468025208\n",
      "iteration 28, loss value 921.3455810546875, l2 loss: 0.10687040537595749\n",
      "iteration 29, loss value 899.6425170898438, l2 loss: 0.10693509876728058\n",
      "iteration 30, loss value 878.8755493164062, l2 loss: 0.10701702535152435\n",
      "iteration 31, loss value 859.0066528320312, l2 loss: 0.10711439698934555\n",
      "iteration 32, loss value 840.0009155273438, l2 loss: 0.10722542554140091\n",
      "iteration 33, loss value 821.82080078125, l2 loss: 0.10734851658344269\n",
      "iteration 34, loss value 804.431884765625, l2 loss: 0.10748213529586792\n",
      "iteration 35, loss value 787.7993774414062, l2 loss: 0.10762481391429901\n",
      "iteration 36, loss value 771.8887939453125, l2 loss: 0.1077752485871315\n",
      "iteration 37, loss value 756.6693725585938, l2 loss: 0.10793213546276093\n",
      "iteration 38, loss value 742.109375, l2 loss: 0.10809427499771118\n",
      "iteration 39, loss value 728.1781005859375, l2 loss: 0.10826057940721512\n",
      "iteration 40, loss value 714.8470458984375, l2 loss: 0.10842994600534439\n",
      "iteration 41, loss value 702.0872802734375, l2 loss: 0.108601413667202\n",
      "iteration 42, loss value 689.8738403320312, l2 loss: 0.10877400636672974\n",
      "iteration 43, loss value 678.181396484375, l2 loss: 0.1089468002319336\n",
      "iteration 44, loss value 666.9838256835938, l2 loss: 0.10911901295185089\n",
      "iteration 45, loss value 656.2593994140625, l2 loss: 0.10928975045681\n",
      "iteration 46, loss value 645.985107421875, l2 loss: 0.10945823788642883\n",
      "iteration 47, loss value 636.1400146484375, l2 loss: 0.10962377488613129\n",
      "iteration 48, loss value 626.7039794921875, l2 loss: 0.10978565365076065\n",
      "iteration 49, loss value 617.6573486328125, l2 loss: 0.10994323343038559\n",
      "iteration 50, loss value 608.983154296875, l2 loss: 0.11009588837623596\n",
      "iteration 51, loss value 600.6622924804688, l2 loss: 0.1102430447936058\n",
      "iteration 52, loss value 592.67822265625, l2 loss: 0.11038417369127274\n",
      "iteration 53, loss value 585.015869140625, l2 loss: 0.11051881313323975\n",
      "iteration 54, loss value 577.6595458984375, l2 loss: 0.11064649373292923\n",
      "iteration 55, loss value 570.5949096679688, l2 loss: 0.11076685786247253\n",
      "iteration 56, loss value 563.80810546875, l2 loss: 0.1108795702457428\n",
      "iteration 57, loss value 557.2844848632812, l2 loss: 0.11098423600196838\n",
      "iteration 58, loss value 551.015869140625, l2 loss: 0.11108065396547318\n",
      "iteration 59, loss value 544.9878540039062, l2 loss: 0.11116858571767807\n",
      "iteration 60, loss value 539.189697265625, l2 loss: 0.11124790459871292\n",
      "iteration 61, loss value 533.6109008789062, l2 loss: 0.11131846904754639\n",
      "iteration 62, loss value 528.2399291992188, l2 loss: 0.1113802120089531\n",
      "iteration 63, loss value 523.0678100585938, l2 loss: 0.11143307387828827\n",
      "iteration 64, loss value 518.0850219726562, l2 loss: 0.11147704720497131\n",
      "iteration 65, loss value 513.2828369140625, l2 loss: 0.11151217669248581\n",
      "iteration 66, loss value 508.65350341796875, l2 loss: 0.11153850704431534\n",
      "iteration 67, loss value 504.187744140625, l2 loss: 0.11155610531568527\n",
      "iteration 68, loss value 499.87860107421875, l2 loss: 0.11156512796878815\n",
      "iteration 69, loss value 495.71954345703125, l2 loss: 0.11156564950942993\n",
      "iteration 70, loss value 491.70330810546875, l2 loss: 0.11155790090560913\n",
      "iteration 71, loss value 487.8236083984375, l2 loss: 0.1115420013666153\n",
      "iteration 72, loss value 484.0740051269531, l2 loss: 0.11151819676160812\n",
      "iteration 73, loss value 480.4488830566406, l2 loss: 0.11148667335510254\n",
      "iteration 74, loss value 476.94305419921875, l2 loss: 0.11144769936800003\n",
      "iteration 75, loss value 473.55120849609375, l2 loss: 0.11140146851539612\n",
      "iteration 76, loss value 470.2681579589844, l2 loss: 0.11134828627109528\n",
      "iteration 77, loss value 467.08941650390625, l2 loss: 0.1112884134054184\n",
      "iteration 78, loss value 464.0113220214844, l2 loss: 0.11122207343578339\n",
      "iteration 79, loss value 461.0280456542969, l2 loss: 0.11114964634180069\n",
      "iteration 80, loss value 458.1368408203125, l2 loss: 0.11107134073972702\n",
      "iteration 81, loss value 455.3334045410156, l2 loss: 0.11098749935626984\n",
      "iteration 82, loss value 452.6147155761719, l2 loss: 0.11089839041233063\n",
      "iteration 83, loss value 449.9765319824219, l2 loss: 0.11080433428287506\n",
      "iteration 84, loss value 447.4159240722656, l2 loss: 0.11070564389228821\n",
      "iteration 85, loss value 444.93060302734375, l2 loss: 0.11060260981321335\n",
      "iteration 86, loss value 442.516845703125, l2 loss: 0.11049552261829376\n",
      "iteration 87, loss value 440.1720886230469, l2 loss: 0.11038471013307571\n",
      "iteration 88, loss value 437.8939208984375, l2 loss: 0.11027046293020248\n",
      "iteration 89, loss value 435.6797180175781, l2 loss: 0.11015307158231735\n",
      "iteration 90, loss value 433.52655029296875, l2 loss: 0.11003284901380539\n",
      "iteration 91, loss value 431.43341064453125, l2 loss: 0.1099100410938263\n",
      "iteration 92, loss value 429.3964538574219, l2 loss: 0.10978499054908752\n",
      "iteration 93, loss value 427.41485595703125, l2 loss: 0.10965792089700699\n",
      "iteration 94, loss value 425.4866638183594, l2 loss: 0.10952914506196976\n",
      "iteration 95, loss value 423.6092529296875, l2 loss: 0.10939893126487732\n",
      "iteration 96, loss value 421.7820739746094, l2 loss: 0.10926750302314758\n",
      "iteration 97, loss value 420.0022277832031, l2 loss: 0.10913516581058502\n",
      "iteration 98, loss value 418.26873779296875, l2 loss: 0.10900213569402695\n",
      "iteration 99, loss value 416.58001708984375, l2 loss: 0.10886865109205246\n",
      "iteration 100, loss value 414.9338684082031, l2 loss: 0.10873496532440186\n",
      "iteration 101, loss value 413.3296813964844, l2 loss: 0.10860127955675125\n",
      "iteration 102, loss value 411.76641845703125, l2 loss: 0.10846783965826035\n",
      "iteration 103, loss value 410.24249267578125, l2 loss: 0.10833483189344406\n",
      "iteration 104, loss value 408.75592041015625, l2 loss: 0.10820245742797852\n",
      "iteration 105, loss value 407.30670166015625, l2 loss: 0.10807090252637863\n",
      "iteration 106, loss value 405.8926696777344, l2 loss: 0.10794038325548172\n",
      "iteration 107, loss value 404.5135192871094, l2 loss: 0.10781104862689972\n",
      "iteration 108, loss value 403.1681823730469, l2 loss: 0.10768307000398636\n",
      "iteration 109, loss value 401.854736328125, l2 loss: 0.10755660384893417\n",
      "iteration 110, loss value 400.5732116699219, l2 loss: 0.1074318066239357\n",
      "iteration 111, loss value 399.32281494140625, l2 loss: 0.10730879753828049\n",
      "iteration 112, loss value 398.10198974609375, l2 loss: 0.10718775540590286\n",
      "iteration 113, loss value 396.9096374511719, l2 loss: 0.10706878453493118\n",
      "iteration 114, loss value 395.74578857421875, l2 loss: 0.1069519892334938\n",
      "iteration 115, loss value 394.60931396484375, l2 loss: 0.10683749616146088\n",
      "iteration 116, loss value 393.4996643066406, l2 loss: 0.10672543197870255\n",
      "iteration 117, loss value 392.4157409667969, l2 loss: 0.1066158190369606\n",
      "iteration 118, loss value 391.3573303222656, l2 loss: 0.10650880634784698\n",
      "iteration 119, loss value 390.32330322265625, l2 loss: 0.10640446096658707\n",
      "iteration 120, loss value 389.31292724609375, l2 loss: 0.10630285739898682\n",
      "iteration 121, loss value 388.32586669921875, l2 loss: 0.10620405524969101\n",
      "iteration 122, loss value 387.36126708984375, l2 loss: 0.10610810667276382\n",
      "iteration 123, loss value 386.419189453125, l2 loss: 0.10601510107517242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 124, loss value 385.498779296875, l2 loss: 0.1059250459074974\n",
      "iteration 125, loss value 384.5989990234375, l2 loss: 0.10583799332380295\n",
      "iteration 126, loss value 383.7192687988281, l2 loss: 0.10575400292873383\n",
      "iteration 127, loss value 382.86004638671875, l2 loss: 0.10567308962345123\n",
      "iteration 128, loss value 382.0199279785156, l2 loss: 0.10559529066085815\n",
      "iteration 129, loss value 381.1991271972656, l2 loss: 0.1055205911397934\n",
      "iteration 130, loss value 380.3960876464844, l2 loss: 0.10544902086257935\n",
      "iteration 131, loss value 379.6112976074219, l2 loss: 0.10538060963153839\n",
      "iteration 132, loss value 378.8444519042969, l2 loss: 0.10531537979841232\n",
      "iteration 133, loss value 378.0948791503906, l2 loss: 0.10525327920913696\n",
      "iteration 134, loss value 377.36138916015625, l2 loss: 0.1051943302154541\n",
      "iteration 135, loss value 376.64453125, l2 loss: 0.10513855516910553\n",
      "iteration 136, loss value 375.94366455078125, l2 loss: 0.10508589446544647\n",
      "iteration 137, loss value 375.2581481933594, l2 loss: 0.10503636300563812\n",
      "iteration 138, loss value 374.5877685546875, l2 loss: 0.10498996824026108\n",
      "iteration 139, loss value 373.9327087402344, l2 loss: 0.10494664311408997\n",
      "iteration 140, loss value 373.29132080078125, l2 loss: 0.10490641742944717\n",
      "iteration 141, loss value 372.6645812988281, l2 loss: 0.10486923903226852\n",
      "iteration 142, loss value 372.0516357421875, l2 loss: 0.10483509302139282\n",
      "iteration 143, loss value 371.452392578125, l2 loss: 0.10480394959449768\n",
      "iteration 144, loss value 370.8659362792969, l2 loss: 0.10477577894926071\n",
      "iteration 145, loss value 370.29278564453125, l2 loss: 0.10475049912929535\n",
      "iteration 146, loss value 369.7313537597656, l2 loss: 0.10472816228866577\n",
      "iteration 147, loss value 369.18280029296875, l2 loss: 0.1047087013721466\n",
      "iteration 148, loss value 368.6463317871094, l2 loss: 0.10469204187393188\n",
      "iteration 149, loss value 368.1210021972656, l2 loss: 0.1046781986951828\n",
      "iteration 150, loss value 367.6072998046875, l2 loss: 0.10466710478067398\n",
      "iteration 151, loss value 367.1051940917969, l2 loss: 0.10465874522924423\n",
      "iteration 152, loss value 366.6141052246094, l2 loss: 0.10465303063392639\n",
      "iteration 153, loss value 366.1330261230469, l2 loss: 0.10464995354413986\n",
      "iteration 154, loss value 365.6627502441406, l2 loss: 0.10464946925640106\n",
      "iteration 155, loss value 365.20269775390625, l2 loss: 0.10465151816606522\n",
      "iteration 156, loss value 364.75238037109375, l2 loss: 0.10465609282255173\n",
      "iteration 157, loss value 364.31182861328125, l2 loss: 0.10466311872005463\n",
      "iteration 158, loss value 363.88116455078125, l2 loss: 0.10467255115509033\n",
      "iteration 159, loss value 363.4586181640625, l2 loss: 0.10468439757823944\n",
      "iteration 160, loss value 363.04730224609375, l2 loss: 0.10469852387905121\n",
      "iteration 161, loss value 362.64361572265625, l2 loss: 0.10471493005752563\n",
      "iteration 162, loss value 362.2482604980469, l2 loss: 0.10473362356424332\n",
      "iteration 163, loss value 361.8621520996094, l2 loss: 0.10475446283817291\n",
      "iteration 164, loss value 361.48394775390625, l2 loss: 0.10477746278047562\n",
      "iteration 165, loss value 361.1143493652344, l2 loss: 0.10480253398418427\n",
      "iteration 166, loss value 360.7522888183594, l2 loss: 0.10482964664697647\n",
      "iteration 167, loss value 360.3975524902344, l2 loss: 0.10485878586769104\n",
      "iteration 168, loss value 360.0504455566406, l2 loss: 0.1048898845911026\n",
      "iteration 169, loss value 359.7115478515625, l2 loss: 0.10492289811372757\n",
      "iteration 170, loss value 359.3789367675781, l2 loss: 0.10495780408382416\n",
      "iteration 171, loss value 359.054443359375, l2 loss: 0.1049945130944252\n",
      "iteration 172, loss value 358.7361755371094, l2 loss: 0.10503300279378891\n",
      "iteration 173, loss value 358.4248046875, l2 loss: 0.10507327318191528\n",
      "iteration 174, loss value 358.1204833984375, l2 loss: 0.10511519759893417\n",
      "iteration 175, loss value 357.8222351074219, l2 loss: 0.10515882074832916\n",
      "iteration 176, loss value 357.5301208496094, l2 loss: 0.1052040085196495\n",
      "iteration 177, loss value 357.24462890625, l2 loss: 0.10525079816579819\n",
      "iteration 178, loss value 356.9648742675781, l2 loss: 0.10529913008213043\n",
      "iteration 179, loss value 356.69146728515625, l2 loss: 0.10534889250993729\n",
      "iteration 180, loss value 356.42364501953125, l2 loss: 0.10540014505386353\n",
      "iteration 181, loss value 356.16094970703125, l2 loss: 0.10545279830694199\n",
      "iteration 182, loss value 355.9043884277344, l2 loss: 0.1055067628622055\n",
      "iteration 183, loss value 355.6533508300781, l2 loss: 0.10556210577487946\n",
      "iteration 184, loss value 355.4076843261719, l2 loss: 0.1056186854839325\n",
      "iteration 185, loss value 355.1666259765625, l2 loss: 0.10567651689052582\n",
      "iteration 186, loss value 354.9309387207031, l2 loss: 0.10573554784059525\n",
      "iteration 187, loss value 354.7001647949219, l2 loss: 0.10579574108123779\n",
      "iteration 188, loss value 354.4742736816406, l2 loss: 0.10585705190896988\n",
      "iteration 189, loss value 354.2532653808594, l2 loss: 0.10591947287321091\n",
      "iteration 190, loss value 354.0365905761719, l2 loss: 0.10598292946815491\n",
      "iteration 191, loss value 353.82464599609375, l2 loss: 0.10604739189147949\n",
      "iteration 192, loss value 353.6174011230469, l2 loss: 0.10611280798912048\n",
      "iteration 193, loss value 353.4137878417969, l2 loss: 0.10617919266223907\n",
      "iteration 194, loss value 353.2154235839844, l2 loss: 0.1062464639544487\n",
      "iteration 195, loss value 353.0201110839844, l2 loss: 0.10631461441516876\n",
      "iteration 196, loss value 352.8296813964844, l2 loss: 0.10638356953859329\n",
      "iteration 197, loss value 352.64263916015625, l2 loss: 0.10645334422588348\n",
      "iteration 198, loss value 352.46014404296875, l2 loss: 0.10652387887239456\n",
      "iteration 199, loss value 352.28082275390625, l2 loss: 0.10659512877464294\n",
      "iteration 200, loss value 352.1058349609375, l2 loss: 0.10666704922914505\n",
      "iteration 201, loss value 351.9338684082031, l2 loss: 0.10673965513706207\n",
      "iteration 202, loss value 351.7655944824219, l2 loss: 0.10681288689374924\n",
      "iteration 203, loss value 351.6012878417969, l2 loss: 0.10688670724630356\n",
      "iteration 204, loss value 351.4393310546875, l2 loss: 0.10696108639240265\n",
      "iteration 205, loss value 351.28179931640625, l2 loss: 0.10703600943088531\n",
      "iteration 206, loss value 351.127197265625, l2 loss: 0.10711142420768738\n",
      "iteration 207, loss value 350.97528076171875, l2 loss: 0.10718728601932526\n",
      "iteration 208, loss value 350.8268737792969, l2 loss: 0.10726363211870193\n",
      "iteration 209, loss value 350.68194580078125, l2 loss: 0.10734031349420547\n",
      "iteration 210, loss value 350.5390319824219, l2 loss: 0.10741743445396423\n",
      "iteration 211, loss value 350.4000244140625, l2 loss: 0.10749488323926926\n",
      "iteration 212, loss value 350.2635192871094, l2 loss: 0.10757263004779816\n",
      "iteration 213, loss value 350.1292419433594, l2 loss: 0.10765068978071213\n",
      "iteration 214, loss value 349.9980163574219, l2 loss: 0.10772901028394699\n",
      "iteration 215, loss value 349.86956787109375, l2 loss: 0.10780755430459976\n",
      "iteration 216, loss value 349.7441711425781, l2 loss: 0.10788632184267044\n",
      "iteration 217, loss value 349.62017822265625, l2 loss: 0.10796527564525604\n",
      "iteration 218, loss value 349.4997253417969, l2 loss: 0.10804437845945358\n",
      "iteration 219, loss value 349.38177490234375, l2 loss: 0.10812357813119888\n",
      "iteration 220, loss value 349.26556396484375, l2 loss: 0.10820288956165314\n",
      "iteration 221, loss value 349.15167236328125, l2 loss: 0.10828230530023575\n",
      "iteration 222, loss value 349.0406799316406, l2 loss: 0.10836177319288254\n",
      "iteration 223, loss value 348.9315490722656, l2 loss: 0.10844123363494873\n",
      "iteration 224, loss value 348.82403564453125, l2 loss: 0.10852069407701492\n",
      "iteration 225, loss value 348.7190246582031, l2 loss: 0.10860016196966171\n",
      "iteration 226, loss value 348.61602783203125, l2 loss: 0.10867961496114731\n",
      "iteration 227, loss value 348.51617431640625, l2 loss: 0.10875895619392395\n",
      "iteration 228, loss value 348.417236328125, l2 loss: 0.10883820801973343\n",
      "iteration 229, loss value 348.3205261230469, l2 loss: 0.10891736298799515\n",
      "iteration 230, loss value 348.225341796875, l2 loss: 0.1089964285492897\n",
      "iteration 231, loss value 348.13311767578125, l2 loss: 0.10907530784606934\n",
      "iteration 232, loss value 348.0415954589844, l2 loss: 0.10915398597717285\n",
      "iteration 233, loss value 347.95220947265625, l2 loss: 0.10923250764608383\n",
      "iteration 234, loss value 347.8646240234375, l2 loss: 0.10931079834699631\n",
      "iteration 235, loss value 347.778564453125, l2 loss: 0.10938888788223267\n",
      "iteration 236, loss value 347.6944274902344, l2 loss: 0.10946670174598694\n",
      "iteration 237, loss value 347.6116943359375, l2 loss: 0.10954426229000092\n",
      "iteration 238, loss value 347.5307312011719, l2 loss: 0.10962152481079102\n",
      "iteration 239, loss value 347.4507751464844, l2 loss: 0.10969851911067963\n",
      "iteration 240, loss value 347.37359619140625, l2 loss: 0.10977515578269958\n",
      "iteration 241, loss value 347.296875, l2 loss: 0.10985149443149567\n",
      "iteration 242, loss value 347.2228088378906, l2 loss: 0.10992742329835892\n",
      "iteration 243, loss value 347.1487121582031, l2 loss: 0.1100030243396759\n",
      "iteration 244, loss value 347.0767517089844, l2 loss: 0.11007822304964066\n",
      "iteration 245, loss value 347.00579833984375, l2 loss: 0.11015304177999496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 246, loss value 346.93646240234375, l2 loss: 0.11022739857435226\n",
      "iteration 247, loss value 346.8678894042969, l2 loss: 0.11030140519142151\n",
      "iteration 248, loss value 346.80169677734375, l2 loss: 0.11037492752075195\n",
      "iteration 249, loss value 346.73583984375, l2 loss: 0.11044799536466599\n",
      "iteration 250, loss value 346.6713562011719, l2 loss: 0.1105206087231636\n",
      "iteration 251, loss value 346.6081237792969, l2 loss: 0.11059275269508362\n",
      "iteration 252, loss value 346.54638671875, l2 loss: 0.11066437512636185\n",
      "iteration 253, loss value 346.4851989746094, l2 loss: 0.11073553562164307\n",
      "iteration 254, loss value 346.4257507324219, l2 loss: 0.11080612987279892\n",
      "iteration 255, loss value 346.3666687011719, l2 loss: 0.11087625473737717\n",
      "iteration 256, loss value 346.30902099609375, l2 loss: 0.11094580590724945\n",
      "iteration 257, loss value 346.2525634765625, l2 loss: 0.11101483553647995\n",
      "iteration 258, loss value 346.1969909667969, l2 loss: 0.11108329147100449\n",
      "iteration 259, loss value 346.1424255371094, l2 loss: 0.11115118116140366\n",
      "iteration 260, loss value 346.0886535644531, l2 loss: 0.11121849715709686\n",
      "iteration 261, loss value 346.0359802246094, l2 loss: 0.1112852469086647\n",
      "iteration 262, loss value 345.98406982421875, l2 loss: 0.11135140061378479\n",
      "iteration 263, loss value 345.9333801269531, l2 loss: 0.11141695827245712\n",
      "iteration 264, loss value 345.88409423828125, l2 loss: 0.11148189753293991\n",
      "iteration 265, loss value 345.8346862792969, l2 loss: 0.11154622584581375\n",
      "iteration 266, loss value 345.7862548828125, l2 loss: 0.11160995066165924\n",
      "iteration 267, loss value 345.7392883300781, l2 loss: 0.11167305707931519\n",
      "iteration 268, loss value 345.69219970703125, l2 loss: 0.1117355152964592\n",
      "iteration 269, loss value 345.64630126953125, l2 loss: 0.11179733276367188\n",
      "iteration 270, loss value 345.6017150878906, l2 loss: 0.11185852438211441\n",
      "iteration 271, loss value 345.55731201171875, l2 loss: 0.11191905289888382\n",
      "iteration 272, loss value 345.5139465332031, l2 loss: 0.11197894811630249\n",
      "iteration 273, loss value 345.4709167480469, l2 loss: 0.11203815788030624\n",
      "iteration 274, loss value 345.4288635253906, l2 loss: 0.11209669709205627\n",
      "iteration 275, loss value 345.3875427246094, l2 loss: 0.11215459555387497\n",
      "iteration 276, loss value 345.3465576171875, l2 loss: 0.11221184581518173\n",
      "iteration 277, loss value 345.3067932128906, l2 loss: 0.11226838082075119\n",
      "iteration 278, loss value 345.26739501953125, l2 loss: 0.11232425272464752\n",
      "iteration 279, loss value 345.228271484375, l2 loss: 0.11237946152687073\n",
      "iteration 280, loss value 345.1903991699219, l2 loss: 0.11243396997451782\n",
      "iteration 281, loss value 345.1525573730469, l2 loss: 0.1124878078699112\n",
      "iteration 282, loss value 345.11566162109375, l2 loss: 0.11254097521305084\n",
      "iteration 283, loss value 345.07940673828125, l2 loss: 0.11259343475103378\n",
      "iteration 284, loss value 345.0433654785156, l2 loss: 0.112645223736763\n",
      "iteration 285, loss value 345.0081787109375, l2 loss: 0.1126963198184967\n",
      "iteration 286, loss value 344.9737243652344, l2 loss: 0.1127467006444931\n",
      "iteration 287, loss value 344.9385986328125, l2 loss: 0.11279642581939697\n",
      "iteration 288, loss value 344.9053649902344, l2 loss: 0.11284543573856354\n",
      "iteration 289, loss value 344.8717956542969, l2 loss: 0.11289378255605698\n",
      "iteration 290, loss value 344.8389892578125, l2 loss: 0.11294141411781311\n",
      "iteration 291, loss value 344.80670166015625, l2 loss: 0.11298836767673492\n",
      "iteration 292, loss value 344.775146484375, l2 loss: 0.11303463578224182\n",
      "iteration 293, loss value 344.743408203125, l2 loss: 0.11308020353317261\n",
      "iteration 294, loss value 344.7128601074219, l2 loss: 0.11312508583068848\n",
      "iteration 295, loss value 344.6826477050781, l2 loss: 0.11316925287246704\n",
      "iteration 296, loss value 344.65228271484375, l2 loss: 0.11321278661489487\n",
      "iteration 297, loss value 344.6229248046875, l2 loss: 0.11325561255216599\n",
      "iteration 298, loss value 344.59332275390625, l2 loss: 0.11329776048660278\n",
      "iteration 299, loss value 344.5644226074219, l2 loss: 0.11333921551704407\n",
      "iteration 300, loss value 344.53631591796875, l2 loss: 0.11337999999523163\n",
      "iteration 301, loss value 344.5081787109375, l2 loss: 0.11342009156942368\n",
      "iteration 302, loss value 344.4803771972656, l2 loss: 0.113459512591362\n",
      "iteration 303, loss value 344.45343017578125, l2 loss: 0.1134982630610466\n",
      "iteration 304, loss value 344.42620849609375, l2 loss: 0.11353635787963867\n",
      "iteration 305, loss value 344.39947509765625, l2 loss: 0.11357375979423523\n",
      "iteration 306, loss value 344.3728332519531, l2 loss: 0.11361049115657806\n",
      "iteration 307, loss value 344.346923828125, l2 loss: 0.11364659667015076\n",
      "iteration 308, loss value 344.3213806152344, l2 loss: 0.11368203163146973\n",
      "iteration 309, loss value 344.2967224121094, l2 loss: 0.11371679604053497\n",
      "iteration 310, loss value 344.27130126953125, l2 loss: 0.11375091224908829\n",
      "iteration 311, loss value 344.2464904785156, l2 loss: 0.11378437280654907\n",
      "iteration 312, loss value 344.22247314453125, l2 loss: 0.11381717771291733\n",
      "iteration 313, loss value 344.197998046875, l2 loss: 0.11384939402341843\n",
      "iteration 314, loss value 344.17431640625, l2 loss: 0.1138809323310852\n",
      "iteration 315, loss value 344.1507568359375, l2 loss: 0.11391188204288483\n",
      "iteration 316, loss value 344.1275634765625, l2 loss: 0.11394215375185013\n",
      "iteration 317, loss value 344.1037902832031, l2 loss: 0.11397180706262589\n",
      "iteration 318, loss value 344.0813293457031, l2 loss: 0.11400085687637329\n",
      "iteration 319, loss value 344.05865478515625, l2 loss: 0.11402928829193115\n",
      "iteration 320, loss value 344.03662109375, l2 loss: 0.11405709385871887\n",
      "iteration 321, loss value 344.01446533203125, l2 loss: 0.11408431082963943\n",
      "iteration 322, loss value 343.99298095703125, l2 loss: 0.11411088705062866\n",
      "iteration 323, loss value 343.9714660644531, l2 loss: 0.11413691192865372\n",
      "iteration 324, loss value 343.9501953125, l2 loss: 0.11416233330965042\n",
      "iteration 325, loss value 343.9294128417969, l2 loss: 0.11418715119361877\n",
      "iteration 326, loss value 343.9081726074219, l2 loss: 0.11421142518520355\n",
      "iteration 327, loss value 343.8879089355469, l2 loss: 0.1142350435256958\n",
      "iteration 328, loss value 343.8661804199219, l2 loss: 0.11425814032554626\n",
      "iteration 329, loss value 343.8464050292969, l2 loss: 0.11428065598011017\n",
      "iteration 330, loss value 343.8260192871094, l2 loss: 0.1143026128411293\n",
      "iteration 331, loss value 343.8062744140625, l2 loss: 0.11432398855686188\n",
      "iteration 332, loss value 343.78668212890625, l2 loss: 0.11434484273195267\n",
      "iteration 333, loss value 343.76715087890625, l2 loss: 0.11436513066291809\n",
      "iteration 334, loss value 343.7474365234375, l2 loss: 0.11438488215208054\n",
      "iteration 335, loss value 343.7282409667969, l2 loss: 0.1144041046500206\n",
      "iteration 336, loss value 343.7096862792969, l2 loss: 0.11442281305789948\n",
      "iteration 337, loss value 343.691162109375, l2 loss: 0.1144409328699112\n",
      "iteration 338, loss value 343.672119140625, l2 loss: 0.1144586056470871\n",
      "iteration 339, loss value 343.65399169921875, l2 loss: 0.11447569727897644\n",
      "iteration 340, loss value 343.6349792480469, l2 loss: 0.11449231207370758\n",
      "iteration 341, loss value 343.61724853515625, l2 loss: 0.11450841277837753\n",
      "iteration 342, loss value 343.598876953125, l2 loss: 0.11452403664588928\n",
      "iteration 343, loss value 343.5811462402344, l2 loss: 0.11453912407159805\n",
      "iteration 344, loss value 343.5636901855469, l2 loss: 0.1145537719130516\n",
      "iteration 345, loss value 343.5461730957031, l2 loss: 0.11456791311502457\n",
      "iteration 346, loss value 343.52850341796875, l2 loss: 0.11458155512809753\n",
      "iteration 347, loss value 343.5108947753906, l2 loss: 0.11459475010633469\n",
      "iteration 348, loss value 343.4937438964844, l2 loss: 0.11460746079683304\n",
      "iteration 349, loss value 343.47698974609375, l2 loss: 0.11461973190307617\n",
      "iteration 350, loss value 343.46026611328125, l2 loss: 0.1146315485239029\n",
      "iteration 351, loss value 343.443359375, l2 loss: 0.11464285850524902\n",
      "iteration 352, loss value 343.4264221191406, l2 loss: 0.11465378105640411\n",
      "iteration 353, loss value 343.4102478027344, l2 loss: 0.11466428637504578\n",
      "iteration 354, loss value 343.393798828125, l2 loss: 0.11467430740594864\n",
      "iteration 355, loss value 343.3773193359375, l2 loss: 0.11468395590782166\n",
      "iteration 356, loss value 343.36114501953125, l2 loss: 0.11469312012195587\n",
      "iteration 357, loss value 343.3448486328125, l2 loss: 0.11470187455415726\n",
      "iteration 358, loss value 343.3289489746094, l2 loss: 0.1147102415561676\n",
      "iteration 359, loss value 343.31292724609375, l2 loss: 0.11471821367740631\n",
      "iteration 360, loss value 343.2978820800781, l2 loss: 0.114725761115551\n",
      "iteration 361, loss value 343.2820739746094, l2 loss: 0.11473289877176285\n",
      "iteration 362, loss value 343.2664489746094, l2 loss: 0.11473968625068665\n",
      "iteration 363, loss value 343.25115966796875, l2 loss: 0.11474607139825821\n",
      "iteration 364, loss value 343.23565673828125, l2 loss: 0.11475205421447754\n",
      "iteration 365, loss value 343.2206726074219, l2 loss: 0.11475770175457001\n",
      "iteration 366, loss value 343.2054748535156, l2 loss: 0.11476294696331024\n",
      "iteration 367, loss value 343.1903991699219, l2 loss: 0.11476784199476242\n",
      "iteration 368, loss value 343.1747741699219, l2 loss: 0.11477241665124893\n",
      "iteration 369, loss value 343.16021728515625, l2 loss: 0.1147766038775444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 370, loss value 343.1457214355469, l2 loss: 0.1147804707288742\n",
      "iteration 371, loss value 343.1309814453125, l2 loss: 0.11478397995233536\n",
      "iteration 372, loss value 343.11651611328125, l2 loss: 0.11478712409734726\n",
      "iteration 373, loss value 343.1015319824219, l2 loss: 0.11478996276855469\n",
      "iteration 374, loss value 343.0870056152344, l2 loss: 0.11479248851537704\n",
      "iteration 375, loss value 343.0729675292969, l2 loss: 0.11479468643665314\n",
      "iteration 376, loss value 343.0587158203125, l2 loss: 0.11479657888412476\n",
      "iteration 377, loss value 343.0447998046875, l2 loss: 0.1147981509566307\n",
      "iteration 378, loss value 343.0307312011719, l2 loss: 0.11479941755533218\n",
      "iteration 379, loss value 343.01654052734375, l2 loss: 0.11480037868022919\n",
      "iteration 380, loss value 343.00250244140625, l2 loss: 0.11480103433132172\n",
      "iteration 381, loss value 342.9883728027344, l2 loss: 0.11480142176151276\n",
      "iteration 382, loss value 342.97442626953125, l2 loss: 0.11480154097080231\n",
      "iteration 383, loss value 342.96112060546875, l2 loss: 0.11480133980512619\n",
      "iteration 384, loss value 342.9471130371094, l2 loss: 0.11480088531970978\n",
      "iteration 385, loss value 342.9336853027344, l2 loss: 0.11480014026165009\n",
      "iteration 386, loss value 342.91998291015625, l2 loss: 0.1147991418838501\n",
      "iteration 387, loss value 342.9065856933594, l2 loss: 0.11479788273572922\n",
      "iteration 388, loss value 342.89312744140625, l2 loss: 0.11479634791612625\n",
      "iteration 389, loss value 342.87982177734375, l2 loss: 0.11479459702968597\n",
      "iteration 390, loss value 342.86688232421875, l2 loss: 0.1147925853729248\n",
      "iteration 391, loss value 342.8536071777344, l2 loss: 0.11479031294584274\n",
      "iteration 392, loss value 342.8404235839844, l2 loss: 0.11478780955076218\n",
      "iteration 393, loss value 342.8271179199219, l2 loss: 0.1147850975394249\n",
      "iteration 394, loss value 342.8141784667969, l2 loss: 0.11478214710950851\n",
      "iteration 395, loss value 342.80181884765625, l2 loss: 0.11477895826101303\n",
      "iteration 396, loss value 342.78851318359375, l2 loss: 0.11477555334568024\n",
      "iteration 397, loss value 342.7756042480469, l2 loss: 0.11477189511060715\n",
      "iteration 398, loss value 342.762451171875, l2 loss: 0.11476805806159973\n",
      "iteration 399, loss value 342.75018310546875, l2 loss: 0.114764004945755\n",
      "iteration 400, loss value 342.73736572265625, l2 loss: 0.11475976556539536\n",
      "iteration 401, loss value 342.72491455078125, l2 loss: 0.11475532501935959\n",
      "iteration 402, loss value 342.71209716796875, l2 loss: 0.11475064605474472\n",
      "iteration 403, loss value 342.69915771484375, l2 loss: 0.11474583297967911\n",
      "iteration 404, loss value 342.6873474121094, l2 loss: 0.11474079638719559\n",
      "iteration 405, loss value 342.6748046875, l2 loss: 0.11473558843135834\n",
      "iteration 406, loss value 342.6622009277344, l2 loss: 0.11473019421100616\n",
      "iteration 407, loss value 342.6500549316406, l2 loss: 0.11472464352846146\n",
      "iteration 408, loss value 342.63800048828125, l2 loss: 0.11471889913082123\n",
      "iteration 409, loss value 342.6260070800781, l2 loss: 0.11471298336982727\n",
      "iteration 410, loss value 342.61346435546875, l2 loss: 0.11470691859722137\n",
      "iteration 411, loss value 342.6015319824219, l2 loss: 0.11470068246126175\n",
      "iteration 412, loss value 342.5890197753906, l2 loss: 0.11469430476427078\n",
      "iteration 413, loss value 342.57757568359375, l2 loss: 0.11468775570392609\n",
      "iteration 414, loss value 342.5656433105469, l2 loss: 0.11468106508255005\n",
      "iteration 415, loss value 342.55316162109375, l2 loss: 0.11467421799898148\n",
      "iteration 416, loss value 342.54150390625, l2 loss: 0.11466724425554276\n",
      "iteration 417, loss value 342.52996826171875, l2 loss: 0.11466013640165329\n",
      "iteration 418, loss value 342.5180358886719, l2 loss: 0.11465287953615189\n",
      "iteration 419, loss value 342.5065612792969, l2 loss: 0.11464547365903854\n",
      "iteration 420, loss value 342.4949035644531, l2 loss: 0.11463796347379684\n",
      "iteration 421, loss value 342.4830627441406, l2 loss: 0.11463029682636261\n",
      "iteration 422, loss value 342.4716491699219, l2 loss: 0.1146225556731224\n",
      "iteration 423, loss value 342.4598388671875, l2 loss: 0.11461468040943146\n",
      "iteration 424, loss value 342.4484558105469, l2 loss: 0.11460667103528976\n",
      "iteration 425, loss value 342.43695068359375, l2 loss: 0.11459855735301971\n",
      "iteration 426, loss value 342.4252624511719, l2 loss: 0.1145903617143631\n",
      "iteration 427, loss value 342.4143981933594, l2 loss: 0.11458199471235275\n",
      "iteration 428, loss value 342.4024353027344, l2 loss: 0.11457356810569763\n",
      "iteration 429, loss value 342.39166259765625, l2 loss: 0.11456505209207535\n",
      "iteration 430, loss value 342.3798828125, l2 loss: 0.11455642431974411\n",
      "iteration 431, loss value 342.3690490722656, l2 loss: 0.11454769223928452\n",
      "iteration 432, loss value 342.35784912109375, l2 loss: 0.11453890055418015\n",
      "iteration 433, loss value 342.3466491699219, l2 loss: 0.11452998965978622\n",
      "iteration 434, loss value 342.3353271484375, l2 loss: 0.11452097445726395\n",
      "iteration 435, loss value 342.3245849609375, l2 loss: 0.1145118847489357\n",
      "iteration 436, loss value 342.3135070800781, l2 loss: 0.11450271308422089\n",
      "iteration 437, loss value 342.3022155761719, l2 loss: 0.11449352651834488\n",
      "iteration 438, loss value 342.29205322265625, l2 loss: 0.11448419839143753\n",
      "iteration 439, loss value 342.28106689453125, l2 loss: 0.1144748330116272\n",
      "iteration 440, loss value 342.27056884765625, l2 loss: 0.11446534097194672\n",
      "iteration 441, loss value 342.25958251953125, l2 loss: 0.11445580422878265\n",
      "iteration 442, loss value 342.2482604980469, l2 loss: 0.1144462376832962\n",
      "iteration 443, loss value 342.23773193359375, l2 loss: 0.1144365593791008\n",
      "iteration 444, loss value 342.2265625, l2 loss: 0.1144268661737442\n",
      "iteration 445, loss value 342.2162780761719, l2 loss: 0.11441708356142044\n",
      "iteration 446, loss value 342.20623779296875, l2 loss: 0.11440723389387131\n",
      "iteration 447, loss value 342.19500732421875, l2 loss: 0.11439736187458038\n",
      "iteration 448, loss value 342.1849060058594, l2 loss: 0.11438741534948349\n",
      "iteration 449, loss value 342.1743469238281, l2 loss: 0.114377461373806\n",
      "iteration 450, loss value 342.16387939453125, l2 loss: 0.11436741054058075\n",
      "iteration 451, loss value 342.15374755859375, l2 loss: 0.11435732990503311\n",
      "iteration 452, loss value 342.14288330078125, l2 loss: 0.1143471971154213\n",
      "iteration 453, loss value 342.13262939453125, l2 loss: 0.11433704942464828\n",
      "iteration 454, loss value 342.12274169921875, l2 loss: 0.1143268346786499\n",
      "iteration 455, loss value 342.1117248535156, l2 loss: 0.11431661248207092\n",
      "iteration 456, loss value 342.1020812988281, l2 loss: 0.11430633068084717\n",
      "iteration 457, loss value 342.0911865234375, l2 loss: 0.11429604887962341\n",
      "iteration 458, loss value 342.08148193359375, l2 loss: 0.11428569257259369\n",
      "iteration 459, loss value 342.0715026855469, l2 loss: 0.11427532136440277\n",
      "iteration 460, loss value 342.0604248046875, l2 loss: 0.11426492780447006\n",
      "iteration 461, loss value 342.0508728027344, l2 loss: 0.11425450444221497\n",
      "iteration 462, loss value 342.04083251953125, l2 loss: 0.11424405872821808\n",
      "iteration 463, loss value 342.0308532714844, l2 loss: 0.1142336055636406\n",
      "iteration 464, loss value 342.02105712890625, l2 loss: 0.11422311514616013\n",
      "iteration 465, loss value 342.01104736328125, l2 loss: 0.11421262472867966\n",
      "iteration 466, loss value 342.0009765625, l2 loss: 0.1142020970582962\n",
      "iteration 467, loss value 341.99139404296875, l2 loss: 0.11419156938791275\n",
      "iteration 468, loss value 341.98114013671875, l2 loss: 0.1141810193657875\n",
      "iteration 469, loss value 341.9715576171875, l2 loss: 0.11417045444250107\n",
      "iteration 470, loss value 341.96209716796875, l2 loss: 0.11415988951921463\n",
      "iteration 471, loss value 341.9520568847656, l2 loss: 0.1141493022441864\n",
      "iteration 472, loss value 341.9423828125, l2 loss: 0.11413869261741638\n",
      "iteration 473, loss value 341.9325866699219, l2 loss: 0.11412806808948517\n",
      "iteration 474, loss value 341.92266845703125, l2 loss: 0.11411747336387634\n",
      "iteration 475, loss value 341.9132995605469, l2 loss: 0.11410690099000931\n",
      "iteration 476, loss value 341.9040222167969, l2 loss: 0.11409630626440048\n",
      "iteration 477, loss value 341.8941345214844, l2 loss: 0.11408565193414688\n",
      "iteration 478, loss value 341.8843688964844, l2 loss: 0.11407505720853806\n",
      "iteration 479, loss value 341.87493896484375, l2 loss: 0.11406447738409042\n",
      "iteration 480, loss value 341.86590576171875, l2 loss: 0.11405384540557861\n",
      "iteration 481, loss value 341.8559265136719, l2 loss: 0.11404327303171158\n",
      "iteration 482, loss value 341.8470153808594, l2 loss: 0.11403267830610275\n",
      "iteration 483, loss value 341.83740234375, l2 loss: 0.11402205377817154\n",
      "iteration 484, loss value 341.8279113769531, l2 loss: 0.1140115037560463\n",
      "iteration 485, loss value 341.8189697265625, l2 loss: 0.11400093883275986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 486, loss value 341.80938720703125, l2 loss: 0.11399037390947342\n",
      "iteration 487, loss value 341.80023193359375, l2 loss: 0.11397980898618698\n",
      "iteration 488, loss value 341.7913818359375, l2 loss: 0.11396925151348114\n",
      "iteration 489, loss value 341.7815856933594, l2 loss: 0.11395873129367828\n",
      "iteration 490, loss value 341.77264404296875, l2 loss: 0.11394823342561722\n",
      "iteration 491, loss value 341.7635498046875, l2 loss: 0.11393772065639496\n",
      "iteration 492, loss value 341.7544860839844, l2 loss: 0.11392723023891449\n",
      "iteration 493, loss value 341.7452087402344, l2 loss: 0.1139167994260788\n",
      "iteration 494, loss value 341.736328125, l2 loss: 0.1139063611626625\n",
      "iteration 495, loss value 341.7273864746094, l2 loss: 0.11389593034982681\n",
      "iteration 496, loss value 341.71832275390625, l2 loss: 0.11388552188873291\n",
      "iteration 497, loss value 341.7095642089844, l2 loss: 0.11387515813112259\n",
      "iteration 498, loss value 341.7004089355469, l2 loss: 0.11386478692293167\n",
      "iteration 499, loss value 341.6915283203125, l2 loss: 0.11385443806648254\n",
      "iteration 500, loss value 341.6827087402344, l2 loss: 0.113844133913517\n",
      "iteration 501, loss value 341.67401123046875, l2 loss: 0.11383381485939026\n",
      "iteration 502, loss value 341.66461181640625, l2 loss: 0.11382358521223068\n",
      "iteration 503, loss value 341.6563415527344, l2 loss: 0.11381334811449051\n",
      "iteration 504, loss value 341.64764404296875, l2 loss: 0.11380314081907272\n",
      "iteration 505, loss value 341.63909912109375, l2 loss: 0.11379293352365494\n",
      "iteration 506, loss value 341.6299133300781, l2 loss: 0.11378281563520432\n",
      "iteration 507, loss value 341.62164306640625, l2 loss: 0.1137726679444313\n",
      "iteration 508, loss value 341.6130065917969, l2 loss: 0.11376256495714188\n",
      "iteration 509, loss value 341.6040954589844, l2 loss: 0.11375249177217484\n",
      "iteration 510, loss value 341.5955505371094, l2 loss: 0.11374244838953018\n",
      "iteration 511, loss value 341.5870361328125, l2 loss: 0.11373242735862732\n",
      "iteration 512, loss value 341.5784606933594, l2 loss: 0.11372247338294983\n",
      "iteration 513, loss value 341.5700378417969, l2 loss: 0.11371251940727234\n",
      "iteration 514, loss value 341.56207275390625, l2 loss: 0.11370261013507843\n",
      "iteration 515, loss value 341.5531005859375, l2 loss: 0.11369272321462631\n",
      "iteration 516, loss value 341.5444641113281, l2 loss: 0.11368288099765778\n",
      "iteration 517, loss value 341.5364685058594, l2 loss: 0.11367305368185043\n",
      "iteration 518, loss value 341.5278625488281, l2 loss: 0.11366329342126846\n",
      "iteration 519, loss value 341.519287109375, l2 loss: 0.11365357786417007\n",
      "iteration 520, loss value 341.5116882324219, l2 loss: 0.11364386230707169\n",
      "iteration 521, loss value 341.50323486328125, l2 loss: 0.11363417655229568\n",
      "iteration 522, loss value 341.49444580078125, l2 loss: 0.11362454295158386\n",
      "iteration 523, loss value 341.48638916015625, l2 loss: 0.11361497640609741\n",
      "iteration 524, loss value 341.478515625, l2 loss: 0.11360538005828857\n",
      "iteration 525, loss value 341.469970703125, l2 loss: 0.1135958731174469\n",
      "iteration 526, loss value 341.4617614746094, l2 loss: 0.11358639597892761\n",
      "iteration 527, loss value 341.4535827636719, l2 loss: 0.11357695609331131\n",
      "iteration 528, loss value 341.4457702636719, l2 loss: 0.1135675460100174\n",
      "iteration 529, loss value 341.4376525878906, l2 loss: 0.11355818808078766\n",
      "iteration 530, loss value 341.429443359375, l2 loss: 0.11354885250329971\n",
      "iteration 531, loss value 341.4214172363281, l2 loss: 0.11353959143161774\n",
      "iteration 532, loss value 341.413818359375, l2 loss: 0.11353035271167755\n",
      "iteration 533, loss value 341.40594482421875, l2 loss: 0.11352115869522095\n",
      "iteration 534, loss value 341.3979797363281, l2 loss: 0.11351199448108673\n",
      "iteration 535, loss value 341.3901062011719, l2 loss: 0.1135028600692749\n",
      "iteration 536, loss value 341.38214111328125, l2 loss: 0.11349379271268845\n",
      "iteration 537, loss value 341.3740234375, l2 loss: 0.11348473280668259\n",
      "iteration 538, loss value 341.3659362792969, l2 loss: 0.1134757250547409\n",
      "iteration 539, loss value 341.3583679199219, l2 loss: 0.1134667843580246\n",
      "iteration 540, loss value 341.3501892089844, l2 loss: 0.11345789581537247\n",
      "iteration 541, loss value 341.3429870605469, l2 loss: 0.11344900727272034\n",
      "iteration 542, loss value 341.3348388671875, l2 loss: 0.11344018578529358\n",
      "iteration 543, loss value 341.326904296875, l2 loss: 0.1134314313530922\n",
      "iteration 544, loss value 341.31988525390625, l2 loss: 0.1134226992726326\n",
      "iteration 545, loss value 341.31231689453125, l2 loss: 0.1134139820933342\n",
      "iteration 546, loss value 341.3044128417969, l2 loss: 0.11340535432100296\n",
      "iteration 547, loss value 341.297119140625, l2 loss: 0.11339672654867172\n",
      "iteration 548, loss value 341.2890625, l2 loss: 0.11338817328214645\n",
      "iteration 549, loss value 341.28204345703125, l2 loss: 0.11337964981794357\n",
      "iteration 550, loss value 341.2737731933594, l2 loss: 0.11337118595838547\n",
      "iteration 551, loss value 341.26666259765625, l2 loss: 0.11336274445056915\n",
      "iteration 552, loss value 341.2591247558594, l2 loss: 0.11335436999797821\n",
      "iteration 553, loss value 341.2514343261719, l2 loss: 0.11334602534770966\n",
      "iteration 554, loss value 341.2441711425781, l2 loss: 0.11333771795034409\n",
      "iteration 555, loss value 341.2364196777344, l2 loss: 0.11332948505878448\n",
      "iteration 556, loss value 341.2294921875, l2 loss: 0.11332126706838608\n",
      "iteration 557, loss value 341.22186279296875, l2 loss: 0.11331311613321304\n",
      "iteration 558, loss value 341.2145690917969, l2 loss: 0.113305002450943\n",
      "iteration 559, loss value 341.2076721191406, l2 loss: 0.11329691857099533\n",
      "iteration 560, loss value 341.2002258300781, l2 loss: 0.11328889429569244\n",
      "iteration 561, loss value 341.1927185058594, l2 loss: 0.11328091472387314\n",
      "iteration 562, loss value 341.18560791015625, l2 loss: 0.11327297240495682\n",
      "iteration 563, loss value 341.17840576171875, l2 loss: 0.11326510459184647\n",
      "iteration 564, loss value 341.1714782714844, l2 loss: 0.11325722187757492\n",
      "iteration 565, loss value 341.1639099121094, l2 loss: 0.11324942857027054\n",
      "iteration 566, loss value 341.15667724609375, l2 loss: 0.11324167251586914\n",
      "iteration 567, loss value 341.1495056152344, l2 loss: 0.11323394626379013\n",
      "iteration 568, loss value 341.1423645019531, l2 loss: 0.1132262796163559\n",
      "iteration 569, loss value 341.1353454589844, l2 loss: 0.11321868747472763\n",
      "iteration 570, loss value 341.1286315917969, l2 loss: 0.11321109533309937\n",
      "iteration 571, loss value 341.1215515136719, l2 loss: 0.11320352554321289\n",
      "iteration 572, loss value 341.1139831542969, l2 loss: 0.11319607496261597\n",
      "iteration 573, loss value 341.1075439453125, l2 loss: 0.11318863928318024\n",
      "iteration 574, loss value 341.1004638671875, l2 loss: 0.1131812185049057\n",
      "iteration 575, loss value 341.0935974121094, l2 loss: 0.11317384988069534\n",
      "iteration 576, loss value 341.0862731933594, l2 loss: 0.11316652595996857\n",
      "iteration 577, loss value 341.0795593261719, l2 loss: 0.11315926909446716\n",
      "iteration 578, loss value 341.0728454589844, l2 loss: 0.11315205693244934\n",
      "iteration 579, loss value 341.0660705566406, l2 loss: 0.1131448820233345\n",
      "iteration 580, loss value 341.05926513671875, l2 loss: 0.11313773691654205\n",
      "iteration 581, loss value 341.0517883300781, l2 loss: 0.11313065141439438\n",
      "iteration 582, loss value 341.0453796386719, l2 loss: 0.1131235808134079\n",
      "iteration 583, loss value 341.0384826660156, l2 loss: 0.11311661452054977\n",
      "iteration 584, loss value 341.0320739746094, l2 loss: 0.11310963332653046\n",
      "iteration 585, loss value 341.0252380371094, l2 loss: 0.11310270428657532\n",
      "iteration 586, loss value 341.0180358886719, l2 loss: 0.11309585720300674\n",
      "iteration 587, loss value 341.0115966796875, l2 loss: 0.11308902502059937\n",
      "iteration 588, loss value 341.0047302246094, l2 loss: 0.11308227479457855\n",
      "iteration 589, loss value 340.99871826171875, l2 loss: 0.11307551711797714\n",
      "iteration 590, loss value 340.99176025390625, l2 loss: 0.11306881159543991\n",
      "iteration 591, loss value 340.9851989746094, l2 loss: 0.11306214332580566\n",
      "iteration 592, loss value 340.978271484375, l2 loss: 0.11305554956197739\n",
      "iteration 593, loss value 340.9726257324219, l2 loss: 0.1130489930510521\n",
      "iteration 594, loss value 340.96575927734375, l2 loss: 0.1130424439907074\n",
      "iteration 595, loss value 340.958984375, l2 loss: 0.11303596943616867\n",
      "iteration 596, loss value 340.95257568359375, l2 loss: 0.11302956193685532\n",
      "iteration 597, loss value 340.94635009765625, l2 loss: 0.11302313953638077\n",
      "iteration 598, loss value 340.9396667480469, l2 loss: 0.113016776740551\n",
      "iteration 599, loss value 340.93310546875, l2 loss: 0.1130104660987854\n",
      "iteration 600, loss value 340.9269714355469, l2 loss: 0.11300420016050339\n",
      "iteration 601, loss value 340.92059326171875, l2 loss: 0.11299796402454376\n",
      "iteration 602, loss value 340.9139099121094, l2 loss: 0.11299178749322891\n",
      "iteration 603, loss value 340.9077453613281, l2 loss: 0.11298565566539764\n",
      "iteration 604, loss value 340.90185546875, l2 loss: 0.11297953873872757\n",
      "iteration 605, loss value 340.8948669433594, l2 loss: 0.11297348141670227\n",
      "iteration 606, loss value 340.88897705078125, l2 loss: 0.11296746134757996\n",
      "iteration 607, loss value 340.8824768066406, l2 loss: 0.11296146363019943\n",
      "iteration 608, loss value 340.8763122558594, l2 loss: 0.11295557022094727\n",
      "iteration 609, loss value 340.8707275390625, l2 loss: 0.1129496619105339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 610, loss value 340.8643493652344, l2 loss: 0.11294375360012054\n",
      "iteration 611, loss value 340.85791015625, l2 loss: 0.11293796449899673\n",
      "iteration 612, loss value 340.85186767578125, l2 loss: 0.11293214559555054\n",
      "iteration 613, loss value 340.8454284667969, l2 loss: 0.11292639374732971\n",
      "iteration 614, loss value 340.8392333984375, l2 loss: 0.11292070895433426\n",
      "iteration 615, loss value 340.83319091796875, l2 loss: 0.11291506141424179\n",
      "iteration 616, loss value 340.82733154296875, l2 loss: 0.1129094585776329\n",
      "iteration 617, loss value 340.8211669921875, l2 loss: 0.11290384083986282\n",
      "iteration 618, loss value 340.8149108886719, l2 loss: 0.11289830505847931\n",
      "iteration 619, loss value 340.8089294433594, l2 loss: 0.11289280652999878\n",
      "iteration 620, loss value 340.80303955078125, l2 loss: 0.11288730055093765\n",
      "iteration 621, loss value 340.7967224121094, l2 loss: 0.11288188397884369\n",
      "iteration 622, loss value 340.7908630371094, l2 loss: 0.11287649720907211\n",
      "iteration 623, loss value 340.78509521484375, l2 loss: 0.11287114769220352\n",
      "iteration 624, loss value 340.77935791015625, l2 loss: 0.11286582052707672\n",
      "iteration 625, loss value 340.77349853515625, l2 loss: 0.1128605306148529\n",
      "iteration 626, loss value 340.7673034667969, l2 loss: 0.11285529285669327\n",
      "iteration 627, loss value 340.76129150390625, l2 loss: 0.11285008490085602\n",
      "iteration 628, loss value 340.75567626953125, l2 loss: 0.11284492164850235\n",
      "iteration 629, loss value 340.7498474121094, l2 loss: 0.11283979564905167\n",
      "iteration 630, loss value 340.74395751953125, l2 loss: 0.11283471435308456\n",
      "iteration 631, loss value 340.7383728027344, l2 loss: 0.11282960325479507\n",
      "iteration 632, loss value 340.7325439453125, l2 loss: 0.11282461136579514\n",
      "iteration 633, loss value 340.72662353515625, l2 loss: 0.1128196194767952\n",
      "iteration 634, loss value 340.7208557128906, l2 loss: 0.11281466484069824\n",
      "iteration 635, loss value 340.71514892578125, l2 loss: 0.11280974745750427\n",
      "iteration 636, loss value 340.7098388671875, l2 loss: 0.1128048449754715\n",
      "iteration 637, loss value 340.7037353515625, l2 loss: 0.11280000954866409\n",
      "iteration 638, loss value 340.6981201171875, l2 loss: 0.11279520392417908\n",
      "iteration 639, loss value 340.6929626464844, l2 loss: 0.11279041320085526\n",
      "iteration 640, loss value 340.6868591308594, l2 loss: 0.11278566718101501\n",
      "iteration 641, loss value 340.6811218261719, l2 loss: 0.11278095841407776\n",
      "iteration 642, loss value 340.67645263671875, l2 loss: 0.1127762645483017\n",
      "iteration 643, loss value 340.6700744628906, l2 loss: 0.11277160048484802\n",
      "iteration 644, loss value 340.6645812988281, l2 loss: 0.11276699602603912\n",
      "iteration 645, loss value 340.6590576171875, l2 loss: 0.11276242136955261\n",
      "iteration 646, loss value 340.65386962890625, l2 loss: 0.11275789141654968\n",
      "iteration 647, loss value 340.6480712890625, l2 loss: 0.11275336146354675\n",
      "iteration 648, loss value 340.6424255371094, l2 loss: 0.11274885386228561\n",
      "iteration 649, loss value 340.6368408203125, l2 loss: 0.11274440586566925\n",
      "iteration 650, loss value 340.63153076171875, l2 loss: 0.11274000257253647\n",
      "iteration 651, loss value 340.6260681152344, l2 loss: 0.11273562163114548\n",
      "iteration 652, loss value 340.6209716796875, l2 loss: 0.11273125559091568\n",
      "iteration 653, loss value 340.6155700683594, l2 loss: 0.11272692680358887\n",
      "iteration 654, loss value 340.6100158691406, l2 loss: 0.11272262781858444\n",
      "iteration 655, loss value 340.60467529296875, l2 loss: 0.1127183809876442\n",
      "iteration 656, loss value 340.5993957519531, l2 loss: 0.11271415650844574\n",
      "iteration 657, loss value 340.5938415527344, l2 loss: 0.11270994693040848\n",
      "iteration 658, loss value 340.58868408203125, l2 loss: 0.1127057820558548\n",
      "iteration 659, loss value 340.58319091796875, l2 loss: 0.11270162463188171\n",
      "iteration 660, loss value 340.57818603515625, l2 loss: 0.11269751936197281\n",
      "iteration 661, loss value 340.5733337402344, l2 loss: 0.11269345134496689\n",
      "iteration 662, loss value 340.5677795410156, l2 loss: 0.11268940567970276\n",
      "iteration 663, loss value 340.5625915527344, l2 loss: 0.11268533021211624\n",
      "iteration 664, loss value 340.5572814941406, l2 loss: 0.11268135160207748\n",
      "iteration 665, loss value 340.5520935058594, l2 loss: 0.11267737299203873\n",
      "iteration 666, loss value 340.5468444824219, l2 loss: 0.11267346143722534\n",
      "iteration 667, loss value 340.5418701171875, l2 loss: 0.11266952753067017\n",
      "iteration 668, loss value 340.5369567871094, l2 loss: 0.11266563087701797\n",
      "iteration 669, loss value 340.5311279296875, l2 loss: 0.11266177892684937\n",
      "iteration 670, loss value 340.5259704589844, l2 loss: 0.11265794187784195\n",
      "iteration 671, loss value 340.52069091796875, l2 loss: 0.11265412718057632\n",
      "iteration 672, loss value 340.5159606933594, l2 loss: 0.11265037208795547\n",
      "iteration 673, loss value 340.51092529296875, l2 loss: 0.11264661699533463\n",
      "iteration 674, loss value 340.5057678222656, l2 loss: 0.11264291405677795\n",
      "iteration 675, loss value 340.501220703125, l2 loss: 0.11263918876647949\n",
      "iteration 676, loss value 340.4956970214844, l2 loss: 0.11263551563024521\n",
      "iteration 677, loss value 340.4904479980469, l2 loss: 0.1126318871974945\n",
      "iteration 678, loss value 340.48590087890625, l2 loss: 0.11262824386358261\n",
      "iteration 679, loss value 340.4810485839844, l2 loss: 0.11262466758489609\n",
      "iteration 680, loss value 340.4758605957031, l2 loss: 0.11262111365795135\n",
      "iteration 681, loss value 340.4714050292969, l2 loss: 0.11261754482984543\n",
      "iteration 682, loss value 340.4661865234375, l2 loss: 0.11261401325464249\n",
      "iteration 683, loss value 340.4612731933594, l2 loss: 0.11261052638292313\n",
      "iteration 684, loss value 340.4563903808594, l2 loss: 0.11260705441236496\n",
      "iteration 685, loss value 340.45135498046875, l2 loss: 0.11260361224412918\n",
      "iteration 686, loss value 340.4465026855469, l2 loss: 0.11260019242763519\n",
      "iteration 687, loss value 340.4418029785156, l2 loss: 0.1125967875123024\n",
      "iteration 688, loss value 340.43701171875, l2 loss: 0.11259341239929199\n",
      "iteration 689, loss value 340.4318542480469, l2 loss: 0.11259004473686218\n",
      "iteration 690, loss value 340.4269714355469, l2 loss: 0.11258672177791595\n",
      "iteration 691, loss value 340.4228210449219, l2 loss: 0.11258339136838913\n",
      "iteration 692, loss value 340.4176330566406, l2 loss: 0.11258015036582947\n",
      "iteration 693, loss value 340.41351318359375, l2 loss: 0.11257686465978622\n",
      "iteration 694, loss value 340.40863037109375, l2 loss: 0.11257362365722656\n",
      "iteration 695, loss value 340.40338134765625, l2 loss: 0.1125703975558281\n",
      "iteration 696, loss value 340.39910888671875, l2 loss: 0.11256717890501022\n",
      "iteration 697, loss value 340.39398193359375, l2 loss: 0.11256401985883713\n",
      "iteration 698, loss value 340.38946533203125, l2 loss: 0.11256086081266403\n",
      "iteration 699, loss value 340.3845520019531, l2 loss: 0.11255770921707153\n",
      "iteration 700, loss value 340.37994384765625, l2 loss: 0.11255458742380142\n",
      "iteration 701, loss value 340.3759460449219, l2 loss: 0.11255151033401489\n",
      "iteration 702, loss value 340.37103271484375, l2 loss: 0.11254843324422836\n",
      "iteration 703, loss value 340.3661804199219, l2 loss: 0.11254537105560303\n",
      "iteration 704, loss value 340.3617858886719, l2 loss: 0.11254234611988068\n",
      "iteration 705, loss value 340.35772705078125, l2 loss: 0.11253930628299713\n",
      "iteration 706, loss value 340.3525695800781, l2 loss: 0.11253631114959717\n",
      "iteration 707, loss value 340.34796142578125, l2 loss: 0.11253336071968079\n",
      "iteration 708, loss value 340.34368896484375, l2 loss: 0.11253038793802261\n",
      "iteration 709, loss value 340.3391418457031, l2 loss: 0.11252745985984802\n",
      "iteration 710, loss value 340.3349609375, l2 loss: 0.11252453178167343\n",
      "iteration 711, loss value 340.33001708984375, l2 loss: 0.11252162605524063\n",
      "iteration 712, loss value 340.3254089355469, l2 loss: 0.11251875013113022\n",
      "iteration 713, loss value 340.32159423828125, l2 loss: 0.1125159040093422\n",
      "iteration 714, loss value 340.3169860839844, l2 loss: 0.11251301318407059\n",
      "iteration 715, loss value 340.3119201660156, l2 loss: 0.11251019686460495\n",
      "iteration 716, loss value 340.30767822265625, l2 loss: 0.11250738054513931\n",
      "iteration 717, loss value 340.3035583496094, l2 loss: 0.11250457167625427\n",
      "iteration 718, loss value 340.29913330078125, l2 loss: 0.11250182241201401\n",
      "iteration 719, loss value 340.2945556640625, l2 loss: 0.11249905079603195\n",
      "iteration 720, loss value 340.2903137207031, l2 loss: 0.11249630153179169\n",
      "iteration 721, loss value 340.2857360839844, l2 loss: 0.11249355971813202\n",
      "iteration 722, loss value 340.2814636230469, l2 loss: 0.11249084025621414\n",
      "iteration 723, loss value 340.2770690917969, l2 loss: 0.11248815059661865\n",
      "iteration 724, loss value 340.272705078125, l2 loss: 0.11248547583818436\n",
      "iteration 725, loss value 340.2687683105469, l2 loss: 0.11248281598091125\n",
      "iteration 726, loss value 340.2636413574219, l2 loss: 0.11248015612363815\n",
      "iteration 727, loss value 340.25982666015625, l2 loss: 0.11247754096984863\n",
      "iteration 728, loss value 340.2558288574219, l2 loss: 0.11247491836547852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 729, loss value 340.2516784667969, l2 loss: 0.1124722957611084\n",
      "iteration 730, loss value 340.2474060058594, l2 loss: 0.11246970295906067\n",
      "iteration 731, loss value 340.2431640625, l2 loss: 0.11246711760759354\n",
      "iteration 732, loss value 340.2389831542969, l2 loss: 0.1124645471572876\n",
      "iteration 733, loss value 340.23480224609375, l2 loss: 0.11246203631162643\n",
      "iteration 734, loss value 340.2305603027344, l2 loss: 0.1124594584107399\n",
      "iteration 735, loss value 340.2263488769531, l2 loss: 0.11245694756507874\n",
      "iteration 736, loss value 340.22247314453125, l2 loss: 0.11245445907115936\n",
      "iteration 737, loss value 340.2184753417969, l2 loss: 0.1124519556760788\n",
      "iteration 738, loss value 340.2137756347656, l2 loss: 0.11244946718215942\n",
      "iteration 739, loss value 340.2098083496094, l2 loss: 0.11244699358940125\n",
      "iteration 740, loss value 340.2056579589844, l2 loss: 0.11244450509548187\n",
      "iteration 741, loss value 340.20123291015625, l2 loss: 0.11244210600852966\n",
      "iteration 742, loss value 340.1977233886719, l2 loss: 0.11243964731693268\n",
      "iteration 743, loss value 340.1934509277344, l2 loss: 0.11243722587823868\n",
      "iteration 744, loss value 340.1894836425781, l2 loss: 0.11243483424186707\n",
      "iteration 745, loss value 340.18524169921875, l2 loss: 0.11243243515491486\n",
      "iteration 746, loss value 340.1812438964844, l2 loss: 0.11243005096912384\n",
      "iteration 747, loss value 340.1771240234375, l2 loss: 0.11242768168449402\n",
      "iteration 748, loss value 340.1733703613281, l2 loss: 0.11242533475160599\n",
      "iteration 749, loss value 340.16912841796875, l2 loss: 0.11242297291755676\n",
      "iteration 750, loss value 340.1654357910156, l2 loss: 0.11242061853408813\n",
      "iteration 751, loss value 340.1612243652344, l2 loss: 0.11241832375526428\n",
      "iteration 752, loss value 340.15753173828125, l2 loss: 0.11241597682237625\n",
      "iteration 753, loss value 340.1534118652344, l2 loss: 0.11241371184587479\n",
      "iteration 754, loss value 340.14971923828125, l2 loss: 0.11241137236356735\n",
      "iteration 755, loss value 340.14569091796875, l2 loss: 0.1124090701341629\n",
      "iteration 756, loss value 340.1415100097656, l2 loss: 0.11240682750940323\n",
      "iteration 757, loss value 340.13812255859375, l2 loss: 0.11240454018115997\n",
      "iteration 758, loss value 340.1337585449219, l2 loss: 0.1124022901058197\n",
      "iteration 759, loss value 340.1296691894531, l2 loss: 0.11240003257989883\n",
      "iteration 760, loss value 340.1260681152344, l2 loss: 0.11239781975746155\n",
      "iteration 761, loss value 340.12261962890625, l2 loss: 0.11239556223154068\n",
      "iteration 762, loss value 340.11834716796875, l2 loss: 0.11239337921142578\n",
      "iteration 763, loss value 340.11474609375, l2 loss: 0.11239117383956909\n",
      "iteration 764, loss value 340.11083984375, l2 loss: 0.11238894611597061\n",
      "iteration 765, loss value 340.10693359375, l2 loss: 0.11238675564527512\n",
      "iteration 766, loss value 340.10333251953125, l2 loss: 0.11238458007574081\n",
      "iteration 767, loss value 340.099365234375, l2 loss: 0.11238241195678711\n",
      "iteration 768, loss value 340.0959167480469, l2 loss: 0.11238022893667221\n",
      "iteration 769, loss value 340.09136962890625, l2 loss: 0.1123780831694603\n",
      "iteration 770, loss value 340.08831787109375, l2 loss: 0.11237592250108719\n",
      "iteration 771, loss value 340.08489990234375, l2 loss: 0.11237378418445587\n",
      "iteration 772, loss value 340.080810546875, l2 loss: 0.11237163096666336\n",
      "iteration 773, loss value 340.0767517089844, l2 loss: 0.11236947774887085\n",
      "iteration 774, loss value 340.07281494140625, l2 loss: 0.11236739158630371\n",
      "iteration 775, loss value 340.06976318359375, l2 loss: 0.11236528307199478\n",
      "iteration 776, loss value 340.0659484863281, l2 loss: 0.11236315965652466\n",
      "iteration 777, loss value 340.06231689453125, l2 loss: 0.11236107349395752\n",
      "iteration 778, loss value 340.0591735839844, l2 loss: 0.1123589500784874\n",
      "iteration 779, loss value 340.0548400878906, l2 loss: 0.11235686391592026\n",
      "iteration 780, loss value 340.0513610839844, l2 loss: 0.11235480010509491\n",
      "iteration 781, loss value 340.04791259765625, l2 loss: 0.11235270649194717\n",
      "iteration 782, loss value 340.0444030761719, l2 loss: 0.11235063523054123\n",
      "iteration 783, loss value 340.04058837890625, l2 loss: 0.11234856396913528\n",
      "iteration 784, loss value 340.0365905761719, l2 loss: 0.11234650760889053\n",
      "iteration 785, loss value 340.0335388183594, l2 loss: 0.11234442889690399\n",
      "iteration 786, loss value 340.0295104980469, l2 loss: 0.11234240978956223\n",
      "iteration 787, loss value 340.0267639160156, l2 loss: 0.11234033852815628\n",
      "iteration 788, loss value 340.02264404296875, l2 loss: 0.11233828216791153\n",
      "iteration 789, loss value 340.01898193359375, l2 loss: 0.11233627796173096\n",
      "iteration 790, loss value 340.0160217285156, l2 loss: 0.1123342365026474\n",
      "iteration 791, loss value 340.0122985839844, l2 loss: 0.11233220249414444\n",
      "iteration 792, loss value 340.00897216796875, l2 loss: 0.11233016103506088\n",
      "iteration 793, loss value 340.0052490234375, l2 loss: 0.11232814192771912\n",
      "iteration 794, loss value 340.0016784667969, l2 loss: 0.11232614517211914\n",
      "iteration 795, loss value 339.99847412109375, l2 loss: 0.11232415586709976\n",
      "iteration 796, loss value 339.99505615234375, l2 loss: 0.11232214421033859\n",
      "iteration 797, loss value 339.9911804199219, l2 loss: 0.11232015490531921\n",
      "iteration 798, loss value 339.9879150390625, l2 loss: 0.11231814324855804\n",
      "iteration 799, loss value 339.9847106933594, l2 loss: 0.11231611669063568\n",
      "iteration 800, loss value 339.9805603027344, l2 loss: 0.1123141422867775\n",
      "iteration 801, loss value 339.97735595703125, l2 loss: 0.11231216043233871\n",
      "iteration 802, loss value 339.9743347167969, l2 loss: 0.11231014877557755\n",
      "iteration 803, loss value 339.9705505371094, l2 loss: 0.11230820417404175\n",
      "iteration 804, loss value 339.967529296875, l2 loss: 0.11230617761611938\n",
      "iteration 805, loss value 339.96380615234375, l2 loss: 0.1123041957616806\n",
      "iteration 806, loss value 339.96038818359375, l2 loss: 0.11230224370956421\n",
      "iteration 807, loss value 339.95733642578125, l2 loss: 0.11230029165744781\n",
      "iteration 808, loss value 339.95391845703125, l2 loss: 0.11229830235242844\n",
      "iteration 809, loss value 339.95062255859375, l2 loss: 0.11229632049798965\n",
      "iteration 810, loss value 339.9472351074219, l2 loss: 0.11229436844587326\n",
      "iteration 811, loss value 339.9444274902344, l2 loss: 0.11229240149259567\n",
      "iteration 812, loss value 339.9401550292969, l2 loss: 0.11229045689105988\n",
      "iteration 813, loss value 339.93743896484375, l2 loss: 0.11228849738836288\n",
      "iteration 814, loss value 339.93450927734375, l2 loss: 0.11228654533624649\n",
      "iteration 815, loss value 339.9313659667969, l2 loss: 0.1122845709323883\n",
      "iteration 816, loss value 339.9276123046875, l2 loss: 0.11228261142969131\n",
      "iteration 817, loss value 339.9243469238281, l2 loss: 0.11228066682815552\n",
      "iteration 818, loss value 339.9212646484375, l2 loss: 0.11227872222661972\n",
      "iteration 819, loss value 339.91827392578125, l2 loss: 0.11227676272392273\n",
      "iteration 820, loss value 339.9149169921875, l2 loss: 0.11227478086948395\n",
      "iteration 821, loss value 339.9112243652344, l2 loss: 0.11227285116910934\n",
      "iteration 822, loss value 339.9079895019531, l2 loss: 0.11227092146873474\n",
      "iteration 823, loss value 339.9052429199219, l2 loss: 0.11226896196603775\n",
      "iteration 824, loss value 339.90185546875, l2 loss: 0.11226698011159897\n",
      "iteration 825, loss value 339.89801025390625, l2 loss: 0.11226504296064377\n",
      "iteration 826, loss value 339.89532470703125, l2 loss: 0.11226312071084976\n",
      "iteration 827, loss value 339.8923034667969, l2 loss: 0.11226116865873337\n",
      "iteration 828, loss value 339.88934326171875, l2 loss: 0.11225921660661697\n",
      "iteration 829, loss value 339.88604736328125, l2 loss: 0.11225726455450058\n",
      "iteration 830, loss value 339.8829040527344, l2 loss: 0.11225532740354538\n",
      "iteration 831, loss value 339.879638671875, l2 loss: 0.11225336045026779\n",
      "iteration 832, loss value 339.87652587890625, l2 loss: 0.1122514083981514\n",
      "iteration 833, loss value 339.8733825683594, l2 loss: 0.1122494637966156\n",
      "iteration 834, loss value 339.8700866699219, l2 loss: 0.1122475266456604\n",
      "iteration 835, loss value 339.8672790527344, l2 loss: 0.11224555224180222\n",
      "iteration 836, loss value 339.8639831542969, l2 loss: 0.11224362254142761\n",
      "iteration 837, loss value 339.86138916015625, l2 loss: 0.11224167048931122\n",
      "iteration 838, loss value 339.8582763671875, l2 loss: 0.11223971098661423\n",
      "iteration 839, loss value 339.8551025390625, l2 loss: 0.11223776638507843\n",
      "iteration 840, loss value 339.85205078125, l2 loss: 0.11223578453063965\n",
      "iteration 841, loss value 339.8489074707031, l2 loss: 0.11223383247852325\n",
      "iteration 842, loss value 339.8461608886719, l2 loss: 0.11223188042640686\n",
      "iteration 843, loss value 339.8428649902344, l2 loss: 0.11222988367080688\n",
      "iteration 844, loss value 339.83917236328125, l2 loss: 0.11222793906927109\n",
      "iteration 845, loss value 339.83673095703125, l2 loss: 0.11222600191831589\n",
      "iteration 846, loss value 339.83447265625, l2 loss: 0.1122240200638771\n",
      "iteration 847, loss value 339.83074951171875, l2 loss: 0.11222204566001892\n",
      "iteration 848, loss value 339.827880859375, l2 loss: 0.11222007125616074\n",
      "iteration 849, loss value 339.8246765136719, l2 loss: 0.11221809685230255\n",
      "iteration 850, loss value 339.82220458984375, l2 loss: 0.11221612989902496\n",
      "iteration 851, loss value 339.8185729980469, l2 loss: 0.11221417784690857\n",
      "iteration 852, loss value 339.81622314453125, l2 loss: 0.11221219599246979\n",
      "iteration 853, loss value 339.813232421875, l2 loss: 0.1122102215886116\n",
      "iteration 854, loss value 339.81048583984375, l2 loss: 0.11220823228359222\n",
      "iteration 855, loss value 339.8072814941406, l2 loss: 0.11220626533031464\n",
      "iteration 856, loss value 339.80438232421875, l2 loss: 0.11220426857471466\n",
      "iteration 857, loss value 339.8017272949219, l2 loss: 0.11220225691795349\n",
      "iteration 858, loss value 339.79852294921875, l2 loss: 0.1122002899646759\n",
      "iteration 859, loss value 339.7962341308594, l2 loss: 0.11219827830791473\n",
      "iteration 860, loss value 339.79266357421875, l2 loss: 0.11219625920057297\n",
      "iteration 861, loss value 339.7900695800781, l2 loss: 0.11219426989555359\n",
      "iteration 862, loss value 339.78741455078125, l2 loss: 0.11219227313995361\n",
      "iteration 863, loss value 339.7842712402344, l2 loss: 0.11219028383493423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 864, loss value 339.78143310546875, l2 loss: 0.11218827217817307\n",
      "iteration 865, loss value 339.77911376953125, l2 loss: 0.11218623071908951\n",
      "iteration 866, loss value 339.77557373046875, l2 loss: 0.11218422651290894\n",
      "iteration 867, loss value 339.77276611328125, l2 loss: 0.11218216270208359\n",
      "iteration 868, loss value 339.76995849609375, l2 loss: 0.11218015849590302\n",
      "iteration 869, loss value 339.76739501953125, l2 loss: 0.11217813938856125\n",
      "iteration 870, loss value 339.7646179199219, l2 loss: 0.1121760681271553\n",
      "iteration 871, loss value 339.761474609375, l2 loss: 0.11217404901981354\n",
      "iteration 872, loss value 339.7591552734375, l2 loss: 0.11217201501131058\n",
      "iteration 873, loss value 339.7562561035156, l2 loss: 0.11216994374990463\n",
      "iteration 874, loss value 339.753173828125, l2 loss: 0.11216792464256287\n",
      "iteration 875, loss value 339.7503967285156, l2 loss: 0.11216584593057632\n",
      "iteration 876, loss value 339.7478942871094, l2 loss: 0.11216380447149277\n",
      "iteration 877, loss value 339.74530029296875, l2 loss: 0.11216172575950623\n",
      "iteration 878, loss value 339.7423095703125, l2 loss: 0.11215966939926147\n",
      "iteration 879, loss value 339.7402648925781, l2 loss: 0.11215762048959732\n",
      "iteration 880, loss value 339.7370910644531, l2 loss: 0.11215551942586899\n",
      "iteration 881, loss value 339.7345275878906, l2 loss: 0.11215344816446304\n",
      "iteration 882, loss value 339.73150634765625, l2 loss: 0.11215134710073471\n",
      "iteration 883, loss value 339.72894287109375, l2 loss: 0.11214927583932877\n",
      "iteration 884, loss value 339.7264099121094, l2 loss: 0.11214715242385864\n",
      "iteration 885, loss value 339.7235107421875, l2 loss: 0.11214504390954971\n",
      "iteration 886, loss value 339.7207336425781, l2 loss: 0.11214295029640198\n",
      "iteration 887, loss value 339.7182312011719, l2 loss: 0.11214082688093185\n",
      "iteration 888, loss value 339.7154541015625, l2 loss: 0.11213868856430054\n",
      "iteration 889, loss value 339.7124328613281, l2 loss: 0.11213656514883041\n",
      "iteration 890, loss value 339.7100524902344, l2 loss: 0.11213447153568268\n",
      "iteration 891, loss value 339.70758056640625, l2 loss: 0.11213234066963196\n",
      "iteration 892, loss value 339.7051696777344, l2 loss: 0.11213018000125885\n",
      "iteration 893, loss value 339.7023010253906, l2 loss: 0.11212801933288574\n",
      "iteration 894, loss value 339.69989013671875, l2 loss: 0.11212589591741562\n",
      "iteration 895, loss value 339.6973571777344, l2 loss: 0.11212372779846191\n",
      "iteration 896, loss value 339.69439697265625, l2 loss: 0.11212155967950821\n",
      "iteration 897, loss value 339.6917419433594, l2 loss: 0.11211938410997391\n",
      "iteration 898, loss value 339.68896484375, l2 loss: 0.1121172159910202\n",
      "iteration 899, loss value 339.6868896484375, l2 loss: 0.1121150478720665\n",
      "iteration 900, loss value 339.6841735839844, l2 loss: 0.11211283504962921\n",
      "iteration 901, loss value 339.6807861328125, l2 loss: 0.1121106967329979\n",
      "iteration 902, loss value 339.67938232421875, l2 loss: 0.11210846900939941\n",
      "iteration 903, loss value 339.6763000488281, l2 loss: 0.11210629343986511\n",
      "iteration 904, loss value 339.674072265625, l2 loss: 0.11210408806800842\n",
      "iteration 905, loss value 339.671630859375, l2 loss: 0.11210186779499054\n",
      "iteration 906, loss value 339.66912841796875, l2 loss: 0.11209962517023087\n",
      "iteration 907, loss value 339.6662292480469, l2 loss: 0.11209740489721298\n",
      "iteration 908, loss value 339.6637878417969, l2 loss: 0.11209516227245331\n",
      "iteration 909, loss value 339.6608581542969, l2 loss: 0.11209291964769363\n",
      "iteration 910, loss value 339.6587219238281, l2 loss: 0.11209065467119217\n",
      "iteration 911, loss value 339.6559753417969, l2 loss: 0.11208844184875488\n",
      "iteration 912, loss value 339.6539001464844, l2 loss: 0.11208618432283401\n",
      "iteration 913, loss value 339.65155029296875, l2 loss: 0.11208388954401016\n",
      "iteration 914, loss value 339.6485290527344, l2 loss: 0.1120816320180893\n",
      "iteration 915, loss value 339.64630126953125, l2 loss: 0.11207935214042664\n",
      "iteration 916, loss value 339.6441345214844, l2 loss: 0.11207707971334457\n",
      "iteration 917, loss value 339.64178466796875, l2 loss: 0.11207476258277893\n",
      "iteration 918, loss value 339.63922119140625, l2 loss: 0.11207244545221329\n",
      "iteration 919, loss value 339.6364440917969, l2 loss: 0.11207015067338943\n",
      "iteration 920, loss value 339.63409423828125, l2 loss: 0.1120678260922432\n",
      "iteration 921, loss value 339.6314392089844, l2 loss: 0.11206549406051636\n",
      "iteration 922, loss value 339.6290283203125, l2 loss: 0.11206316947937012\n",
      "iteration 923, loss value 339.62677001953125, l2 loss: 0.11206082254648209\n",
      "iteration 924, loss value 339.6242370605469, l2 loss: 0.11205846816301346\n",
      "iteration 925, loss value 339.6219482421875, l2 loss: 0.11205611377954483\n",
      "iteration 926, loss value 339.6193542480469, l2 loss: 0.1120537668466568\n",
      "iteration 927, loss value 339.6170349121094, l2 loss: 0.11205141246318817\n",
      "iteration 928, loss value 339.61456298828125, l2 loss: 0.11204904317855835\n",
      "iteration 929, loss value 339.61260986328125, l2 loss: 0.11204663664102554\n",
      "iteration 930, loss value 339.6098937988281, l2 loss: 0.11204423755407333\n",
      "iteration 931, loss value 339.6070861816406, l2 loss: 0.11204186081886292\n",
      "iteration 932, loss value 339.6054382324219, l2 loss: 0.11203945428133011\n",
      "iteration 933, loss value 339.6031799316406, l2 loss: 0.1120370477437973\n",
      "iteration 934, loss value 339.60076904296875, l2 loss: 0.11203460395336151\n",
      "iteration 935, loss value 339.5982360839844, l2 loss: 0.11203217506408691\n",
      "iteration 936, loss value 339.5958557128906, l2 loss: 0.11202976107597351\n",
      "iteration 937, loss value 339.59381103515625, l2 loss: 0.11202730238437653\n",
      "iteration 938, loss value 339.59124755859375, l2 loss: 0.11202485114336014\n",
      "iteration 939, loss value 339.5892028808594, l2 loss: 0.11202238500118256\n",
      "iteration 940, loss value 339.5865478515625, l2 loss: 0.11201991885900497\n",
      "iteration 941, loss value 339.5843505859375, l2 loss: 0.11201740801334381\n",
      "iteration 942, loss value 339.58172607421875, l2 loss: 0.11201494187116623\n",
      "iteration 943, loss value 339.5795593261719, l2 loss: 0.11201243847608566\n",
      "iteration 944, loss value 339.5770568847656, l2 loss: 0.1120099201798439\n",
      "iteration 945, loss value 339.57501220703125, l2 loss: 0.11200740933418274\n",
      "iteration 946, loss value 339.5727233886719, l2 loss: 0.11200489103794098\n",
      "iteration 947, loss value 339.5703430175781, l2 loss: 0.11200237274169922\n",
      "iteration 948, loss value 339.5680847167969, l2 loss: 0.11199985444545746\n",
      "iteration 949, loss value 339.56622314453125, l2 loss: 0.11199727654457092\n",
      "iteration 950, loss value 339.5633544921875, l2 loss: 0.11199475079774857\n",
      "iteration 951, loss value 339.5616455078125, l2 loss: 0.11199215799570084\n",
      "iteration 952, loss value 339.5591735839844, l2 loss: 0.11198960989713669\n",
      "iteration 953, loss value 339.5566101074219, l2 loss: 0.11198703944683075\n",
      "iteration 954, loss value 339.5545654296875, l2 loss: 0.11198445409536362\n",
      "iteration 955, loss value 339.55242919921875, l2 loss: 0.11198185384273529\n",
      "iteration 956, loss value 339.5505065917969, l2 loss: 0.11197923868894577\n",
      "iteration 957, loss value 339.5478820800781, l2 loss: 0.11197664588689804\n",
      "iteration 958, loss value 339.54595947265625, l2 loss: 0.11197401583194733\n",
      "iteration 959, loss value 339.5436706542969, l2 loss: 0.11197139322757721\n",
      "iteration 960, loss value 339.5416564941406, l2 loss: 0.11196873337030411\n",
      "iteration 961, loss value 339.539306640625, l2 loss: 0.1119661033153534\n",
      "iteration 962, loss value 339.53704833984375, l2 loss: 0.11196344345808029\n",
      "iteration 963, loss value 339.5350646972656, l2 loss: 0.11196077615022659\n",
      "iteration 964, loss value 339.5326843261719, l2 loss: 0.1119580790400505\n",
      "iteration 965, loss value 339.53057861328125, l2 loss: 0.11195540428161621\n",
      "iteration 966, loss value 339.5281982421875, l2 loss: 0.1119527593255043\n",
      "iteration 967, loss value 339.5264587402344, l2 loss: 0.11195003986358643\n",
      "iteration 968, loss value 339.5241394042969, l2 loss: 0.11194734275341034\n",
      "iteration 969, loss value 339.5219421386719, l2 loss: 0.11194462329149246\n",
      "iteration 970, loss value 339.5204162597656, l2 loss: 0.1119418814778328\n",
      "iteration 971, loss value 339.5179748535156, l2 loss: 0.11193913966417313\n",
      "iteration 972, loss value 339.5152587890625, l2 loss: 0.11193642020225525\n",
      "iteration 973, loss value 339.5138854980469, l2 loss: 0.11193365603685379\n",
      "iteration 974, loss value 339.51116943359375, l2 loss: 0.11193087697029114\n",
      "iteration 975, loss value 339.5092468261719, l2 loss: 0.11192812025547028\n",
      "iteration 976, loss value 339.5072021484375, l2 loss: 0.11192536354064941\n",
      "iteration 977, loss value 339.5050048828125, l2 loss: 0.11192252486944199\n",
      "iteration 978, loss value 339.5027770996094, l2 loss: 0.11191976070404053\n",
      "iteration 979, loss value 339.5006103515625, l2 loss: 0.11191695928573608\n",
      "iteration 980, loss value 339.4990234375, l2 loss: 0.11191412806510925\n",
      "iteration 981, loss value 339.4964294433594, l2 loss: 0.11191129684448242\n",
      "iteration 982, loss value 339.4944763183594, l2 loss: 0.11190848052501678\n",
      "iteration 983, loss value 339.4923095703125, l2 loss: 0.11190561950206757\n",
      "iteration 984, loss value 339.4903259277344, l2 loss: 0.11190276592969894\n",
      "iteration 985, loss value 339.48809814453125, l2 loss: 0.11189990490674973\n",
      "iteration 986, loss value 339.486572265625, l2 loss: 0.11189700663089752\n",
      "iteration 987, loss value 339.4840393066406, l2 loss: 0.1118941456079483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 988, loss value 339.4822082519531, l2 loss: 0.1118912473320961\n",
      "iteration 989, loss value 339.4799499511719, l2 loss: 0.11188837140798569\n",
      "iteration 990, loss value 339.47845458984375, l2 loss: 0.1118854284286499\n",
      "iteration 991, loss value 339.4759826660156, l2 loss: 0.1118825227022171\n",
      "iteration 992, loss value 339.4739990234375, l2 loss: 0.11187957972288132\n",
      "iteration 993, loss value 339.4722595214844, l2 loss: 0.11187667399644852\n",
      "iteration 994, loss value 339.47015380859375, l2 loss: 0.11187368631362915\n",
      "iteration 995, loss value 339.4676513671875, l2 loss: 0.11187072098255157\n",
      "iteration 996, loss value 339.4657897949219, l2 loss: 0.1118677631020546\n",
      "iteration 997, loss value 339.464111328125, l2 loss: 0.11186479032039642\n",
      "iteration 998, loss value 339.4620056152344, l2 loss: 0.11186179518699646\n",
      "iteration 999, loss value 339.46002197265625, l2 loss: 0.1118588000535965\n",
      "iteration 1000, loss value 339.4579772949219, l2 loss: 0.11185579746961594\n",
      "iteration 1001, loss value 339.45611572265625, l2 loss: 0.11185278743505478\n",
      "iteration 1002, loss value 339.4543762207031, l2 loss: 0.11184976249933243\n",
      "iteration 1003, loss value 339.4523620605469, l2 loss: 0.11184671521186829\n",
      "iteration 1004, loss value 339.4498596191406, l2 loss: 0.11184367537498474\n",
      "iteration 1005, loss value 339.4481201171875, l2 loss: 0.11184062063694\n",
      "iteration 1006, loss value 339.44647216796875, l2 loss: 0.11183754354715347\n",
      "iteration 1007, loss value 339.44403076171875, l2 loss: 0.11183445155620575\n",
      "iteration 1008, loss value 339.44189453125, l2 loss: 0.11183138936758041\n",
      "iteration 1009, loss value 339.4402160644531, l2 loss: 0.1118282899260521\n",
      "iteration 1010, loss value 339.4379577636719, l2 loss: 0.11182518303394318\n",
      "iteration 1011, loss value 339.4364929199219, l2 loss: 0.11182206869125366\n",
      "iteration 1012, loss value 339.4343566894531, l2 loss: 0.11181899160146713\n",
      "iteration 1013, loss value 339.43310546875, l2 loss: 0.11181579530239105\n",
      "iteration 1014, loss value 339.4302978515625, l2 loss: 0.11181265860795975\n",
      "iteration 1015, loss value 339.4284973144531, l2 loss: 0.11180951446294785\n",
      "iteration 1016, loss value 339.4270935058594, l2 loss: 0.11180637031793594\n",
      "iteration 1017, loss value 339.4254455566406, l2 loss: 0.11180317401885986\n",
      "iteration 1018, loss value 339.42266845703125, l2 loss: 0.11180001497268677\n",
      "iteration 1019, loss value 339.4213562011719, l2 loss: 0.1117968037724495\n",
      "iteration 1020, loss value 339.4191589355469, l2 loss: 0.11179360002279282\n",
      "iteration 1021, loss value 339.41748046875, l2 loss: 0.11179038882255554\n",
      "iteration 1022, loss value 339.4161682128906, l2 loss: 0.11178717017173767\n",
      "iteration 1023, loss value 339.41400146484375, l2 loss: 0.11178391426801682\n",
      "iteration 1024, loss value 339.4114074707031, l2 loss: 0.11178066581487656\n",
      "iteration 1025, loss value 339.409912109375, l2 loss: 0.1117774248123169\n",
      "iteration 1026, loss value 339.4081726074219, l2 loss: 0.11177414655685425\n",
      "iteration 1027, loss value 339.406005859375, l2 loss: 0.11177089065313339\n",
      "iteration 1028, loss value 339.4043884277344, l2 loss: 0.11176760494709015\n",
      "iteration 1029, loss value 339.40252685546875, l2 loss: 0.11176430433988571\n",
      "iteration 1030, loss value 339.4005126953125, l2 loss: 0.11176101118326187\n",
      "iteration 1031, loss value 339.39862060546875, l2 loss: 0.11175768077373505\n",
      "iteration 1032, loss value 339.3970642089844, l2 loss: 0.11175438016653061\n",
      "iteration 1033, loss value 339.3948669433594, l2 loss: 0.11175105720758438\n",
      "iteration 1034, loss value 339.3934631347656, l2 loss: 0.11174770444631577\n",
      "iteration 1035, loss value 339.39154052734375, l2 loss: 0.11174434423446655\n",
      "iteration 1036, loss value 339.3894348144531, l2 loss: 0.11174097657203674\n",
      "iteration 1037, loss value 339.3879089355469, l2 loss: 0.11173760890960693\n",
      "iteration 1038, loss value 339.3862609863281, l2 loss: 0.11173419654369354\n",
      "iteration 1039, loss value 339.38397216796875, l2 loss: 0.11173082143068314\n",
      "iteration 1040, loss value 339.3824157714844, l2 loss: 0.11172740906476974\n",
      "iteration 1041, loss value 339.3799133300781, l2 loss: 0.11172400414943695\n",
      "iteration 1042, loss value 339.3784484863281, l2 loss: 0.11172059178352356\n",
      "iteration 1043, loss value 339.37689208984375, l2 loss: 0.11171715706586838\n",
      "iteration 1044, loss value 339.3753356933594, l2 loss: 0.11171373724937439\n",
      "iteration 1045, loss value 339.3733825683594, l2 loss: 0.11171027272939682\n",
      "iteration 1046, loss value 339.3715515136719, l2 loss: 0.11170680820941925\n",
      "iteration 1047, loss value 339.3695373535156, l2 loss: 0.11170335859060287\n",
      "iteration 1048, loss value 339.3684387207031, l2 loss: 0.11169985681772232\n",
      "iteration 1049, loss value 339.3663330078125, l2 loss: 0.11169636249542236\n",
      "iteration 1050, loss value 339.3646240234375, l2 loss: 0.11169286072254181\n",
      "iteration 1051, loss value 339.3627624511719, l2 loss: 0.11168935149908066\n",
      "iteration 1052, loss value 339.3609313964844, l2 loss: 0.11168582737445831\n",
      "iteration 1053, loss value 339.35968017578125, l2 loss: 0.11168228834867477\n",
      "iteration 1054, loss value 339.3573913574219, l2 loss: 0.11167871206998825\n",
      "iteration 1055, loss value 339.3553771972656, l2 loss: 0.1116751879453659\n",
      "iteration 1056, loss value 339.3539733886719, l2 loss: 0.11167162656784058\n",
      "iteration 1057, loss value 339.3522644042969, l2 loss: 0.11166805773973465\n",
      "iteration 1058, loss value 339.35052490234375, l2 loss: 0.11166448891162872\n",
      "iteration 1059, loss value 339.34954833984375, l2 loss: 0.11166086047887802\n",
      "iteration 1060, loss value 339.3468322753906, l2 loss: 0.1116572692990303\n",
      "iteration 1061, loss value 339.34552001953125, l2 loss: 0.1116536557674408\n",
      "iteration 1062, loss value 339.3435363769531, l2 loss: 0.11165004968643188\n",
      "iteration 1063, loss value 339.3417663574219, l2 loss: 0.11164642125368118\n",
      "iteration 1064, loss value 339.34033203125, l2 loss: 0.11164277046918869\n",
      "iteration 1065, loss value 339.3383483886719, l2 loss: 0.1116391271352768\n",
      "iteration 1066, loss value 339.3364562988281, l2 loss: 0.11163544654846191\n",
      "iteration 1067, loss value 339.3350830078125, l2 loss: 0.11163181811571121\n",
      "iteration 1068, loss value 339.33331298828125, l2 loss: 0.11162811517715454\n",
      "iteration 1069, loss value 339.3315124511719, l2 loss: 0.11162444204092026\n",
      "iteration 1070, loss value 339.3301696777344, l2 loss: 0.11162074655294418\n",
      "iteration 1071, loss value 339.32861328125, l2 loss: 0.11161703616380692\n",
      "iteration 1072, loss value 339.3268737792969, l2 loss: 0.11161331087350845\n",
      "iteration 1073, loss value 339.32501220703125, l2 loss: 0.1116095706820488\n",
      "iteration 1074, loss value 339.3228759765625, l2 loss: 0.11160586774349213\n",
      "iteration 1075, loss value 339.32171630859375, l2 loss: 0.11160212010145187\n",
      "iteration 1076, loss value 339.3204345703125, l2 loss: 0.11159832775592804\n",
      "iteration 1077, loss value 339.3179016113281, l2 loss: 0.11159458011388779\n",
      "iteration 1078, loss value 339.316650390625, l2 loss: 0.11159081757068634\n",
      "iteration 1079, loss value 339.3153991699219, l2 loss: 0.11158700287342072\n",
      "iteration 1080, loss value 339.3132629394531, l2 loss: 0.11158323287963867\n",
      "iteration 1081, loss value 339.3121032714844, l2 loss: 0.11157941818237305\n",
      "iteration 1082, loss value 339.3102111816406, l2 loss: 0.11157558858394623\n",
      "iteration 1083, loss value 339.30841064453125, l2 loss: 0.11157175898551941\n",
      "iteration 1084, loss value 339.30670166015625, l2 loss: 0.1115679144859314\n",
      "iteration 1085, loss value 339.3051452636719, l2 loss: 0.11156406998634338\n",
      "iteration 1086, loss value 339.3028259277344, l2 loss: 0.11156023293733597\n",
      "iteration 1087, loss value 339.3023376464844, l2 loss: 0.11155634373426437\n",
      "iteration 1088, loss value 339.3001403808594, l2 loss: 0.11155248433351517\n",
      "iteration 1089, loss value 339.2987060546875, l2 loss: 0.11154856532812119\n",
      "iteration 1090, loss value 339.2963562011719, l2 loss: 0.11154469102621078\n",
      "iteration 1091, loss value 339.29547119140625, l2 loss: 0.1115407720208168\n",
      "iteration 1092, loss value 339.29351806640625, l2 loss: 0.11153685301542282\n",
      "iteration 1093, loss value 339.2920837402344, l2 loss: 0.11153293401002884\n",
      "iteration 1094, loss value 339.29071044921875, l2 loss: 0.11152900755405426\n",
      "iteration 1095, loss value 339.2891540527344, l2 loss: 0.1115250289440155\n",
      "iteration 1096, loss value 339.28692626953125, l2 loss: 0.11152111738920212\n",
      "iteration 1097, loss value 339.2857666015625, l2 loss: 0.11151714622974396\n",
      "iteration 1098, loss value 339.2843017578125, l2 loss: 0.1115131601691246\n",
      "iteration 1099, loss value 339.2823791503906, l2 loss: 0.11150917410850525\n",
      "iteration 1100, loss value 339.2812805175781, l2 loss: 0.11150520294904709\n",
      "iteration 1101, loss value 339.2798767089844, l2 loss: 0.11150117963552475\n",
      "iteration 1102, loss value 339.2779235839844, l2 loss: 0.11149715632200241\n",
      "iteration 1103, loss value 339.2760925292969, l2 loss: 0.11149319261312485\n",
      "iteration 1104, loss value 339.27508544921875, l2 loss: 0.11148913204669952\n",
      "iteration 1105, loss value 339.2730407714844, l2 loss: 0.1114850789308548\n",
      "iteration 1106, loss value 339.2715759277344, l2 loss: 0.11148103326559067\n",
      "iteration 1107, loss value 339.2698669433594, l2 loss: 0.11147698014974594\n",
      "iteration 1108, loss value 339.26873779296875, l2 loss: 0.11147291213274002\n",
      "iteration 1109, loss value 339.2671813964844, l2 loss: 0.11146882176399231\n",
      "iteration 1110, loss value 339.2654724121094, l2 loss: 0.1114647164940834\n",
      "iteration 1111, loss value 339.2638244628906, l2 loss: 0.11146063357591629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1112, loss value 339.26251220703125, l2 loss: 0.11145651340484619\n",
      "iteration 1113, loss value 339.2606506347656, l2 loss: 0.11145241558551788\n",
      "iteration 1114, loss value 339.2594909667969, l2 loss: 0.11144828051328659\n",
      "iteration 1115, loss value 339.2578125, l2 loss: 0.1114441528916359\n",
      "iteration 1116, loss value 339.2563171386719, l2 loss: 0.11144000291824341\n",
      "iteration 1117, loss value 339.2547302246094, l2 loss: 0.11143582314252853\n",
      "iteration 1118, loss value 339.2532043457031, l2 loss: 0.11143167316913605\n",
      "iteration 1119, loss value 339.2518005371094, l2 loss: 0.11142749339342117\n",
      "iteration 1120, loss value 339.25018310546875, l2 loss: 0.1114233061671257\n",
      "iteration 1121, loss value 339.2484130859375, l2 loss: 0.11141911894083023\n",
      "iteration 1122, loss value 339.2471923828125, l2 loss: 0.11141493171453476\n",
      "iteration 1123, loss value 339.2455749511719, l2 loss: 0.1114107072353363\n",
      "iteration 1124, loss value 339.2439270019531, l2 loss: 0.11140648275613785\n",
      "iteration 1125, loss value 339.2425537109375, l2 loss: 0.11140225827693939\n",
      "iteration 1126, loss value 339.2415466308594, l2 loss: 0.11139804124832153\n",
      "iteration 1127, loss value 339.24029541015625, l2 loss: 0.11139379441738129\n",
      "iteration 1128, loss value 339.2386474609375, l2 loss: 0.11138953268527985\n",
      "iteration 1129, loss value 339.2364196777344, l2 loss: 0.11138526350259781\n",
      "iteration 1130, loss value 339.2350769042969, l2 loss: 0.11138099431991577\n",
      "iteration 1131, loss value 339.2335205078125, l2 loss: 0.11137668788433075\n",
      "iteration 1132, loss value 339.2325134277344, l2 loss: 0.11137238889932632\n",
      "iteration 1133, loss value 339.2305603027344, l2 loss: 0.1113680973649025\n",
      "iteration 1134, loss value 339.2289123535156, l2 loss: 0.11136379092931747\n",
      "iteration 1135, loss value 339.2281799316406, l2 loss: 0.11135947704315186\n",
      "iteration 1136, loss value 339.2266845703125, l2 loss: 0.11135515570640564\n",
      "iteration 1137, loss value 339.22509765625, l2 loss: 0.11135081201791763\n",
      "iteration 1138, loss value 339.2235107421875, l2 loss: 0.11134646832942963\n",
      "iteration 1139, loss value 339.2221374511719, l2 loss: 0.11134210973978043\n",
      "iteration 1140, loss value 339.2208251953125, l2 loss: 0.11133774369955063\n",
      "iteration 1141, loss value 339.21917724609375, l2 loss: 0.11133334040641785\n",
      "iteration 1142, loss value 339.2176818847656, l2 loss: 0.11132896691560745\n",
      "iteration 1143, loss value 339.2162170410156, l2 loss: 0.11132456362247467\n",
      "iteration 1144, loss value 339.2149353027344, l2 loss: 0.11132016777992249\n",
      "iteration 1145, loss value 339.2131042480469, l2 loss: 0.1113157793879509\n",
      "iteration 1146, loss value 339.21221923828125, l2 loss: 0.11131137609481812\n",
      "iteration 1147, loss value 339.21063232421875, l2 loss: 0.11130696535110474\n",
      "iteration 1148, loss value 339.20904541015625, l2 loss: 0.11130249500274658\n",
      "iteration 1149, loss value 339.20770263671875, l2 loss: 0.11129806935787201\n",
      "iteration 1150, loss value 339.2062683105469, l2 loss: 0.11129361391067505\n",
      "iteration 1151, loss value 339.20465087890625, l2 loss: 0.11128918081521988\n",
      "iteration 1152, loss value 339.2039489746094, l2 loss: 0.11128471046686172\n",
      "iteration 1153, loss value 339.20245361328125, l2 loss: 0.11128021776676178\n",
      "iteration 1154, loss value 339.20062255859375, l2 loss: 0.11127573996782303\n",
      "iteration 1155, loss value 339.1993103027344, l2 loss: 0.11127123981714249\n",
      "iteration 1156, loss value 339.1976318359375, l2 loss: 0.11126673966646194\n",
      "iteration 1157, loss value 339.1966247558594, l2 loss: 0.11126220971345901\n",
      "iteration 1158, loss value 339.1946716308594, l2 loss: 0.1112576574087143\n",
      "iteration 1159, loss value 339.19317626953125, l2 loss: 0.11125316470861435\n",
      "iteration 1160, loss value 339.19219970703125, l2 loss: 0.11124862730503082\n",
      "iteration 1161, loss value 339.1907653808594, l2 loss: 0.1112440824508667\n",
      "iteration 1162, loss value 339.1893005371094, l2 loss: 0.11123953014612198\n",
      "iteration 1163, loss value 339.18798828125, l2 loss: 0.11123496294021606\n",
      "iteration 1164, loss value 339.18670654296875, l2 loss: 0.11123043298721313\n",
      "iteration 1165, loss value 339.1856384277344, l2 loss: 0.11122580617666245\n",
      "iteration 1166, loss value 339.1836853027344, l2 loss: 0.11122123897075653\n",
      "iteration 1167, loss value 339.1822814941406, l2 loss: 0.11121664196252823\n",
      "iteration 1168, loss value 339.1812438964844, l2 loss: 0.11121202260255814\n",
      "iteration 1169, loss value 339.1795654296875, l2 loss: 0.11120741069316864\n",
      "iteration 1170, loss value 339.1780700683594, l2 loss: 0.11120279878377914\n",
      "iteration 1171, loss value 339.1769104003906, l2 loss: 0.11119815707206726\n",
      "iteration 1172, loss value 339.175537109375, l2 loss: 0.11119354516267776\n",
      "iteration 1173, loss value 339.1742858886719, l2 loss: 0.11118891090154648\n",
      "iteration 1174, loss value 339.17291259765625, l2 loss: 0.1111842468380928\n",
      "iteration 1175, loss value 339.17181396484375, l2 loss: 0.11117959022521973\n",
      "iteration 1176, loss value 339.1704406738281, l2 loss: 0.11117491871118546\n",
      "iteration 1177, loss value 339.16900634765625, l2 loss: 0.1111702099442482\n",
      "iteration 1178, loss value 339.1675720214844, l2 loss: 0.11116555333137512\n",
      "iteration 1179, loss value 339.1661071777344, l2 loss: 0.11116087436676025\n",
      "iteration 1180, loss value 339.1648254394531, l2 loss: 0.1111561730504036\n",
      "iteration 1181, loss value 339.16363525390625, l2 loss: 0.11115147918462753\n",
      "iteration 1182, loss value 339.16192626953125, l2 loss: 0.11114675551652908\n",
      "iteration 1183, loss value 339.16094970703125, l2 loss: 0.11114200949668884\n",
      "iteration 1184, loss value 339.1590270996094, l2 loss: 0.1111372783780098\n",
      "iteration 1185, loss value 339.1582946777344, l2 loss: 0.11113254725933075\n",
      "iteration 1186, loss value 339.1568908691406, l2 loss: 0.11112780123949051\n",
      "iteration 1187, loss value 339.1553955078125, l2 loss: 0.11112307012081146\n",
      "iteration 1188, loss value 339.1543884277344, l2 loss: 0.11111830174922943\n",
      "iteration 1189, loss value 339.15283203125, l2 loss: 0.1111135184764862\n",
      "iteration 1190, loss value 339.1513977050781, l2 loss: 0.11110875755548477\n",
      "iteration 1191, loss value 339.1502685546875, l2 loss: 0.11110396683216095\n",
      "iteration 1192, loss value 339.1487731933594, l2 loss: 0.11109917610883713\n",
      "iteration 1193, loss value 339.1473083496094, l2 loss: 0.1110943853855133\n",
      "iteration 1194, loss value 339.146728515625, l2 loss: 0.1110895574092865\n",
      "iteration 1195, loss value 339.14520263671875, l2 loss: 0.11108473688364029\n",
      "iteration 1196, loss value 339.1436462402344, l2 loss: 0.11107992380857468\n",
      "iteration 1197, loss value 339.14288330078125, l2 loss: 0.11107508838176727\n",
      "iteration 1198, loss value 339.1411437988281, l2 loss: 0.11107026785612106\n",
      "iteration 1199, loss value 339.1402282714844, l2 loss: 0.11106541007757187\n",
      "iteration 1200, loss value 339.1385498046875, l2 loss: 0.11106056720018387\n",
      "iteration 1201, loss value 339.13763427734375, l2 loss: 0.11105573922395706\n",
      "iteration 1202, loss value 339.13629150390625, l2 loss: 0.11105083674192429\n",
      "iteration 1203, loss value 339.1349182128906, l2 loss: 0.1110459640622139\n",
      "iteration 1204, loss value 339.13348388671875, l2 loss: 0.1110411062836647\n",
      "iteration 1205, loss value 339.13232421875, l2 loss: 0.11103621125221252\n",
      "iteration 1206, loss value 339.1315612792969, l2 loss: 0.11103133857250214\n",
      "iteration 1207, loss value 339.130126953125, l2 loss: 0.11102640628814697\n",
      "iteration 1208, loss value 339.128662109375, l2 loss: 0.111021488904953\n",
      "iteration 1209, loss value 339.1268615722656, l2 loss: 0.11101658642292023\n",
      "iteration 1210, loss value 339.12591552734375, l2 loss: 0.11101165413856506\n",
      "iteration 1211, loss value 339.12481689453125, l2 loss: 0.1110067293047905\n",
      "iteration 1212, loss value 339.12347412109375, l2 loss: 0.11100181937217712\n",
      "iteration 1213, loss value 339.12249755859375, l2 loss: 0.11099684983491898\n",
      "iteration 1214, loss value 339.12091064453125, l2 loss: 0.11099192500114441\n",
      "iteration 1215, loss value 339.11962890625, l2 loss: 0.11098697036504745\n",
      "iteration 1216, loss value 339.1183776855469, l2 loss: 0.1109820008277893\n",
      "iteration 1217, loss value 339.1171569824219, l2 loss: 0.11097701638936996\n",
      "iteration 1218, loss value 339.1158752441406, l2 loss: 0.11097205430269241\n",
      "iteration 1219, loss value 339.1148681640625, l2 loss: 0.11096706241369247\n",
      "iteration 1220, loss value 339.11322021484375, l2 loss: 0.11096208542585373\n",
      "iteration 1221, loss value 339.112060546875, l2 loss: 0.1109570786356926\n",
      "iteration 1222, loss value 339.1110534667969, l2 loss: 0.11095205694437027\n",
      "iteration 1223, loss value 339.10980224609375, l2 loss: 0.11094705760478973\n",
      "iteration 1224, loss value 339.1080627441406, l2 loss: 0.11094203591346741\n",
      "iteration 1225, loss value 339.1070556640625, l2 loss: 0.11093703657388687\n",
      "iteration 1226, loss value 339.10601806640625, l2 loss: 0.11093201488256454\n",
      "iteration 1227, loss value 339.10504150390625, l2 loss: 0.11092691868543625\n",
      "iteration 1228, loss value 339.10333251953125, l2 loss: 0.11092192679643631\n",
      "iteration 1229, loss value 339.1025085449219, l2 loss: 0.1109168604016304\n",
      "iteration 1230, loss value 339.10125732421875, l2 loss: 0.11091180145740509\n",
      "iteration 1231, loss value 339.0994873046875, l2 loss: 0.11090675741434097\n",
      "iteration 1232, loss value 339.09881591796875, l2 loss: 0.11090167611837387\n",
      "iteration 1233, loss value 339.09759521484375, l2 loss: 0.11089659482240677\n",
      "iteration 1234, loss value 339.096435546875, l2 loss: 0.11089152097702026\n",
      "iteration 1235, loss value 339.094970703125, l2 loss: 0.11088640987873077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1236, loss value 339.0935363769531, l2 loss: 0.11088134348392487\n",
      "iteration 1237, loss value 339.0926208496094, l2 loss: 0.11087623983621597\n",
      "iteration 1238, loss value 339.09161376953125, l2 loss: 0.11087113618850708\n",
      "iteration 1239, loss value 339.09039306640625, l2 loss: 0.11086602509021759\n",
      "iteration 1240, loss value 339.08935546875, l2 loss: 0.1108609065413475\n",
      "iteration 1241, loss value 339.087890625, l2 loss: 0.11085575819015503\n",
      "iteration 1242, loss value 339.08648681640625, l2 loss: 0.11085066944360733\n",
      "iteration 1243, loss value 339.08575439453125, l2 loss: 0.11084552109241486\n",
      "iteration 1244, loss value 339.08416748046875, l2 loss: 0.11084038764238358\n",
      "iteration 1245, loss value 339.08306884765625, l2 loss: 0.1108352318406105\n",
      "iteration 1246, loss value 339.0819396972656, l2 loss: 0.11083007603883743\n",
      "iteration 1247, loss value 339.08026123046875, l2 loss: 0.11082492768764496\n",
      "iteration 1248, loss value 339.0794677734375, l2 loss: 0.1108197346329689\n",
      "iteration 1249, loss value 339.0780944824219, l2 loss: 0.11081457138061523\n",
      "iteration 1250, loss value 339.07684326171875, l2 loss: 0.11080942302942276\n",
      "iteration 1251, loss value 339.07574462890625, l2 loss: 0.11080418527126312\n",
      "iteration 1252, loss value 339.0744323730469, l2 loss: 0.11079902946949005\n",
      "iteration 1253, loss value 339.07342529296875, l2 loss: 0.11079385131597519\n",
      "iteration 1254, loss value 339.0724792480469, l2 loss: 0.11078866571187973\n",
      "iteration 1255, loss value 339.0712585449219, l2 loss: 0.11078347265720367\n",
      "iteration 1256, loss value 339.07025146484375, l2 loss: 0.11077824234962463\n",
      "iteration 1257, loss value 339.06890869140625, l2 loss: 0.11077304184436798\n",
      "iteration 1258, loss value 339.06781005859375, l2 loss: 0.11076779663562775\n",
      "iteration 1259, loss value 339.0665588378906, l2 loss: 0.1107625812292099\n",
      "iteration 1260, loss value 339.0653991699219, l2 loss: 0.11075735092163086\n",
      "iteration 1261, loss value 339.064208984375, l2 loss: 0.11075213551521301\n",
      "iteration 1262, loss value 339.06365966796875, l2 loss: 0.11074686795473099\n",
      "iteration 1263, loss value 339.061767578125, l2 loss: 0.11074162274599075\n",
      "iteration 1264, loss value 339.0605163574219, l2 loss: 0.11073639243841171\n",
      "iteration 1265, loss value 339.05975341796875, l2 loss: 0.11073113977909088\n",
      "iteration 1266, loss value 339.05859375, l2 loss: 0.11072589457035065\n",
      "iteration 1267, loss value 339.05767822265625, l2 loss: 0.11072062700986862\n",
      "iteration 1268, loss value 339.05657958984375, l2 loss: 0.110715351998806\n",
      "iteration 1269, loss value 339.0552673339844, l2 loss: 0.11071007698774338\n",
      "iteration 1270, loss value 339.05413818359375, l2 loss: 0.11070478707551956\n",
      "iteration 1271, loss value 339.0530090332031, l2 loss: 0.11069950461387634\n",
      "iteration 1272, loss value 339.0516052246094, l2 loss: 0.11069420725107193\n",
      "iteration 1273, loss value 339.0505065917969, l2 loss: 0.11068892478942871\n",
      "iteration 1274, loss value 339.0493469238281, l2 loss: 0.1106836199760437\n",
      "iteration 1275, loss value 339.04840087890625, l2 loss: 0.11067833751440048\n",
      "iteration 1276, loss value 339.0470886230469, l2 loss: 0.11067304015159607\n",
      "iteration 1277, loss value 339.0466003417969, l2 loss: 0.11066769808530807\n",
      "iteration 1278, loss value 339.04461669921875, l2 loss: 0.11066240072250366\n",
      "iteration 1279, loss value 339.0439147949219, l2 loss: 0.11065708100795746\n",
      "iteration 1280, loss value 339.042724609375, l2 loss: 0.11065173149108887\n",
      "iteration 1281, loss value 339.0417175292969, l2 loss: 0.11064640432596207\n",
      "iteration 1282, loss value 339.04071044921875, l2 loss: 0.11064108461141586\n",
      "iteration 1283, loss value 339.0392150878906, l2 loss: 0.11063572764396667\n",
      "iteration 1284, loss value 339.0381774902344, l2 loss: 0.11063037812709808\n",
      "iteration 1285, loss value 339.0374755859375, l2 loss: 0.11062503606081009\n",
      "iteration 1286, loss value 339.0362243652344, l2 loss: 0.11061970144510269\n",
      "iteration 1287, loss value 339.0351257324219, l2 loss: 0.11061432957649231\n",
      "iteration 1288, loss value 339.0340270996094, l2 loss: 0.11060898751020432\n",
      "iteration 1289, loss value 339.03289794921875, l2 loss: 0.11060360819101334\n",
      "iteration 1290, loss value 339.0317077636719, l2 loss: 0.11059824377298355\n",
      "iteration 1291, loss value 339.0304870605469, l2 loss: 0.11059288680553436\n",
      "iteration 1292, loss value 339.02984619140625, l2 loss: 0.1105874702334404\n",
      "iteration 1293, loss value 339.0279235839844, l2 loss: 0.11058211326599121\n",
      "iteration 1294, loss value 339.0274353027344, l2 loss: 0.11057671159505844\n",
      "iteration 1295, loss value 339.0261535644531, l2 loss: 0.11057133227586746\n",
      "iteration 1296, loss value 339.02484130859375, l2 loss: 0.1105659231543541\n",
      "iteration 1297, loss value 339.02435302734375, l2 loss: 0.11056054383516312\n",
      "iteration 1298, loss value 339.02313232421875, l2 loss: 0.11055511981248856\n",
      "iteration 1299, loss value 339.02166748046875, l2 loss: 0.11054974794387817\n",
      "iteration 1300, loss value 339.02069091796875, l2 loss: 0.1105443462729454\n",
      "iteration 1301, loss value 339.01953125, l2 loss: 0.11053892225027084\n",
      "iteration 1302, loss value 339.01837158203125, l2 loss: 0.11053349822759628\n",
      "iteration 1303, loss value 339.0177307128906, l2 loss: 0.11052808910608292\n",
      "iteration 1304, loss value 339.0166320800781, l2 loss: 0.11052267253398895\n",
      "iteration 1305, loss value 339.01556396484375, l2 loss: 0.1105172261595726\n",
      "iteration 1306, loss value 339.01409912109375, l2 loss: 0.11051180958747864\n",
      "iteration 1307, loss value 339.01348876953125, l2 loss: 0.11050634831190109\n",
      "iteration 1308, loss value 339.0118713378906, l2 loss: 0.11050095409154892\n",
      "iteration 1309, loss value 339.01129150390625, l2 loss: 0.11049549281597137\n",
      "iteration 1310, loss value 339.01019287109375, l2 loss: 0.11049006134271622\n",
      "iteration 1311, loss value 339.009033203125, l2 loss: 0.11048460751771927\n",
      "iteration 1312, loss value 339.0080871582031, l2 loss: 0.11047916114330292\n",
      "iteration 1313, loss value 339.0071716308594, l2 loss: 0.11047369241714478\n",
      "iteration 1314, loss value 339.005615234375, l2 loss: 0.11046825349330902\n",
      "iteration 1315, loss value 339.0052795410156, l2 loss: 0.11046278476715088\n",
      "iteration 1316, loss value 339.00372314453125, l2 loss: 0.11045730859041214\n",
      "iteration 1317, loss value 339.00250244140625, l2 loss: 0.110451839864254\n",
      "iteration 1318, loss value 339.0017395019531, l2 loss: 0.11044638603925705\n",
      "iteration 1319, loss value 339.0003967285156, l2 loss: 0.11044089496135712\n",
      "iteration 1320, loss value 338.9990234375, l2 loss: 0.11043542623519897\n",
      "iteration 1321, loss value 338.9982604980469, l2 loss: 0.11042993515729904\n",
      "iteration 1322, loss value 338.9972229003906, l2 loss: 0.11042448878288269\n",
      "iteration 1323, loss value 338.9964599609375, l2 loss: 0.11041900515556335\n",
      "iteration 1324, loss value 338.9955749511719, l2 loss: 0.11041352897882462\n",
      "iteration 1325, loss value 338.9943542480469, l2 loss: 0.11040802299976349\n",
      "iteration 1326, loss value 338.9931335449219, l2 loss: 0.11040253937244415\n",
      "iteration 1327, loss value 338.9923400878906, l2 loss: 0.11039703339338303\n",
      "iteration 1328, loss value 338.9913635253906, l2 loss: 0.11039154976606369\n",
      "iteration 1329, loss value 338.9906311035156, l2 loss: 0.11038602888584137\n",
      "iteration 1330, loss value 338.9887390136719, l2 loss: 0.11038054525852203\n",
      "iteration 1331, loss value 338.9883117675781, l2 loss: 0.11037503182888031\n",
      "iteration 1332, loss value 338.9873352050781, l2 loss: 0.11036951839923859\n",
      "iteration 1333, loss value 338.9859313964844, l2 loss: 0.11036400496959686\n",
      "iteration 1334, loss value 338.98480224609375, l2 loss: 0.11035849899053574\n",
      "iteration 1335, loss value 338.98370361328125, l2 loss: 0.11035297811031342\n",
      "iteration 1336, loss value 338.9830017089844, l2 loss: 0.11034748703241348\n",
      "iteration 1337, loss value 338.98248291015625, l2 loss: 0.11034194380044937\n",
      "iteration 1338, loss value 338.9811096191406, l2 loss: 0.11033643782138824\n",
      "iteration 1339, loss value 338.97991943359375, l2 loss: 0.11033090949058533\n",
      "iteration 1340, loss value 338.978759765625, l2 loss: 0.11032537370920181\n",
      "iteration 1341, loss value 338.9778747558594, l2 loss: 0.11031985282897949\n",
      "iteration 1342, loss value 338.97674560546875, l2 loss: 0.11031432449817657\n",
      "iteration 1343, loss value 338.9758605957031, l2 loss: 0.11030880361795425\n",
      "iteration 1344, loss value 338.9749755859375, l2 loss: 0.11030327528715134\n",
      "iteration 1345, loss value 338.9742431640625, l2 loss: 0.11029773205518723\n",
      "iteration 1346, loss value 338.9730224609375, l2 loss: 0.11029218137264252\n",
      "iteration 1347, loss value 338.9721374511719, l2 loss: 0.1102866381406784\n",
      "iteration 1348, loss value 338.9709167480469, l2 loss: 0.1102810949087143\n",
      "iteration 1349, loss value 338.96990966796875, l2 loss: 0.11027555912733078\n",
      "iteration 1350, loss value 338.96881103515625, l2 loss: 0.11027001589536667\n",
      "iteration 1351, loss value 338.9679870605469, l2 loss: 0.11026445776224136\n",
      "iteration 1352, loss value 338.9670104980469, l2 loss: 0.11025891453027725\n",
      "iteration 1353, loss value 338.96563720703125, l2 loss: 0.11025335639715195\n",
      "iteration 1354, loss value 338.96453857421875, l2 loss: 0.11024781316518784\n",
      "iteration 1355, loss value 338.963623046875, l2 loss: 0.11024224758148193\n",
      "iteration 1356, loss value 338.9628601074219, l2 loss: 0.11023671180009842\n",
      "iteration 1357, loss value 338.96185302734375, l2 loss: 0.11023114621639252\n",
      "iteration 1358, loss value 338.96063232421875, l2 loss: 0.1102256029844284\n",
      "iteration 1359, loss value 338.9599609375, l2 loss: 0.1102200448513031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1360, loss value 338.9590759277344, l2 loss: 0.1102144792675972\n",
      "iteration 1361, loss value 338.95794677734375, l2 loss: 0.11020892858505249\n",
      "iteration 1362, loss value 338.9576110839844, l2 loss: 0.11020335555076599\n",
      "iteration 1363, loss value 338.9557189941406, l2 loss: 0.11019778251647949\n",
      "iteration 1364, loss value 338.9549865722656, l2 loss: 0.11019222438335419\n",
      "iteration 1365, loss value 338.9537353515625, l2 loss: 0.11018666625022888\n",
      "iteration 1366, loss value 338.95355224609375, l2 loss: 0.11018108576536179\n",
      "iteration 1367, loss value 338.95196533203125, l2 loss: 0.11017552763223648\n",
      "iteration 1368, loss value 338.9510192871094, l2 loss: 0.11016996949911118\n",
      "iteration 1369, loss value 338.95025634765625, l2 loss: 0.11016438901424408\n",
      "iteration 1370, loss value 338.94915771484375, l2 loss: 0.11015880852937698\n",
      "iteration 1371, loss value 338.9480895996094, l2 loss: 0.11015326529741287\n",
      "iteration 1372, loss value 338.9473571777344, l2 loss: 0.11014767736196518\n",
      "iteration 1373, loss value 338.9462585449219, l2 loss: 0.11014210432767868\n",
      "iteration 1374, loss value 338.9454040527344, l2 loss: 0.11013651639223099\n",
      "iteration 1375, loss value 338.9442443847656, l2 loss: 0.11013096570968628\n",
      "iteration 1376, loss value 338.9433288574219, l2 loss: 0.11012540012598038\n",
      "iteration 1377, loss value 338.9423522949219, l2 loss: 0.11011980473995209\n",
      "iteration 1378, loss value 338.9412536621094, l2 loss: 0.11011423170566559\n",
      "iteration 1379, loss value 338.94024658203125, l2 loss: 0.1101086363196373\n",
      "iteration 1380, loss value 338.9388732910156, l2 loss: 0.1101030707359314\n",
      "iteration 1381, loss value 338.93841552734375, l2 loss: 0.1100975051522255\n",
      "iteration 1382, loss value 338.93768310546875, l2 loss: 0.11009193956851959\n",
      "iteration 1383, loss value 338.9363098144531, l2 loss: 0.11008637398481369\n",
      "iteration 1384, loss value 338.9356384277344, l2 loss: 0.11008080095052719\n",
      "iteration 1385, loss value 338.93487548828125, l2 loss: 0.11007518321275711\n",
      "iteration 1386, loss value 338.93389892578125, l2 loss: 0.1100696250796318\n",
      "iteration 1387, loss value 338.9327392578125, l2 loss: 0.11006403714418411\n",
      "iteration 1388, loss value 338.93218994140625, l2 loss: 0.11005847156047821\n",
      "iteration 1389, loss value 338.93096923828125, l2 loss: 0.11005286127328873\n",
      "iteration 1390, loss value 338.9299621582031, l2 loss: 0.11004731059074402\n",
      "iteration 1391, loss value 338.92913818359375, l2 loss: 0.11004175245761871\n",
      "iteration 1392, loss value 338.92822265625, l2 loss: 0.11003614217042923\n",
      "iteration 1393, loss value 338.9270935058594, l2 loss: 0.11003058403730392\n",
      "iteration 1394, loss value 338.9262390136719, l2 loss: 0.11002500355243683\n",
      "iteration 1395, loss value 338.92535400390625, l2 loss: 0.11001938581466675\n",
      "iteration 1396, loss value 338.9245300292969, l2 loss: 0.11001384258270264\n",
      "iteration 1397, loss value 338.92376708984375, l2 loss: 0.11000826209783554\n",
      "iteration 1398, loss value 338.9228210449219, l2 loss: 0.11000265926122665\n",
      "iteration 1399, loss value 338.9214782714844, l2 loss: 0.10999711602926254\n",
      "iteration 1400, loss value 338.9210510253906, l2 loss: 0.10999152809381485\n",
      "iteration 1401, loss value 338.9198913574219, l2 loss: 0.10998593270778656\n",
      "iteration 1402, loss value 338.91864013671875, l2 loss: 0.10998037457466125\n",
      "iteration 1403, loss value 338.9180603027344, l2 loss: 0.10997477173805237\n",
      "iteration 1404, loss value 338.9171142578125, l2 loss: 0.10996922105550766\n",
      "iteration 1405, loss value 338.91650390625, l2 loss: 0.10996364802122116\n",
      "iteration 1406, loss value 338.91522216796875, l2 loss: 0.10995806753635406\n",
      "iteration 1407, loss value 338.91424560546875, l2 loss: 0.10995251685380936\n",
      "iteration 1408, loss value 338.91357421875, l2 loss: 0.10994692891836166\n",
      "iteration 1409, loss value 338.91241455078125, l2 loss: 0.10994137078523636\n",
      "iteration 1410, loss value 338.9118347167969, l2 loss: 0.10993580520153046\n",
      "iteration 1411, loss value 338.9109191894531, l2 loss: 0.10993020236492157\n",
      "iteration 1412, loss value 338.9093933105469, l2 loss: 0.10992464423179626\n",
      "iteration 1413, loss value 338.90875244140625, l2 loss: 0.10991908609867096\n",
      "iteration 1414, loss value 338.9076843261719, l2 loss: 0.10991351306438446\n",
      "iteration 1415, loss value 338.9070129394531, l2 loss: 0.10990793257951736\n",
      "iteration 1416, loss value 338.90582275390625, l2 loss: 0.10990241169929504\n",
      "iteration 1417, loss value 338.90557861328125, l2 loss: 0.10989682376384735\n",
      "iteration 1418, loss value 338.90386962890625, l2 loss: 0.10989125818014145\n",
      "iteration 1419, loss value 338.9033508300781, l2 loss: 0.10988570004701614\n",
      "iteration 1420, loss value 338.9024353027344, l2 loss: 0.10988014936447144\n",
      "iteration 1421, loss value 338.9018249511719, l2 loss: 0.10987459868192673\n",
      "iteration 1422, loss value 338.90118408203125, l2 loss: 0.10986901074647903\n",
      "iteration 1423, loss value 338.89959716796875, l2 loss: 0.10986345261335373\n",
      "iteration 1424, loss value 338.8988952636719, l2 loss: 0.10985789448022842\n",
      "iteration 1425, loss value 338.8980407714844, l2 loss: 0.10985233634710312\n",
      "iteration 1426, loss value 338.8970642089844, l2 loss: 0.1098468005657196\n",
      "iteration 1427, loss value 338.89605712890625, l2 loss: 0.1098412349820137\n",
      "iteration 1428, loss value 338.8950500488281, l2 loss: 0.10983571410179138\n",
      "iteration 1429, loss value 338.89459228515625, l2 loss: 0.10983014106750488\n",
      "iteration 1430, loss value 338.89306640625, l2 loss: 0.10982462018728256\n",
      "iteration 1431, loss value 338.8927001953125, l2 loss: 0.10981905460357666\n",
      "iteration 1432, loss value 338.8917541503906, l2 loss: 0.10981351882219315\n",
      "iteration 1433, loss value 338.8905944824219, l2 loss: 0.10980799049139023\n",
      "iteration 1434, loss value 338.8897399902344, l2 loss: 0.10980244725942612\n",
      "iteration 1435, loss value 338.8890686035156, l2 loss: 0.109796904027462\n",
      "iteration 1436, loss value 338.8880615234375, l2 loss: 0.1097913533449173\n",
      "iteration 1437, loss value 338.8869323730469, l2 loss: 0.10978584736585617\n",
      "iteration 1438, loss value 338.8866882324219, l2 loss: 0.10978029668331146\n",
      "iteration 1439, loss value 338.88507080078125, l2 loss: 0.10977479815483093\n",
      "iteration 1440, loss value 338.8849792480469, l2 loss: 0.10976924747228622\n",
      "iteration 1441, loss value 338.8835144042969, l2 loss: 0.1097637191414833\n",
      "iteration 1442, loss value 338.8829040527344, l2 loss: 0.10975819081068039\n",
      "iteration 1443, loss value 338.88201904296875, l2 loss: 0.10975268483161926\n",
      "iteration 1444, loss value 338.88079833984375, l2 loss: 0.10974717140197754\n",
      "iteration 1445, loss value 338.88031005859375, l2 loss: 0.10974166542291641\n",
      "iteration 1446, loss value 338.8797607421875, l2 loss: 0.1097361221909523\n",
      "iteration 1447, loss value 338.8785400390625, l2 loss: 0.10973060876131058\n",
      "iteration 1448, loss value 338.87725830078125, l2 loss: 0.10972511023283005\n",
      "iteration 1449, loss value 338.8763732910156, l2 loss: 0.10971958935260773\n",
      "iteration 1450, loss value 338.8758239746094, l2 loss: 0.10971410572528839\n",
      "iteration 1451, loss value 338.8748779296875, l2 loss: 0.10970858484506607\n",
      "iteration 1452, loss value 338.8742370605469, l2 loss: 0.10970307886600494\n",
      "iteration 1453, loss value 338.8732604980469, l2 loss: 0.1096976026892662\n",
      "iteration 1454, loss value 338.87237548828125, l2 loss: 0.10969209671020508\n",
      "iteration 1455, loss value 338.8714294433594, l2 loss: 0.10968659818172455\n",
      "iteration 1456, loss value 338.87030029296875, l2 loss: 0.10968110710382462\n",
      "iteration 1457, loss value 338.86962890625, l2 loss: 0.10967560857534409\n",
      "iteration 1458, loss value 338.8686218261719, l2 loss: 0.10967013984918594\n",
      "iteration 1459, loss value 338.86810302734375, l2 loss: 0.10966465622186661\n",
      "iteration 1460, loss value 338.86700439453125, l2 loss: 0.10965918004512787\n",
      "iteration 1461, loss value 338.8665466308594, l2 loss: 0.10965371876955032\n",
      "iteration 1462, loss value 338.86572265625, l2 loss: 0.1096482053399086\n",
      "iteration 1463, loss value 338.86456298828125, l2 loss: 0.10964274406433105\n",
      "iteration 1464, loss value 338.8636169433594, l2 loss: 0.10963728278875351\n",
      "iteration 1465, loss value 338.8629455566406, l2 loss: 0.10963183641433716\n",
      "iteration 1466, loss value 338.8622741699219, l2 loss: 0.10962636023759842\n",
      "iteration 1467, loss value 338.8609924316406, l2 loss: 0.10962090641260147\n",
      "iteration 1468, loss value 338.8606262207031, l2 loss: 0.10961546003818512\n",
      "iteration 1469, loss value 338.8594970703125, l2 loss: 0.10960998386144638\n",
      "iteration 1470, loss value 338.8586120605469, l2 loss: 0.10960454493761063\n",
      "iteration 1471, loss value 338.85809326171875, l2 loss: 0.10959909111261368\n",
      "iteration 1472, loss value 338.8570556640625, l2 loss: 0.10959362983703613\n",
      "iteration 1473, loss value 338.8555603027344, l2 loss: 0.10958822071552277\n",
      "iteration 1474, loss value 338.85528564453125, l2 loss: 0.10958278179168701\n",
      "iteration 1475, loss value 338.85455322265625, l2 loss: 0.10957734286785126\n",
      "iteration 1476, loss value 338.8534240722656, l2 loss: 0.1095718964934349\n",
      "iteration 1477, loss value 338.8529357910156, l2 loss: 0.10956646502017975\n",
      "iteration 1478, loss value 338.8519592285156, l2 loss: 0.10956105589866638\n",
      "iteration 1479, loss value 338.85101318359375, l2 loss: 0.1095556691288948\n",
      "iteration 1480, loss value 338.85028076171875, l2 loss: 0.10955022275447845\n",
      "iteration 1481, loss value 338.8492431640625, l2 loss: 0.10954483598470688\n",
      "iteration 1482, loss value 338.84893798828125, l2 loss: 0.10953940451145172\n",
      "iteration 1483, loss value 338.8476867675781, l2 loss: 0.10953399538993835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1484, loss value 338.8468933105469, l2 loss: 0.10952860862016678\n",
      "iteration 1485, loss value 338.84588623046875, l2 loss: 0.10952318459749222\n",
      "iteration 1486, loss value 338.8447570800781, l2 loss: 0.10951782017946243\n",
      "iteration 1487, loss value 338.84454345703125, l2 loss: 0.10951242595911026\n",
      "iteration 1488, loss value 338.8436584472656, l2 loss: 0.10950704663991928\n",
      "iteration 1489, loss value 338.8429260253906, l2 loss: 0.1095016747713089\n",
      "iteration 1490, loss value 338.8416748046875, l2 loss: 0.10949628055095673\n",
      "iteration 1491, loss value 338.8409118652344, l2 loss: 0.10949092358350754\n",
      "iteration 1492, loss value 338.8404235839844, l2 loss: 0.10948555171489716\n",
      "iteration 1493, loss value 338.8392028808594, l2 loss: 0.10948017239570618\n",
      "iteration 1494, loss value 338.8387451171875, l2 loss: 0.10947481542825699\n",
      "iteration 1495, loss value 338.837890625, l2 loss: 0.10946948826313019\n",
      "iteration 1496, loss value 338.83746337890625, l2 loss: 0.1094641163945198\n",
      "iteration 1497, loss value 338.8365173339844, l2 loss: 0.10945877432823181\n",
      "iteration 1498, loss value 338.8355712890625, l2 loss: 0.10945341736078262\n",
      "iteration 1499, loss value 338.8341979980469, l2 loss: 0.10944810509681702\n",
      "iteration 1500, loss value 338.83367919921875, l2 loss: 0.10944275557994843\n",
      "iteration 1501, loss value 338.83294677734375, l2 loss: 0.10943743586540222\n",
      "iteration 1502, loss value 338.83221435546875, l2 loss: 0.10943210124969482\n",
      "iteration 1503, loss value 338.83154296875, l2 loss: 0.10942679643630981\n",
      "iteration 1504, loss value 338.83087158203125, l2 loss: 0.10942147672176361\n",
      "iteration 1505, loss value 338.8298645019531, l2 loss: 0.10941614955663681\n",
      "iteration 1506, loss value 338.82861328125, l2 loss: 0.10941088199615479\n",
      "iteration 1507, loss value 338.8282775878906, l2 loss: 0.10940555483102798\n",
      "iteration 1508, loss value 338.8270263671875, l2 loss: 0.10940027236938477\n",
      "iteration 1509, loss value 338.8270568847656, l2 loss: 0.10939497500658035\n",
      "iteration 1510, loss value 338.82574462890625, l2 loss: 0.10938969254493713\n",
      "iteration 1511, loss value 338.8252868652344, l2 loss: 0.10938438773155212\n",
      "iteration 1512, loss value 338.8240661621094, l2 loss: 0.1093791052699089\n",
      "iteration 1513, loss value 338.823486328125, l2 loss: 0.10937384516000748\n",
      "iteration 1514, loss value 338.8226318359375, l2 loss: 0.10936854779720306\n",
      "iteration 1515, loss value 338.8217468261719, l2 loss: 0.10936328768730164\n",
      "iteration 1516, loss value 338.82110595703125, l2 loss: 0.1093580350279808\n",
      "iteration 1517, loss value 338.8200378417969, l2 loss: 0.10935277491807938\n",
      "iteration 1518, loss value 338.81927490234375, l2 loss: 0.10934751480817795\n",
      "iteration 1519, loss value 338.81805419921875, l2 loss: 0.1093422919511795\n",
      "iteration 1520, loss value 338.8179931640625, l2 loss: 0.10933704674243927\n",
      "iteration 1521, loss value 338.81707763671875, l2 loss: 0.10933180153369904\n",
      "iteration 1522, loss value 338.816162109375, l2 loss: 0.10932659357786179\n",
      "iteration 1523, loss value 338.8157043457031, l2 loss: 0.10932137817144394\n",
      "iteration 1524, loss value 338.8148193359375, l2 loss: 0.1093161478638649\n",
      "iteration 1525, loss value 338.81378173828125, l2 loss: 0.10931094735860825\n",
      "iteration 1526, loss value 338.81341552734375, l2 loss: 0.1093057245016098\n",
      "iteration 1527, loss value 338.81243896484375, l2 loss: 0.10930053144693375\n",
      "iteration 1528, loss value 338.81158447265625, l2 loss: 0.10929534584283829\n",
      "iteration 1529, loss value 338.81109619140625, l2 loss: 0.10929015278816223\n",
      "iteration 1530, loss value 338.8102722167969, l2 loss: 0.10928497463464737\n",
      "iteration 1531, loss value 338.8090515136719, l2 loss: 0.10927978903055191\n",
      "iteration 1532, loss value 338.80828857421875, l2 loss: 0.10927461087703705\n",
      "iteration 1533, loss value 338.8074035644531, l2 loss: 0.10926943272352219\n",
      "iteration 1534, loss value 338.8066711425781, l2 loss: 0.10926428437232971\n",
      "iteration 1535, loss value 338.80621337890625, l2 loss: 0.10925915092229843\n",
      "iteration 1536, loss value 338.8055114746094, l2 loss: 0.10925398021936417\n",
      "iteration 1537, loss value 338.8045349121094, l2 loss: 0.10924884676933289\n",
      "iteration 1538, loss value 338.80401611328125, l2 loss: 0.10924368351697922\n",
      "iteration 1539, loss value 338.8032531738281, l2 loss: 0.10923856496810913\n",
      "iteration 1540, loss value 338.80206298828125, l2 loss: 0.10923343151807785\n",
      "iteration 1541, loss value 338.8011474609375, l2 loss: 0.10922830551862717\n",
      "iteration 1542, loss value 338.8001708984375, l2 loss: 0.10922319442033768\n",
      "iteration 1543, loss value 338.80010986328125, l2 loss: 0.109218068420887\n",
      "iteration 1544, loss value 338.7988586425781, l2 loss: 0.10921300202608109\n",
      "iteration 1545, loss value 338.7982177734375, l2 loss: 0.1092078760266304\n",
      "iteration 1546, loss value 338.7976379394531, l2 loss: 0.1092027947306633\n",
      "iteration 1547, loss value 338.79681396484375, l2 loss: 0.1091977134346962\n",
      "iteration 1548, loss value 338.7959899902344, l2 loss: 0.1091926246881485\n",
      "iteration 1549, loss value 338.794921875, l2 loss: 0.10918755829334259\n",
      "iteration 1550, loss value 338.7942810058594, l2 loss: 0.10918247699737549\n",
      "iteration 1551, loss value 338.7930908203125, l2 loss: 0.10917742550373077\n",
      "iteration 1552, loss value 338.7925720214844, l2 loss: 0.10917239636182785\n",
      "iteration 1553, loss value 338.791748046875, l2 loss: 0.10916733741760254\n",
      "iteration 1554, loss value 338.79119873046875, l2 loss: 0.1091623306274414\n",
      "iteration 1555, loss value 338.7908630371094, l2 loss: 0.10915727913379669\n",
      "iteration 1556, loss value 338.7897033691406, l2 loss: 0.10915225744247437\n",
      "iteration 1557, loss value 338.7892150878906, l2 loss: 0.10914722084999084\n",
      "iteration 1558, loss value 338.78826904296875, l2 loss: 0.10914221405982971\n",
      "iteration 1559, loss value 338.7872009277344, l2 loss: 0.10913721472024918\n",
      "iteration 1560, loss value 338.7864990234375, l2 loss: 0.10913223028182983\n",
      "iteration 1561, loss value 338.78594970703125, l2 loss: 0.1091272383928299\n",
      "iteration 1562, loss value 338.7856140136719, l2 loss: 0.10912226140499115\n",
      "iteration 1563, loss value 338.78485107421875, l2 loss: 0.10911726206541061\n",
      "iteration 1564, loss value 338.7840270996094, l2 loss: 0.10911225527524948\n",
      "iteration 1565, loss value 338.7830505371094, l2 loss: 0.10910730063915253\n",
      "iteration 1566, loss value 338.7821350097656, l2 loss: 0.10910235345363617\n",
      "iteration 1567, loss value 338.78125, l2 loss: 0.10909741371870041\n",
      "iteration 1568, loss value 338.78094482421875, l2 loss: 0.10909245908260345\n",
      "iteration 1569, loss value 338.7801513671875, l2 loss: 0.10908753424882889\n",
      "iteration 1570, loss value 338.779541015625, l2 loss: 0.10908257961273193\n",
      "iteration 1571, loss value 338.7784118652344, l2 loss: 0.10907764732837677\n",
      "iteration 1572, loss value 338.77734375, l2 loss: 0.10907275229692459\n",
      "iteration 1573, loss value 338.7770080566406, l2 loss: 0.10906783491373062\n",
      "iteration 1574, loss value 338.7761535644531, l2 loss: 0.10906292498111725\n",
      "iteration 1575, loss value 338.775634765625, l2 loss: 0.10905802994966507\n",
      "iteration 1576, loss value 338.7748718261719, l2 loss: 0.10905313491821289\n",
      "iteration 1577, loss value 338.77410888671875, l2 loss: 0.1090482547879219\n",
      "iteration 1578, loss value 338.7731018066406, l2 loss: 0.10904338210821152\n",
      "iteration 1579, loss value 338.77239990234375, l2 loss: 0.10903852432966232\n",
      "iteration 1580, loss value 338.7719421386719, l2 loss: 0.10903367400169373\n",
      "iteration 1581, loss value 338.77105712890625, l2 loss: 0.10902883112430573\n",
      "iteration 1582, loss value 338.770751953125, l2 loss: 0.10902397334575653\n",
      "iteration 1583, loss value 338.77008056640625, l2 loss: 0.10901912301778793\n",
      "iteration 1584, loss value 338.76885986328125, l2 loss: 0.10901425778865814\n",
      "iteration 1585, loss value 338.7677917480469, l2 loss: 0.10900945961475372\n",
      "iteration 1586, loss value 338.76763916015625, l2 loss: 0.1090046614408493\n",
      "iteration 1587, loss value 338.7669677734375, l2 loss: 0.1089998260140419\n",
      "iteration 1588, loss value 338.76580810546875, l2 loss: 0.10899504274129868\n",
      "iteration 1589, loss value 338.7653503417969, l2 loss: 0.10899025946855545\n",
      "iteration 1590, loss value 338.7647399902344, l2 loss: 0.10898545384407043\n",
      "iteration 1591, loss value 338.76385498046875, l2 loss: 0.1089807078242302\n",
      "iteration 1592, loss value 338.76287841796875, l2 loss: 0.10897590965032578\n",
      "iteration 1593, loss value 338.7627258300781, l2 loss: 0.10897116363048553\n",
      "iteration 1594, loss value 338.7616882324219, l2 loss: 0.1089664101600647\n",
      "iteration 1595, loss value 338.7608337402344, l2 loss: 0.10896166414022446\n",
      "iteration 1596, loss value 338.7602844238281, l2 loss: 0.10895692557096481\n",
      "iteration 1597, loss value 338.75946044921875, l2 loss: 0.10895219445228577\n",
      "iteration 1598, loss value 338.7588806152344, l2 loss: 0.10894747823476791\n",
      "iteration 1599, loss value 338.7579650878906, l2 loss: 0.10894276946783066\n",
      "iteration 1600, loss value 338.7572937011719, l2 loss: 0.1089380532503128\n",
      "iteration 1601, loss value 338.7572326660156, l2 loss: 0.10893335938453674\n",
      "iteration 1602, loss value 338.7560729980469, l2 loss: 0.10892865806818008\n",
      "iteration 1603, loss value 338.7549743652344, l2 loss: 0.10892397910356522\n",
      "iteration 1604, loss value 338.7543029785156, l2 loss: 0.10891932994127274\n",
      "iteration 1605, loss value 338.7541809082031, l2 loss: 0.10891465097665787\n",
      "iteration 1606, loss value 338.75299072265625, l2 loss: 0.1089099869132042\n",
      "iteration 1607, loss value 338.7522277832031, l2 loss: 0.1089053526520729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1608, loss value 338.75128173828125, l2 loss: 0.10890070348978043\n",
      "iteration 1609, loss value 338.7505798339844, l2 loss: 0.10889606922864914\n",
      "iteration 1610, loss value 338.74993896484375, l2 loss: 0.10889145731925964\n",
      "iteration 1611, loss value 338.7496032714844, l2 loss: 0.10888682305812836\n",
      "iteration 1612, loss value 338.74847412109375, l2 loss: 0.10888223350048065\n",
      "iteration 1613, loss value 338.7480163574219, l2 loss: 0.10887762159109116\n",
      "iteration 1614, loss value 338.7472839355469, l2 loss: 0.10887302458286285\n",
      "iteration 1615, loss value 338.74652099609375, l2 loss: 0.10886844992637634\n",
      "iteration 1616, loss value 338.7455139160156, l2 loss: 0.10886386781930923\n",
      "iteration 1617, loss value 338.7448425292969, l2 loss: 0.10885933041572571\n",
      "iteration 1618, loss value 338.7445983886719, l2 loss: 0.10885477066040039\n",
      "iteration 1619, loss value 338.74383544921875, l2 loss: 0.10885024070739746\n",
      "iteration 1620, loss value 338.7431335449219, l2 loss: 0.10884569585323334\n",
      "iteration 1621, loss value 338.7422790527344, l2 loss: 0.10884115099906921\n",
      "iteration 1622, loss value 338.74151611328125, l2 loss: 0.10883663594722748\n",
      "iteration 1623, loss value 338.7408142089844, l2 loss: 0.10883212089538574\n",
      "iteration 1624, loss value 338.7400817871094, l2 loss: 0.10882759094238281\n",
      "iteration 1625, loss value 338.73931884765625, l2 loss: 0.10882310569286346\n",
      "iteration 1626, loss value 338.7386169433594, l2 loss: 0.1088186725974083\n",
      "iteration 1627, loss value 338.7384338378906, l2 loss: 0.10881417244672775\n",
      "iteration 1628, loss value 338.7373962402344, l2 loss: 0.1088097020983696\n",
      "iteration 1629, loss value 338.7368469238281, l2 loss: 0.10880523175001144\n",
      "iteration 1630, loss value 338.736083984375, l2 loss: 0.10880079865455627\n",
      "iteration 1631, loss value 338.73516845703125, l2 loss: 0.10879635065793991\n",
      "iteration 1632, loss value 338.7345886230469, l2 loss: 0.10879192501306534\n",
      "iteration 1633, loss value 338.73382568359375, l2 loss: 0.10878752171993256\n",
      "iteration 1634, loss value 338.73345947265625, l2 loss: 0.10878308862447739\n",
      "iteration 1635, loss value 338.73272705078125, l2 loss: 0.1087786927819252\n",
      "iteration 1636, loss value 338.7319030761719, l2 loss: 0.10877427458763123\n",
      "iteration 1637, loss value 338.7305603027344, l2 loss: 0.10876991599798203\n",
      "iteration 1638, loss value 338.730712890625, l2 loss: 0.10876552760601044\n",
      "iteration 1639, loss value 338.7297058105469, l2 loss: 0.10876115411520004\n",
      "iteration 1640, loss value 338.7285461425781, l2 loss: 0.10875680297613144\n",
      "iteration 1641, loss value 338.72808837890625, l2 loss: 0.10875244438648224\n",
      "iteration 1642, loss value 338.72747802734375, l2 loss: 0.10874812304973602\n",
      "iteration 1643, loss value 338.7270202636719, l2 loss: 0.10874380171298981\n",
      "iteration 1644, loss value 338.72613525390625, l2 loss: 0.1087394654750824\n",
      "iteration 1645, loss value 338.7253112792969, l2 loss: 0.10873514413833618\n",
      "iteration 1646, loss value 338.7247619628906, l2 loss: 0.10873084515333176\n",
      "iteration 1647, loss value 338.7239074707031, l2 loss: 0.10872658342123032\n",
      "iteration 1648, loss value 338.7238464355469, l2 loss: 0.1087222769856453\n",
      "iteration 1649, loss value 338.72308349609375, l2 loss: 0.10871801525354385\n",
      "iteration 1650, loss value 338.72216796875, l2 loss: 0.10871374607086182\n",
      "iteration 1651, loss value 338.7212829589844, l2 loss: 0.10870948433876038\n",
      "iteration 1652, loss value 338.7207946777344, l2 loss: 0.10870525240898132\n",
      "iteration 1653, loss value 338.7201843261719, l2 loss: 0.10870100557804108\n",
      "iteration 1654, loss value 338.7193908691406, l2 loss: 0.10869676619768143\n",
      "iteration 1655, loss value 338.7184143066406, l2 loss: 0.10869255661964417\n",
      "iteration 1656, loss value 338.7178649902344, l2 loss: 0.1086883693933487\n",
      "iteration 1657, loss value 338.7176513671875, l2 loss: 0.10868414491415024\n",
      "iteration 1658, loss value 338.7166442871094, l2 loss: 0.10867998003959656\n",
      "iteration 1659, loss value 338.7162170410156, l2 loss: 0.1086757704615593\n",
      "iteration 1660, loss value 338.7150573730469, l2 loss: 0.1086716279387474\n",
      "iteration 1661, loss value 338.71453857421875, l2 loss: 0.10866746306419373\n",
      "iteration 1662, loss value 338.7138366699219, l2 loss: 0.10866334289312363\n",
      "iteration 1663, loss value 338.71343994140625, l2 loss: 0.10865918546915054\n",
      "iteration 1664, loss value 338.71246337890625, l2 loss: 0.10865505784749985\n",
      "iteration 1665, loss value 338.7123107910156, l2 loss: 0.10865094512701035\n",
      "iteration 1666, loss value 338.7110595703125, l2 loss: 0.10864683985710144\n",
      "iteration 1667, loss value 338.7107238769531, l2 loss: 0.10864274203777313\n",
      "iteration 1668, loss value 338.70977783203125, l2 loss: 0.10863862931728363\n",
      "iteration 1669, loss value 338.7087097167969, l2 loss: 0.1086345687508583\n",
      "iteration 1670, loss value 338.7084045410156, l2 loss: 0.10863052308559418\n",
      "iteration 1671, loss value 338.7078552246094, l2 loss: 0.10862646996974945\n",
      "iteration 1672, loss value 338.7071533203125, l2 loss: 0.10862242430448532\n",
      "iteration 1673, loss value 338.7064208984375, l2 loss: 0.10861838608980179\n",
      "iteration 1674, loss value 338.70599365234375, l2 loss: 0.10861434787511826\n",
      "iteration 1675, loss value 338.70513916015625, l2 loss: 0.10861034691333771\n",
      "iteration 1676, loss value 338.7049255371094, l2 loss: 0.10860631614923477\n",
      "iteration 1677, loss value 338.7035827636719, l2 loss: 0.10860233008861542\n",
      "iteration 1678, loss value 338.7032775878906, l2 loss: 0.10859835147857666\n",
      "iteration 1679, loss value 338.7026062011719, l2 loss: 0.1085943803191185\n",
      "iteration 1680, loss value 338.7025146484375, l2 loss: 0.10859040170907974\n",
      "iteration 1681, loss value 338.7015075683594, l2 loss: 0.10858644545078278\n",
      "iteration 1682, loss value 338.7005920410156, l2 loss: 0.1085825115442276\n",
      "iteration 1683, loss value 338.70037841796875, l2 loss: 0.10857857018709183\n",
      "iteration 1684, loss value 338.6994934082031, l2 loss: 0.10857464373111725\n",
      "iteration 1685, loss value 338.6983947753906, l2 loss: 0.10857071727514267\n",
      "iteration 1686, loss value 338.6974792480469, l2 loss: 0.10856683552265167\n",
      "iteration 1687, loss value 338.6976013183594, l2 loss: 0.10856295377016068\n",
      "iteration 1688, loss value 338.69677734375, l2 loss: 0.10855905711650848\n",
      "iteration 1689, loss value 338.6957092285156, l2 loss: 0.10855519026517868\n",
      "iteration 1690, loss value 338.69549560546875, l2 loss: 0.10855132341384888\n",
      "iteration 1691, loss value 338.6943359375, l2 loss: 0.10854747891426086\n",
      "iteration 1692, loss value 338.6939392089844, l2 loss: 0.10854365676641464\n",
      "iteration 1693, loss value 338.6936950683594, l2 loss: 0.10853981226682663\n",
      "iteration 1694, loss value 338.6930847167969, l2 loss: 0.10853598266839981\n",
      "iteration 1695, loss value 338.6918029785156, l2 loss: 0.10853218287229538\n",
      "iteration 1696, loss value 338.69134521484375, l2 loss: 0.10852837562561035\n",
      "iteration 1697, loss value 338.69091796875, l2 loss: 0.10852459818124771\n",
      "iteration 1698, loss value 338.6897888183594, l2 loss: 0.10852081328630447\n",
      "iteration 1699, loss value 338.6894836425781, l2 loss: 0.10851705074310303\n",
      "iteration 1700, loss value 338.6886901855469, l2 loss: 0.10851332545280457\n",
      "iteration 1701, loss value 338.6884460449219, l2 loss: 0.10850957781076431\n",
      "iteration 1702, loss value 338.6877136230469, l2 loss: 0.10850583761930466\n",
      "iteration 1703, loss value 338.6869201660156, l2 loss: 0.10850212723016739\n",
      "iteration 1704, loss value 338.68597412109375, l2 loss: 0.10849840939044952\n",
      "iteration 1705, loss value 338.6855163574219, l2 loss: 0.10849468410015106\n",
      "iteration 1706, loss value 338.6847229003906, l2 loss: 0.10849101096391678\n",
      "iteration 1707, loss value 338.68426513671875, l2 loss: 0.10848734527826309\n",
      "iteration 1708, loss value 338.6840515136719, l2 loss: 0.1084836795926094\n",
      "iteration 1709, loss value 338.68292236328125, l2 loss: 0.10848002880811691\n",
      "iteration 1710, loss value 338.68243408203125, l2 loss: 0.10847639292478561\n",
      "iteration 1711, loss value 338.6815185546875, l2 loss: 0.10847272723913193\n",
      "iteration 1712, loss value 338.6807861328125, l2 loss: 0.10846912860870361\n",
      "iteration 1713, loss value 338.6799011230469, l2 loss: 0.1084655299782753\n",
      "iteration 1714, loss value 338.67987060546875, l2 loss: 0.1084619015455246\n",
      "iteration 1715, loss value 338.6789245605469, l2 loss: 0.10845834016799927\n",
      "iteration 1716, loss value 338.67877197265625, l2 loss: 0.10845472663640976\n",
      "iteration 1717, loss value 338.6776123046875, l2 loss: 0.10845118761062622\n",
      "iteration 1718, loss value 338.67706298828125, l2 loss: 0.10844764858484268\n",
      "iteration 1719, loss value 338.67681884765625, l2 loss: 0.10844408720731735\n",
      "iteration 1720, loss value 338.6759033203125, l2 loss: 0.1084405854344368\n",
      "iteration 1721, loss value 338.6752624511719, l2 loss: 0.10843703150749207\n",
      "iteration 1722, loss value 338.674560546875, l2 loss: 0.10843352973461151\n",
      "iteration 1723, loss value 338.6738586425781, l2 loss: 0.10843005031347275\n",
      "iteration 1724, loss value 338.6736755371094, l2 loss: 0.10842657834291458\n",
      "iteration 1725, loss value 338.6726989746094, l2 loss: 0.10842306166887283\n",
      "iteration 1726, loss value 338.6719055175781, l2 loss: 0.10841960459947586\n",
      "iteration 1727, loss value 338.6717224121094, l2 loss: 0.10841615498065948\n",
      "iteration 1728, loss value 338.6709289550781, l2 loss: 0.1084127426147461\n",
      "iteration 1729, loss value 338.670654296875, l2 loss: 0.10840930044651031\n",
      "iteration 1730, loss value 338.6695251464844, l2 loss: 0.10840586572885513\n",
      "iteration 1731, loss value 338.6689147949219, l2 loss: 0.10840249806642532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1732, loss value 338.6686706542969, l2 loss: 0.10839907079935074\n",
      "iteration 1733, loss value 338.66778564453125, l2 loss: 0.10839569568634033\n",
      "iteration 1734, loss value 338.66693115234375, l2 loss: 0.10839234292507172\n",
      "iteration 1735, loss value 338.66680908203125, l2 loss: 0.10838896036148071\n",
      "iteration 1736, loss value 338.6653747558594, l2 loss: 0.1083856150507927\n",
      "iteration 1737, loss value 338.6653747558594, l2 loss: 0.10838228464126587\n",
      "iteration 1738, loss value 338.6647644042969, l2 loss: 0.10837893933057785\n",
      "iteration 1739, loss value 338.6636047363281, l2 loss: 0.10837563872337341\n",
      "iteration 1740, loss value 338.6634521484375, l2 loss: 0.10837232321500778\n",
      "iteration 1741, loss value 338.6628112792969, l2 loss: 0.10836905241012573\n",
      "iteration 1742, loss value 338.6620788574219, l2 loss: 0.10836575925350189\n",
      "iteration 1743, loss value 338.6614074707031, l2 loss: 0.10836251080036163\n",
      "iteration 1744, loss value 338.660888671875, l2 loss: 0.10835926234722137\n",
      "iteration 1745, loss value 338.66021728515625, l2 loss: 0.10835601389408112\n",
      "iteration 1746, loss value 338.6595458984375, l2 loss: 0.10835278779268265\n",
      "iteration 1747, loss value 338.65911865234375, l2 loss: 0.10834957659244537\n",
      "iteration 1748, loss value 338.6585998535156, l2 loss: 0.1083463802933693\n",
      "iteration 1749, loss value 338.65789794921875, l2 loss: 0.10834316909313202\n",
      "iteration 1750, loss value 338.65704345703125, l2 loss: 0.10833998769521713\n",
      "iteration 1751, loss value 338.65679931640625, l2 loss: 0.10833679139614105\n",
      "iteration 1752, loss value 338.6555480957031, l2 loss: 0.10833362489938736\n",
      "iteration 1753, loss value 338.6549987792969, l2 loss: 0.10833049565553665\n",
      "iteration 1754, loss value 338.65484619140625, l2 loss: 0.10832734405994415\n",
      "iteration 1755, loss value 338.6539001464844, l2 loss: 0.10832425206899643\n",
      "iteration 1756, loss value 338.65386962890625, l2 loss: 0.10832113027572632\n",
      "iteration 1757, loss value 338.65289306640625, l2 loss: 0.108318030834198\n",
      "iteration 1758, loss value 338.652099609375, l2 loss: 0.10831494629383087\n",
      "iteration 1759, loss value 338.65203857421875, l2 loss: 0.10831187665462494\n",
      "iteration 1760, loss value 338.65118408203125, l2 loss: 0.1083088293671608\n",
      "iteration 1761, loss value 338.6505432128906, l2 loss: 0.10830575972795486\n",
      "iteration 1762, loss value 338.6496887207031, l2 loss: 0.10830272734165192\n",
      "iteration 1763, loss value 338.6494445800781, l2 loss: 0.10829970985651016\n",
      "iteration 1764, loss value 338.6484069824219, l2 loss: 0.10829668492078781\n",
      "iteration 1765, loss value 338.6479187011719, l2 loss: 0.10829368233680725\n",
      "iteration 1766, loss value 338.6474609375, l2 loss: 0.1082906723022461\n",
      "iteration 1767, loss value 338.64654541015625, l2 loss: 0.10828769952058792\n",
      "iteration 1768, loss value 338.6464538574219, l2 loss: 0.10828472673892975\n",
      "iteration 1769, loss value 338.6453552246094, l2 loss: 0.10828179866075516\n",
      "iteration 1770, loss value 338.645263671875, l2 loss: 0.10827883332967758\n",
      "iteration 1771, loss value 338.6440734863281, l2 loss: 0.10827590525150299\n",
      "iteration 1772, loss value 338.6438903808594, l2 loss: 0.10827300697565079\n",
      "iteration 1773, loss value 338.6434020996094, l2 loss: 0.1082700863480568\n",
      "iteration 1774, loss value 338.642578125, l2 loss: 0.10826718807220459\n",
      "iteration 1775, loss value 338.6420593261719, l2 loss: 0.10826432704925537\n",
      "iteration 1776, loss value 338.64178466796875, l2 loss: 0.10826144367456436\n",
      "iteration 1777, loss value 338.6407165527344, l2 loss: 0.10825858265161514\n",
      "iteration 1778, loss value 338.64031982421875, l2 loss: 0.10825575888156891\n",
      "iteration 1779, loss value 338.639892578125, l2 loss: 0.10825292766094208\n",
      "iteration 1780, loss value 338.63934326171875, l2 loss: 0.10825006663799286\n",
      "iteration 1781, loss value 338.6379699707031, l2 loss: 0.1082472875714302\n",
      "iteration 1782, loss value 338.63800048828125, l2 loss: 0.10824447870254517\n",
      "iteration 1783, loss value 338.636962890625, l2 loss: 0.10824169963598251\n",
      "iteration 1784, loss value 338.6363220214844, l2 loss: 0.10823894292116165\n",
      "iteration 1785, loss value 338.6363220214844, l2 loss: 0.10823619365692139\n",
      "iteration 1786, loss value 338.63543701171875, l2 loss: 0.10823346674442291\n",
      "iteration 1787, loss value 338.6348876953125, l2 loss: 0.10823071748018265\n",
      "iteration 1788, loss value 338.63409423828125, l2 loss: 0.10822802037000656\n",
      "iteration 1789, loss value 338.6337585449219, l2 loss: 0.10822530090808868\n",
      "iteration 1790, loss value 338.6330261230469, l2 loss: 0.1082225888967514\n",
      "iteration 1791, loss value 338.6322937011719, l2 loss: 0.1082199439406395\n",
      "iteration 1792, loss value 338.6321716308594, l2 loss: 0.10821723192930222\n",
      "iteration 1793, loss value 338.6312561035156, l2 loss: 0.1082145944237709\n",
      "iteration 1794, loss value 338.63104248046875, l2 loss: 0.10821191221475601\n",
      "iteration 1795, loss value 338.6294860839844, l2 loss: 0.10820930451154709\n",
      "iteration 1796, loss value 338.6293640136719, l2 loss: 0.10820668935775757\n",
      "iteration 1797, loss value 338.62884521484375, l2 loss: 0.10820406675338745\n",
      "iteration 1798, loss value 338.62835693359375, l2 loss: 0.10820148885250092\n",
      "iteration 1799, loss value 338.62799072265625, l2 loss: 0.10819891840219498\n",
      "iteration 1800, loss value 338.627685546875, l2 loss: 0.10819633305072784\n",
      "iteration 1801, loss value 338.62689208984375, l2 loss: 0.1081937626004219\n",
      "iteration 1802, loss value 338.62603759765625, l2 loss: 0.10819119960069656\n",
      "iteration 1803, loss value 338.6252746582031, l2 loss: 0.10818872600793839\n",
      "iteration 1804, loss value 338.6252136230469, l2 loss: 0.10818618535995483\n",
      "iteration 1805, loss value 338.62420654296875, l2 loss: 0.10818367451429367\n",
      "iteration 1806, loss value 338.6237487792969, l2 loss: 0.1081811711192131\n",
      "iteration 1807, loss value 338.6232604980469, l2 loss: 0.10817869007587433\n",
      "iteration 1808, loss value 338.6226501464844, l2 loss: 0.10817621648311615\n",
      "iteration 1809, loss value 338.6215515136719, l2 loss: 0.10817376524209976\n",
      "iteration 1810, loss value 338.6213073730469, l2 loss: 0.10817134380340576\n",
      "iteration 1811, loss value 338.6209411621094, l2 loss: 0.10816889256238937\n",
      "iteration 1812, loss value 338.6202392578125, l2 loss: 0.10816650837659836\n",
      "iteration 1813, loss value 338.6199645996094, l2 loss: 0.10816407948732376\n",
      "iteration 1814, loss value 338.6189880371094, l2 loss: 0.10816172510385513\n",
      "iteration 1815, loss value 338.6189270019531, l2 loss: 0.10815932601690292\n",
      "iteration 1816, loss value 338.61785888671875, l2 loss: 0.10815700143575668\n",
      "iteration 1817, loss value 338.6175537109375, l2 loss: 0.10815463215112686\n",
      "iteration 1818, loss value 338.61676025390625, l2 loss: 0.10815230011940002\n",
      "iteration 1819, loss value 338.61651611328125, l2 loss: 0.10814996063709259\n",
      "iteration 1820, loss value 338.61553955078125, l2 loss: 0.10814765095710754\n",
      "iteration 1821, loss value 338.61517333984375, l2 loss: 0.10814535617828369\n",
      "iteration 1822, loss value 338.6145935058594, l2 loss: 0.10814306139945984\n",
      "iteration 1823, loss value 338.6138000488281, l2 loss: 0.10814077407121658\n",
      "iteration 1824, loss value 338.6131896972656, l2 loss: 0.1081385388970375\n",
      "iteration 1825, loss value 338.61297607421875, l2 loss: 0.10813628882169724\n",
      "iteration 1826, loss value 338.6120910644531, l2 loss: 0.10813406854867935\n",
      "iteration 1827, loss value 338.6116638183594, l2 loss: 0.10813184827566147\n",
      "iteration 1828, loss value 338.611083984375, l2 loss: 0.10812964290380478\n",
      "iteration 1829, loss value 338.61053466796875, l2 loss: 0.10812746733427048\n",
      "iteration 1830, loss value 338.6102294921875, l2 loss: 0.10812525451183319\n",
      "iteration 1831, loss value 338.6091003417969, l2 loss: 0.10812310874462128\n",
      "iteration 1832, loss value 338.60894775390625, l2 loss: 0.10812093317508698\n",
      "iteration 1833, loss value 338.6080017089844, l2 loss: 0.10811880975961685\n",
      "iteration 1834, loss value 338.60797119140625, l2 loss: 0.10811667144298553\n",
      "iteration 1835, loss value 338.60723876953125, l2 loss: 0.1081145703792572\n",
      "iteration 1836, loss value 338.6068115234375, l2 loss: 0.10811247676610947\n",
      "iteration 1837, loss value 338.60601806640625, l2 loss: 0.10811036080121994\n",
      "iteration 1838, loss value 338.6050720214844, l2 loss: 0.108108289539814\n",
      "iteration 1839, loss value 338.60467529296875, l2 loss: 0.10810622572898865\n",
      "iteration 1840, loss value 338.6040954589844, l2 loss: 0.1081041619181633\n",
      "iteration 1841, loss value 338.6036376953125, l2 loss: 0.10810214281082153\n",
      "iteration 1842, loss value 338.6030578613281, l2 loss: 0.10810010135173798\n",
      "iteration 1843, loss value 338.60260009765625, l2 loss: 0.1080980971455574\n",
      "iteration 1844, loss value 338.60198974609375, l2 loss: 0.10809610038995743\n",
      "iteration 1845, loss value 338.60174560546875, l2 loss: 0.10809410363435745\n",
      "iteration 1846, loss value 338.6008605957031, l2 loss: 0.10809214413166046\n",
      "iteration 1847, loss value 338.60028076171875, l2 loss: 0.10809018462896347\n",
      "iteration 1848, loss value 338.5998229980469, l2 loss: 0.10808824002742767\n",
      "iteration 1849, loss value 338.5990905761719, l2 loss: 0.10808629542589188\n",
      "iteration 1850, loss value 338.5983581542969, l2 loss: 0.10808437317609787\n",
      "iteration 1851, loss value 338.5984802246094, l2 loss: 0.10808247327804565\n",
      "iteration 1852, loss value 338.5977783203125, l2 loss: 0.10808058828115463\n",
      "iteration 1853, loss value 338.5977783203125, l2 loss: 0.10807870328426361\n",
      "iteration 1854, loss value 338.5970153808594, l2 loss: 0.10807682573795319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1855, loss value 338.5963134765625, l2 loss: 0.10807497799396515\n",
      "iteration 1856, loss value 338.59564208984375, l2 loss: 0.10807310044765472\n",
      "iteration 1857, loss value 338.5949401855469, l2 loss: 0.10807127505540848\n",
      "iteration 1858, loss value 338.5942077636719, l2 loss: 0.10806948691606522\n",
      "iteration 1859, loss value 338.5936279296875, l2 loss: 0.10806766152381897\n",
      "iteration 1860, loss value 338.5931091308594, l2 loss: 0.1080658808350563\n",
      "iteration 1861, loss value 338.59283447265625, l2 loss: 0.10806410014629364\n",
      "iteration 1862, loss value 338.5921325683594, l2 loss: 0.10806232690811157\n",
      "iteration 1863, loss value 338.591064453125, l2 loss: 0.10806059092283249\n",
      "iteration 1864, loss value 338.5911865234375, l2 loss: 0.1080588772892952\n",
      "iteration 1865, loss value 338.5910339355469, l2 loss: 0.10805714130401611\n",
      "iteration 1866, loss value 338.5899353027344, l2 loss: 0.10805542021989822\n",
      "iteration 1867, loss value 338.58935546875, l2 loss: 0.10805370658636093\n",
      "iteration 1868, loss value 338.5885009765625, l2 loss: 0.10805204510688782\n",
      "iteration 1869, loss value 338.5884094238281, l2 loss: 0.1080503910779953\n",
      "iteration 1870, loss value 338.58770751953125, l2 loss: 0.10804872214794159\n",
      "iteration 1871, loss value 338.58721923828125, l2 loss: 0.10804706811904907\n",
      "iteration 1872, loss value 338.5863342285156, l2 loss: 0.10804544389247894\n",
      "iteration 1873, loss value 338.5859375, l2 loss: 0.10804383456707001\n",
      "iteration 1874, loss value 338.5852966308594, l2 loss: 0.10804223269224167\n",
      "iteration 1875, loss value 338.5850524902344, l2 loss: 0.10804066061973572\n",
      "iteration 1876, loss value 338.58465576171875, l2 loss: 0.10803907364606857\n",
      "iteration 1877, loss value 338.5837097167969, l2 loss: 0.10803750902414322\n",
      "iteration 1878, loss value 338.5831298828125, l2 loss: 0.10803595185279846\n",
      "iteration 1879, loss value 338.58282470703125, l2 loss: 0.10803446173667908\n",
      "iteration 1880, loss value 338.5826416015625, l2 loss: 0.10803292691707611\n",
      "iteration 1881, loss value 338.58160400390625, l2 loss: 0.10803142935037613\n",
      "iteration 1882, loss value 338.580810546875, l2 loss: 0.10802995413541794\n",
      "iteration 1883, loss value 338.58099365234375, l2 loss: 0.10802844911813736\n",
      "iteration 1884, loss value 338.57977294921875, l2 loss: 0.10802700370550156\n",
      "iteration 1885, loss value 338.5797119140625, l2 loss: 0.10802555829286575\n",
      "iteration 1886, loss value 338.5791931152344, l2 loss: 0.10802409797906876\n",
      "iteration 1887, loss value 338.578857421875, l2 loss: 0.10802264511585236\n",
      "iteration 1888, loss value 338.5774841308594, l2 loss: 0.10802126675844193\n",
      "iteration 1889, loss value 338.5774230957031, l2 loss: 0.1080198585987091\n",
      "iteration 1890, loss value 338.576904296875, l2 loss: 0.10801848024129868\n",
      "iteration 1891, loss value 338.5767517089844, l2 loss: 0.10801708698272705\n",
      "iteration 1892, loss value 338.57574462890625, l2 loss: 0.1080157458782196\n",
      "iteration 1893, loss value 338.57501220703125, l2 loss: 0.10801441222429276\n",
      "iteration 1894, loss value 338.5750427246094, l2 loss: 0.10801304876804352\n",
      "iteration 1895, loss value 338.57427978515625, l2 loss: 0.10801174491643906\n",
      "iteration 1896, loss value 338.5736999511719, l2 loss: 0.10801045596599579\n",
      "iteration 1897, loss value 338.5734558105469, l2 loss: 0.10800914466381073\n",
      "iteration 1898, loss value 338.57232666015625, l2 loss: 0.10800788551568985\n",
      "iteration 1899, loss value 338.5721130371094, l2 loss: 0.10800662636756897\n",
      "iteration 1900, loss value 338.572021484375, l2 loss: 0.10800536721944809\n",
      "iteration 1901, loss value 338.57122802734375, l2 loss: 0.10800416022539139\n",
      "iteration 1902, loss value 338.571044921875, l2 loss: 0.1080029234290123\n",
      "iteration 1903, loss value 338.5702819824219, l2 loss: 0.1080016940832138\n",
      "iteration 1904, loss value 338.56988525390625, l2 loss: 0.1080004870891571\n",
      "iteration 1905, loss value 338.5690612792969, l2 loss: 0.1079993024468422\n",
      "iteration 1906, loss value 338.5682067871094, l2 loss: 0.10799814015626907\n",
      "iteration 1907, loss value 338.5679016113281, l2 loss: 0.10799697786569595\n",
      "iteration 1908, loss value 338.56744384765625, l2 loss: 0.10799583047628403\n",
      "iteration 1909, loss value 338.56683349609375, l2 loss: 0.1079946979880333\n",
      "iteration 1910, loss value 338.5662536621094, l2 loss: 0.10799358785152435\n",
      "iteration 1911, loss value 338.5660400390625, l2 loss: 0.10799248516559601\n",
      "iteration 1912, loss value 338.5653076171875, l2 loss: 0.10799137502908707\n",
      "iteration 1913, loss value 338.56463623046875, l2 loss: 0.1079903170466423\n",
      "iteration 1914, loss value 338.5640869140625, l2 loss: 0.10798925161361694\n",
      "iteration 1915, loss value 338.5637512207031, l2 loss: 0.10798821598291397\n",
      "iteration 1916, loss value 338.5634765625, l2 loss: 0.107987180352211\n",
      "iteration 1917, loss value 338.5625915527344, l2 loss: 0.10798616707324982\n",
      "iteration 1918, loss value 338.5621337890625, l2 loss: 0.10798513889312744\n",
      "iteration 1919, loss value 338.56170654296875, l2 loss: 0.10798414051532745\n",
      "iteration 1920, loss value 338.56109619140625, l2 loss: 0.10798315703868866\n",
      "iteration 1921, loss value 338.56072998046875, l2 loss: 0.10798219591379166\n",
      "iteration 1922, loss value 338.5602111816406, l2 loss: 0.10798122733831406\n",
      "iteration 1923, loss value 338.55963134765625, l2 loss: 0.10798028856515884\n",
      "iteration 1924, loss value 338.5589904785156, l2 loss: 0.10797935724258423\n",
      "iteration 1925, loss value 338.5589294433594, l2 loss: 0.107978455722332\n",
      "iteration 1926, loss value 338.5580749511719, l2 loss: 0.10797755420207977\n",
      "iteration 1927, loss value 338.55743408203125, l2 loss: 0.10797666758298874\n",
      "iteration 1928, loss value 338.55731201171875, l2 loss: 0.1079757809638977\n",
      "iteration 1929, loss value 338.5563659667969, l2 loss: 0.10797492414712906\n",
      "iteration 1930, loss value 338.55584716796875, l2 loss: 0.1079741045832634\n",
      "iteration 1931, loss value 338.5556335449219, l2 loss: 0.10797325521707535\n",
      "iteration 1932, loss value 338.554931640625, l2 loss: 0.10797243565320969\n",
      "iteration 1933, loss value 338.554443359375, l2 loss: 0.10797161608934402\n",
      "iteration 1934, loss value 338.5539245605469, l2 loss: 0.10797081887722015\n",
      "iteration 1935, loss value 338.5536804199219, l2 loss: 0.10797005146741867\n",
      "iteration 1936, loss value 338.55291748046875, l2 loss: 0.10796929895877838\n",
      "iteration 1937, loss value 338.552490234375, l2 loss: 0.10796856880187988\n",
      "iteration 1938, loss value 338.55230712890625, l2 loss: 0.1079677939414978\n",
      "iteration 1939, loss value 338.55120849609375, l2 loss: 0.1079670861363411\n",
      "iteration 1940, loss value 338.5505676269531, l2 loss: 0.10796637088060379\n",
      "iteration 1941, loss value 338.55059814453125, l2 loss: 0.10796566307544708\n",
      "iteration 1942, loss value 338.5498962402344, l2 loss: 0.10796502232551575\n",
      "iteration 1943, loss value 338.5496826171875, l2 loss: 0.10796432942152023\n",
      "iteration 1944, loss value 338.5490417480469, l2 loss: 0.1079636737704277\n",
      "iteration 1945, loss value 338.5484313964844, l2 loss: 0.10796302556991577\n",
      "iteration 1946, loss value 338.5479431152344, l2 loss: 0.10796239227056503\n",
      "iteration 1947, loss value 338.5470886230469, l2 loss: 0.1079617515206337\n",
      "iteration 1948, loss value 338.5467224121094, l2 loss: 0.10796117782592773\n",
      "iteration 1949, loss value 338.54638671875, l2 loss: 0.10796058177947998\n",
      "iteration 1950, loss value 338.5460510253906, l2 loss: 0.10796002298593521\n",
      "iteration 1951, loss value 338.54534912109375, l2 loss: 0.10795944929122925\n",
      "iteration 1952, loss value 338.54473876953125, l2 loss: 0.10795889794826508\n",
      "iteration 1953, loss value 338.5443115234375, l2 loss: 0.1079583615064621\n",
      "iteration 1954, loss value 338.5441589355469, l2 loss: 0.10795784741640091\n",
      "iteration 1955, loss value 338.5434265136719, l2 loss: 0.10795732587575912\n",
      "iteration 1956, loss value 338.5428161621094, l2 loss: 0.10795684903860092\n",
      "iteration 1957, loss value 338.5423583984375, l2 loss: 0.10795632004737854\n",
      "iteration 1958, loss value 338.5417175292969, l2 loss: 0.10795588046312332\n",
      "iteration 1959, loss value 338.54144287109375, l2 loss: 0.10795542597770691\n",
      "iteration 1960, loss value 338.5408935546875, l2 loss: 0.10795500874519348\n",
      "iteration 1961, loss value 338.54058837890625, l2 loss: 0.10795456916093826\n",
      "iteration 1962, loss value 338.5400085449219, l2 loss: 0.10795415192842484\n",
      "iteration 1963, loss value 338.5393981933594, l2 loss: 0.1079537644982338\n",
      "iteration 1964, loss value 338.5389099121094, l2 loss: 0.10795336961746216\n",
      "iteration 1965, loss value 338.53802490234375, l2 loss: 0.10795299708843231\n",
      "iteration 1966, loss value 338.53790283203125, l2 loss: 0.10795263200998306\n",
      "iteration 1967, loss value 338.5374450683594, l2 loss: 0.1079522967338562\n",
      "iteration 1968, loss value 338.5372009277344, l2 loss: 0.10795196890830994\n",
      "iteration 1969, loss value 338.53680419921875, l2 loss: 0.10795164853334427\n",
      "iteration 1970, loss value 338.53582763671875, l2 loss: 0.1079513430595398\n",
      "iteration 1971, loss value 338.53533935546875, l2 loss: 0.10795105993747711\n",
      "iteration 1972, loss value 338.53521728515625, l2 loss: 0.10795077681541443\n",
      "iteration 1973, loss value 338.5343933105469, l2 loss: 0.10795048624277115\n",
      "iteration 1974, loss value 338.5338134765625, l2 loss: 0.10795023292303085\n",
      "iteration 1975, loss value 338.5335388183594, l2 loss: 0.10794999450445175\n",
      "iteration 1976, loss value 338.5328674316406, l2 loss: 0.10794977843761444\n",
      "iteration 1977, loss value 338.53204345703125, l2 loss: 0.10794956982135773\n",
      "iteration 1978, loss value 338.5319519042969, l2 loss: 0.10794936120510101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1979, loss value 338.531494140625, l2 loss: 0.10794917494058609\n",
      "iteration 1980, loss value 338.5312805175781, l2 loss: 0.10794901102781296\n",
      "iteration 1981, loss value 338.5301818847656, l2 loss: 0.10794885456562042\n",
      "iteration 1982, loss value 338.5299987792969, l2 loss: 0.10794870555400848\n",
      "iteration 1983, loss value 338.5295715332031, l2 loss: 0.10794857889413834\n",
      "iteration 1984, loss value 338.5289001464844, l2 loss: 0.10794846713542938\n",
      "iteration 1985, loss value 338.52838134765625, l2 loss: 0.10794835537672043\n",
      "iteration 1986, loss value 338.52789306640625, l2 loss: 0.10794827342033386\n",
      "iteration 1987, loss value 338.5279541015625, l2 loss: 0.10794820636510849\n",
      "iteration 1988, loss value 338.52764892578125, l2 loss: 0.10794810950756073\n",
      "iteration 1989, loss value 338.52655029296875, l2 loss: 0.10794807225465775\n",
      "iteration 1990, loss value 338.5263671875, l2 loss: 0.10794802755117416\n",
      "iteration 1991, loss value 338.525634765625, l2 loss: 0.10794798284769058\n",
      "iteration 1992, loss value 338.5251159667969, l2 loss: 0.10794796794652939\n",
      "iteration 1993, loss value 338.5247802734375, l2 loss: 0.10794798284769058\n",
      "iteration 1994, loss value 338.5239562988281, l2 loss: 0.10794798284769058\n",
      "iteration 1995, loss value 338.5235595703125, l2 loss: 0.10794802010059357\n",
      "iteration 1996, loss value 338.5234069824219, l2 loss: 0.10794807970523834\n",
      "iteration 1997, loss value 338.52288818359375, l2 loss: 0.10794810205698013\n",
      "iteration 1998, loss value 338.5221862792969, l2 loss: 0.1079481765627861\n",
      "iteration 1999, loss value 338.5218200683594, l2 loss: 0.10794825851917267\n",
      "iteration 2000, loss value 338.5212707519531, l2 loss: 0.10794834047555923\n",
      "iteration 2001, loss value 338.5209655761719, l2 loss: 0.10794846713542938\n",
      "iteration 2002, loss value 338.52093505859375, l2 loss: 0.10794857144355774\n",
      "iteration 2003, loss value 338.5195617675781, l2 loss: 0.10794872045516968\n",
      "iteration 2004, loss value 338.5195617675781, l2 loss: 0.10794887691736221\n",
      "iteration 2005, loss value 338.51947021484375, l2 loss: 0.10794901847839355\n",
      "iteration 2006, loss value 338.51824951171875, l2 loss: 0.10794919729232788\n",
      "iteration 2007, loss value 338.51812744140625, l2 loss: 0.1079493910074234\n",
      "iteration 2008, loss value 338.51776123046875, l2 loss: 0.10794959217309952\n",
      "iteration 2009, loss value 338.51739501953125, l2 loss: 0.10794979333877563\n",
      "iteration 2010, loss value 338.51666259765625, l2 loss: 0.10795003920793533\n",
      "iteration 2011, loss value 338.51611328125, l2 loss: 0.10795026272535324\n",
      "iteration 2012, loss value 338.5158996582031, l2 loss: 0.10795053839683533\n",
      "iteration 2013, loss value 338.515380859375, l2 loss: 0.10795076936483383\n",
      "iteration 2014, loss value 338.5147399902344, l2 loss: 0.10795105993747711\n",
      "iteration 2015, loss value 338.51422119140625, l2 loss: 0.10795137286186218\n",
      "iteration 2016, loss value 338.51434326171875, l2 loss: 0.10795167088508606\n",
      "iteration 2017, loss value 338.5136413574219, l2 loss: 0.10795196890830994\n",
      "iteration 2018, loss value 338.51300048828125, l2 loss: 0.10795232653617859\n",
      "iteration 2019, loss value 338.5124206542969, l2 loss: 0.10795265436172485\n",
      "iteration 2020, loss value 338.51190185546875, l2 loss: 0.10795299708843231\n",
      "iteration 2021, loss value 338.5113220214844, l2 loss: 0.10795337706804276\n",
      "iteration 2022, loss value 338.51129150390625, l2 loss: 0.1079537644982338\n",
      "iteration 2023, loss value 338.51025390625, l2 loss: 0.10795415937900543\n",
      "iteration 2024, loss value 338.5098571777344, l2 loss: 0.10795457661151886\n",
      "iteration 2025, loss value 338.5097961425781, l2 loss: 0.10795501619577408\n",
      "iteration 2026, loss value 338.5090026855469, l2 loss: 0.1079554334282875\n",
      "iteration 2027, loss value 338.5086364746094, l2 loss: 0.10795590281486511\n",
      "iteration 2028, loss value 338.5086669921875, l2 loss: 0.10795634984970093\n",
      "iteration 2029, loss value 338.50775146484375, l2 loss: 0.10795681923627853\n",
      "iteration 2030, loss value 338.5074157714844, l2 loss: 0.10795729607343674\n",
      "iteration 2031, loss value 338.5062561035156, l2 loss: 0.10795781016349792\n",
      "iteration 2032, loss value 338.5061950683594, l2 loss: 0.10795832425355911\n",
      "iteration 2033, loss value 338.5054626464844, l2 loss: 0.1079588457942009\n",
      "iteration 2034, loss value 338.5052185058594, l2 loss: 0.10795937478542328\n",
      "iteration 2035, loss value 338.5048828125, l2 loss: 0.10795992612838745\n",
      "iteration 2036, loss value 338.5042724609375, l2 loss: 0.10796050727367401\n",
      "iteration 2037, loss value 338.5039367675781, l2 loss: 0.10796109586954117\n",
      "iteration 2038, loss value 338.50347900390625, l2 loss: 0.10796165466308594\n",
      "iteration 2039, loss value 338.5027770996094, l2 loss: 0.10796229541301727\n",
      "iteration 2040, loss value 338.5030212402344, l2 loss: 0.10796289891004562\n",
      "iteration 2041, loss value 338.50238037109375, l2 loss: 0.10796353965997696\n",
      "iteration 2042, loss value 338.50177001953125, l2 loss: 0.1079641804099083\n",
      "iteration 2043, loss value 338.50115966796875, l2 loss: 0.10796480625867844\n",
      "iteration 2044, loss value 338.50079345703125, l2 loss: 0.10796551406383514\n",
      "iteration 2045, loss value 338.5003356933594, l2 loss: 0.10796616971492767\n",
      "iteration 2046, loss value 338.4999694824219, l2 loss: 0.10796687752008438\n",
      "iteration 2047, loss value 338.4996032714844, l2 loss: 0.10796757787466049\n",
      "iteration 2048, loss value 338.49896240234375, l2 loss: 0.10796830803155899\n",
      "iteration 2049, loss value 338.4985046386719, l2 loss: 0.10796905308961868\n",
      "iteration 2050, loss value 338.49810791015625, l2 loss: 0.10796978324651718\n",
      "iteration 2051, loss value 338.4977722167969, l2 loss: 0.10797055810689926\n",
      "iteration 2052, loss value 338.497314453125, l2 loss: 0.10797131806612015\n",
      "iteration 2053, loss value 338.4966735839844, l2 loss: 0.10797211527824402\n",
      "iteration 2054, loss value 338.4961853027344, l2 loss: 0.10797293484210968\n",
      "iteration 2055, loss value 338.495849609375, l2 loss: 0.10797373205423355\n",
      "iteration 2056, loss value 338.4952697753906, l2 loss: 0.10797455906867981\n",
      "iteration 2057, loss value 338.4947814941406, l2 loss: 0.10797537863254547\n",
      "iteration 2058, loss value 338.4942321777344, l2 loss: 0.10797624289989471\n",
      "iteration 2059, loss value 338.49407958984375, l2 loss: 0.10797712206840515\n",
      "iteration 2060, loss value 338.4936828613281, l2 loss: 0.1079779863357544\n",
      "iteration 2061, loss value 338.49310302734375, l2 loss: 0.10797887295484543\n",
      "iteration 2062, loss value 338.4926452636719, l2 loss: 0.10797977447509766\n",
      "iteration 2063, loss value 338.4921569824219, l2 loss: 0.10798068344593048\n",
      "iteration 2064, loss value 338.4917297363281, l2 loss: 0.1079816222190857\n",
      "iteration 2065, loss value 338.491455078125, l2 loss: 0.1079825684428215\n",
      "iteration 2066, loss value 338.49151611328125, l2 loss: 0.10798349976539612\n",
      "iteration 2067, loss value 338.4905090332031, l2 loss: 0.10798447579145432\n",
      "iteration 2068, loss value 338.49005126953125, l2 loss: 0.1079854667186737\n",
      "iteration 2069, loss value 338.48974609375, l2 loss: 0.10798642039299011\n",
      "iteration 2070, loss value 338.4889831542969, l2 loss: 0.10798744857311249\n",
      "iteration 2071, loss value 338.489013671875, l2 loss: 0.10798846185207367\n",
      "iteration 2072, loss value 338.48828125, l2 loss: 0.10798946768045425\n",
      "iteration 2073, loss value 338.48779296875, l2 loss: 0.10799051821231842\n",
      "iteration 2074, loss value 338.4872741699219, l2 loss: 0.10799156874418259\n",
      "iteration 2075, loss value 338.4867248535156, l2 loss: 0.10799263417720795\n",
      "iteration 2076, loss value 338.48675537109375, l2 loss: 0.1079937145113945\n",
      "iteration 2077, loss value 338.4862060546875, l2 loss: 0.10799480229616165\n",
      "iteration 2078, loss value 338.48577880859375, l2 loss: 0.10799592733383179\n",
      "iteration 2079, loss value 338.4852294921875, l2 loss: 0.10799701511859894\n",
      "iteration 2080, loss value 338.48480224609375, l2 loss: 0.10799816250801086\n",
      "iteration 2081, loss value 338.48455810546875, l2 loss: 0.1079992800951004\n",
      "iteration 2082, loss value 338.48394775390625, l2 loss: 0.10800043493509293\n",
      "iteration 2083, loss value 338.48345947265625, l2 loss: 0.10800159722566605\n",
      "iteration 2084, loss value 338.4826965332031, l2 loss: 0.10800278931856155\n",
      "iteration 2085, loss value 338.48297119140625, l2 loss: 0.10800398141145706\n",
      "iteration 2086, loss value 338.48199462890625, l2 loss: 0.10800518840551376\n",
      "iteration 2087, loss value 338.48211669921875, l2 loss: 0.10800641030073166\n",
      "iteration 2088, loss value 338.4816589355469, l2 loss: 0.10800763219594955\n",
      "iteration 2089, loss value 338.4808044433594, l2 loss: 0.10800886154174805\n",
      "iteration 2090, loss value 338.4803771972656, l2 loss: 0.10801012814044952\n",
      "iteration 2091, loss value 338.4803161621094, l2 loss: 0.1080113872885704\n",
      "iteration 2092, loss value 338.4796447753906, l2 loss: 0.10801266133785248\n",
      "iteration 2093, loss value 338.47943115234375, l2 loss: 0.10801395773887634\n",
      "iteration 2094, loss value 338.4788513183594, l2 loss: 0.108015276491642\n",
      "iteration 2095, loss value 338.47833251953125, l2 loss: 0.10801658034324646\n",
      "iteration 2096, loss value 338.4776306152344, l2 loss: 0.10801790654659271\n",
      "iteration 2097, loss value 338.4774475097656, l2 loss: 0.10801925510168076\n",
      "iteration 2098, loss value 338.4772033691406, l2 loss: 0.1080206036567688\n",
      "iteration 2099, loss value 338.47686767578125, l2 loss: 0.10802196711301804\n",
      "iteration 2100, loss value 338.4760437011719, l2 loss: 0.10802333056926727\n",
      "iteration 2101, loss value 338.47576904296875, l2 loss: 0.1080247238278389\n",
      "iteration 2102, loss value 338.4751892089844, l2 loss: 0.10802614688873291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2103, loss value 338.4750061035156, l2 loss: 0.10802754759788513\n",
      "iteration 2104, loss value 338.47418212890625, l2 loss: 0.10802894830703735\n",
      "iteration 2105, loss value 338.47369384765625, l2 loss: 0.10803040117025375\n",
      "iteration 2106, loss value 338.4733581542969, l2 loss: 0.10803184658288956\n",
      "iteration 2107, loss value 338.47283935546875, l2 loss: 0.10803333669900894\n",
      "iteration 2108, loss value 338.4728698730469, l2 loss: 0.10803476721048355\n",
      "iteration 2109, loss value 338.4718322753906, l2 loss: 0.10803626477718353\n",
      "iteration 2110, loss value 338.4716491699219, l2 loss: 0.10803775489330292\n",
      "iteration 2111, loss value 338.4711608886719, l2 loss: 0.10803928971290588\n",
      "iteration 2112, loss value 338.47088623046875, l2 loss: 0.10804082453250885\n",
      "iteration 2113, loss value 338.470703125, l2 loss: 0.10804234445095062\n",
      "iteration 2114, loss value 338.4701843261719, l2 loss: 0.10804390162229538\n",
      "iteration 2115, loss value 338.4700927734375, l2 loss: 0.10804544389247894\n",
      "iteration 2116, loss value 338.46917724609375, l2 loss: 0.1080470159649849\n",
      "iteration 2117, loss value 338.4687805175781, l2 loss: 0.10804858803749084\n",
      "iteration 2118, loss value 338.4684753417969, l2 loss: 0.10805018246173859\n",
      "iteration 2119, loss value 338.4679870605469, l2 loss: 0.10805179178714752\n",
      "iteration 2120, loss value 338.4677429199219, l2 loss: 0.10805337876081467\n",
      "iteration 2121, loss value 338.4670104980469, l2 loss: 0.10805504769086838\n",
      "iteration 2122, loss value 338.4667663574219, l2 loss: 0.1080566868185997\n",
      "iteration 2123, loss value 338.46661376953125, l2 loss: 0.10805832594633102\n",
      "iteration 2124, loss value 338.4659423828125, l2 loss: 0.10805998742580414\n",
      "iteration 2125, loss value 338.4655456542969, l2 loss: 0.10806167870759964\n",
      "iteration 2126, loss value 338.46527099609375, l2 loss: 0.10806335508823395\n",
      "iteration 2127, loss value 338.4646301269531, l2 loss: 0.10806506127119064\n",
      "iteration 2128, loss value 338.464111328125, l2 loss: 0.10806676745414734\n",
      "iteration 2129, loss value 338.4636535644531, l2 loss: 0.10806848853826523\n",
      "iteration 2130, loss value 338.4631652832031, l2 loss: 0.1080702394247055\n",
      "iteration 2131, loss value 338.4631042480469, l2 loss: 0.10807198286056519\n",
      "iteration 2132, loss value 338.4626159667969, l2 loss: 0.10807375609874725\n",
      "iteration 2133, loss value 338.46270751953125, l2 loss: 0.10807552188634872\n",
      "iteration 2134, loss value 338.4616394042969, l2 loss: 0.10807730257511139\n",
      "iteration 2135, loss value 338.46136474609375, l2 loss: 0.10807909071445465\n",
      "iteration 2136, loss value 338.4612731933594, l2 loss: 0.1080809012055397\n",
      "iteration 2137, loss value 338.4606628417969, l2 loss: 0.10808271169662476\n",
      "iteration 2138, loss value 338.4599914550781, l2 loss: 0.10808456689119339\n",
      "iteration 2139, loss value 338.459716796875, l2 loss: 0.10808639228343964\n",
      "iteration 2140, loss value 338.4591979980469, l2 loss: 0.10808826237916946\n",
      "iteration 2141, loss value 338.4590759277344, l2 loss: 0.10809013247489929\n",
      "iteration 2142, loss value 338.45867919921875, l2 loss: 0.10809198766946793\n",
      "iteration 2143, loss value 338.4576416015625, l2 loss: 0.10809389501810074\n",
      "iteration 2144, loss value 338.45745849609375, l2 loss: 0.10809578746557236\n",
      "iteration 2145, loss value 338.45745849609375, l2 loss: 0.10809770226478577\n",
      "iteration 2146, loss value 338.45709228515625, l2 loss: 0.10809963196516037\n",
      "iteration 2147, loss value 338.4567565917969, l2 loss: 0.10810154676437378\n",
      "iteration 2148, loss value 338.4560241699219, l2 loss: 0.10810347646474838\n",
      "iteration 2149, loss value 338.45538330078125, l2 loss: 0.10810545831918716\n",
      "iteration 2150, loss value 338.45526123046875, l2 loss: 0.10810742527246475\n",
      "iteration 2151, loss value 338.4545593261719, l2 loss: 0.10810939967632294\n",
      "iteration 2152, loss value 338.4544372558594, l2 loss: 0.1081114113330841\n",
      "iteration 2153, loss value 338.45416259765625, l2 loss: 0.10811340063810349\n",
      "iteration 2154, loss value 338.45367431640625, l2 loss: 0.10811540484428406\n",
      "iteration 2155, loss value 338.452880859375, l2 loss: 0.10811743140220642\n",
      "iteration 2156, loss value 338.4527282714844, l2 loss: 0.10811947286128998\n",
      "iteration 2157, loss value 338.452392578125, l2 loss: 0.10812152177095413\n",
      "iteration 2158, loss value 338.4518737792969, l2 loss: 0.10812357813119888\n",
      "iteration 2159, loss value 338.4512634277344, l2 loss: 0.10812566429376602\n",
      "iteration 2160, loss value 338.4512023925781, l2 loss: 0.10812772065401077\n",
      "iteration 2161, loss value 338.4505920410156, l2 loss: 0.1081298366189003\n",
      "iteration 2162, loss value 338.4501037597656, l2 loss: 0.10813195258378983\n",
      "iteration 2163, loss value 338.45013427734375, l2 loss: 0.10813406109809875\n",
      "iteration 2164, loss value 338.4498596191406, l2 loss: 0.10813617706298828\n",
      "iteration 2165, loss value 338.44927978515625, l2 loss: 0.1081383153796196\n",
      "iteration 2166, loss value 338.4490966796875, l2 loss: 0.1081404760479927\n",
      "iteration 2167, loss value 338.448486328125, l2 loss: 0.10814261436462402\n",
      "iteration 2168, loss value 338.4479064941406, l2 loss: 0.10814478248357773\n",
      "iteration 2169, loss value 338.4473571777344, l2 loss: 0.10814700275659561\n",
      "iteration 2170, loss value 338.4473876953125, l2 loss: 0.10814919322729111\n",
      "iteration 2171, loss value 338.4469909667969, l2 loss: 0.1081513836979866\n",
      "iteration 2172, loss value 338.44647216796875, l2 loss: 0.10815358906984329\n",
      "iteration 2173, loss value 338.4454650878906, l2 loss: 0.10815583914518356\n",
      "iteration 2174, loss value 338.44549560546875, l2 loss: 0.10815808922052383\n",
      "iteration 2175, loss value 338.4451599121094, l2 loss: 0.1081603392958641\n",
      "iteration 2176, loss value 338.44525146484375, l2 loss: 0.10816258192062378\n",
      "iteration 2177, loss value 338.4442443847656, l2 loss: 0.10816483944654465\n",
      "iteration 2178, loss value 338.44366455078125, l2 loss: 0.10816717147827148\n",
      "iteration 2179, loss value 338.4437255859375, l2 loss: 0.10816943645477295\n",
      "iteration 2180, loss value 338.4429626464844, l2 loss: 0.10817176848649979\n",
      "iteration 2181, loss value 338.44287109375, l2 loss: 0.10817406326532364\n",
      "iteration 2182, loss value 338.4423522949219, l2 loss: 0.10817640274763107\n",
      "iteration 2183, loss value 338.44189453125, l2 loss: 0.1081787571310997\n",
      "iteration 2184, loss value 338.4417419433594, l2 loss: 0.10818108916282654\n",
      "iteration 2185, loss value 338.44110107421875, l2 loss: 0.10818345099687576\n",
      "iteration 2186, loss value 338.4409484863281, l2 loss: 0.10818582028150558\n",
      "iteration 2187, loss value 338.44036865234375, l2 loss: 0.1081882193684578\n",
      "iteration 2188, loss value 338.4400634765625, l2 loss: 0.10819061845541\n",
      "iteration 2189, loss value 338.43988037109375, l2 loss: 0.10819301754236221\n",
      "iteration 2190, loss value 338.4391784667969, l2 loss: 0.10819543153047562\n",
      "iteration 2191, loss value 338.43890380859375, l2 loss: 0.10819784551858902\n",
      "iteration 2192, loss value 338.4380187988281, l2 loss: 0.10820026695728302\n",
      "iteration 2193, loss value 338.43804931640625, l2 loss: 0.1082027480006218\n",
      "iteration 2194, loss value 338.43768310546875, l2 loss: 0.10820519179105759\n",
      "iteration 2195, loss value 338.43701171875, l2 loss: 0.10820765793323517\n",
      "iteration 2196, loss value 338.4370422363281, l2 loss: 0.10821012407541275\n",
      "iteration 2197, loss value 338.4364929199219, l2 loss: 0.10821262001991272\n",
      "iteration 2198, loss value 338.43634033203125, l2 loss: 0.10821514576673508\n",
      "iteration 2199, loss value 338.4361877441406, l2 loss: 0.10821762681007385\n",
      "iteration 2200, loss value 338.4352111816406, l2 loss: 0.1082201674580574\n",
      "iteration 2201, loss value 338.43524169921875, l2 loss: 0.10822267830371857\n",
      "iteration 2202, loss value 338.4347839355469, l2 loss: 0.10822521895170212\n",
      "iteration 2203, loss value 338.43414306640625, l2 loss: 0.10822775959968567\n",
      "iteration 2204, loss value 338.4335632324219, l2 loss: 0.1082303375005722\n",
      "iteration 2205, loss value 338.4334716796875, l2 loss: 0.10823289304971695\n",
      "iteration 2206, loss value 338.4328918457031, l2 loss: 0.10823551565408707\n",
      "iteration 2207, loss value 338.4331970214844, l2 loss: 0.1082381010055542\n",
      "iteration 2208, loss value 338.4327087402344, l2 loss: 0.10824070125818253\n",
      "iteration 2209, loss value 338.43212890625, l2 loss: 0.10824330151081085\n",
      "iteration 2210, loss value 338.431396484375, l2 loss: 0.10824590921401978\n",
      "iteration 2211, loss value 338.4307861328125, l2 loss: 0.10824859142303467\n",
      "iteration 2212, loss value 338.4308776855469, l2 loss: 0.10825122892856598\n",
      "iteration 2213, loss value 338.4309997558594, l2 loss: 0.10825387388467789\n",
      "iteration 2214, loss value 338.43011474609375, l2 loss: 0.10825653374195099\n",
      "iteration 2215, loss value 338.42938232421875, l2 loss: 0.10825923085212708\n",
      "iteration 2216, loss value 338.42938232421875, l2 loss: 0.10826189815998077\n",
      "iteration 2217, loss value 338.4289245605469, l2 loss: 0.10826458781957626\n",
      "iteration 2218, loss value 338.42816162109375, l2 loss: 0.10826730728149414\n",
      "iteration 2219, loss value 338.4281005859375, l2 loss: 0.10827002674341202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2220, loss value 338.4275207519531, l2 loss: 0.1082727387547493\n",
      "iteration 2221, loss value 338.4269714355469, l2 loss: 0.10827551037073135\n",
      "iteration 2222, loss value 338.427490234375, l2 loss: 0.10827821493148804\n",
      "iteration 2223, loss value 338.4267883300781, l2 loss: 0.10828098654747009\n",
      "iteration 2224, loss value 338.4263000488281, l2 loss: 0.10828373581171036\n",
      "iteration 2225, loss value 338.4257507324219, l2 loss: 0.10828652232885361\n",
      "iteration 2226, loss value 338.4253234863281, l2 loss: 0.10828932374715805\n",
      "iteration 2227, loss value 338.4249572753906, l2 loss: 0.1082921251654625\n",
      "iteration 2228, loss value 338.4248962402344, l2 loss: 0.10829492658376694\n",
      "iteration 2229, loss value 338.4244079589844, l2 loss: 0.10829775780439377\n",
      "iteration 2230, loss value 338.4239501953125, l2 loss: 0.1083005890250206\n",
      "iteration 2231, loss value 338.4238586425781, l2 loss: 0.10830342769622803\n",
      "iteration 2232, loss value 338.42333984375, l2 loss: 0.10830625891685486\n",
      "iteration 2233, loss value 338.4228210449219, l2 loss: 0.10830909758806229\n",
      "iteration 2234, loss value 338.4223937988281, l2 loss: 0.10831198841333389\n",
      "iteration 2235, loss value 338.42218017578125, l2 loss: 0.10831485688686371\n",
      "iteration 2236, loss value 338.4215087890625, l2 loss: 0.10831775516271591\n",
      "iteration 2237, loss value 338.4212951660156, l2 loss: 0.10832066088914871\n",
      "iteration 2238, loss value 338.4208679199219, l2 loss: 0.1083235815167427\n",
      "iteration 2239, loss value 338.4207458496094, l2 loss: 0.10832647234201431\n",
      "iteration 2240, loss value 338.42010498046875, l2 loss: 0.1083293929696083\n",
      "iteration 2241, loss value 338.4197082519531, l2 loss: 0.1083323210477829\n",
      "iteration 2242, loss value 338.4192810058594, l2 loss: 0.10833529382944107\n",
      "iteration 2243, loss value 338.4188232421875, l2 loss: 0.10833825170993805\n",
      "iteration 2244, loss value 338.41876220703125, l2 loss: 0.10834120959043503\n",
      "iteration 2245, loss value 338.4185485839844, l2 loss: 0.1083441749215126\n",
      "iteration 2246, loss value 338.41851806640625, l2 loss: 0.10834717005491257\n",
      "iteration 2247, loss value 338.41766357421875, l2 loss: 0.10835017263889313\n",
      "iteration 2248, loss value 338.4172058105469, l2 loss: 0.1083531454205513\n",
      "iteration 2249, loss value 338.4166564941406, l2 loss: 0.10835617780685425\n",
      "iteration 2250, loss value 338.4164123535156, l2 loss: 0.1083591878414154\n",
      "iteration 2251, loss value 338.4161071777344, l2 loss: 0.10836226493120193\n",
      "iteration 2252, loss value 338.4161071777344, l2 loss: 0.10836527496576309\n",
      "iteration 2253, loss value 338.4153747558594, l2 loss: 0.10836833715438843\n",
      "iteration 2254, loss value 338.4150695800781, l2 loss: 0.10837140679359436\n",
      "iteration 2255, loss value 338.4146423339844, l2 loss: 0.10837449133396149\n",
      "iteration 2256, loss value 338.4146423339844, l2 loss: 0.10837754607200623\n",
      "iteration 2257, loss value 338.4139099121094, l2 loss: 0.10838066786527634\n",
      "iteration 2258, loss value 338.4135437011719, l2 loss: 0.10838374495506287\n",
      "iteration 2259, loss value 338.4132080078125, l2 loss: 0.10838685184717178\n",
      "iteration 2260, loss value 338.4131774902344, l2 loss: 0.1083899587392807\n",
      "iteration 2261, loss value 338.4125671386719, l2 loss: 0.108393095433712\n",
      "iteration 2262, loss value 338.4122619628906, l2 loss: 0.10839623957872391\n",
      "iteration 2263, loss value 338.41229248046875, l2 loss: 0.1083993911743164\n",
      "iteration 2264, loss value 338.4118347167969, l2 loss: 0.10840253531932831\n",
      "iteration 2265, loss value 338.4111022949219, l2 loss: 0.10840573906898499\n",
      "iteration 2266, loss value 338.4108581542969, l2 loss: 0.10840889066457748\n",
      "iteration 2267, loss value 338.4101257324219, l2 loss: 0.10841207951307297\n",
      "iteration 2268, loss value 338.4100036621094, l2 loss: 0.10841526091098785\n",
      "iteration 2269, loss value 338.4096984863281, l2 loss: 0.10841846466064453\n",
      "iteration 2270, loss value 338.40924072265625, l2 loss: 0.1084216982126236\n",
      "iteration 2271, loss value 338.4093322753906, l2 loss: 0.10842492431402206\n",
      "iteration 2272, loss value 338.4089660644531, l2 loss: 0.10842815041542053\n",
      "iteration 2273, loss value 338.4085388183594, l2 loss: 0.108431376516819\n",
      "iteration 2274, loss value 338.40826416015625, l2 loss: 0.10843461751937866\n",
      "iteration 2275, loss value 338.4076232910156, l2 loss: 0.1084379032254219\n",
      "iteration 2276, loss value 338.40753173828125, l2 loss: 0.10844118893146515\n",
      "iteration 2277, loss value 338.40740966796875, l2 loss: 0.10844443738460541\n",
      "iteration 2278, loss value 338.4068298339844, l2 loss: 0.10844770818948746\n",
      "iteration 2279, loss value 338.4061584472656, l2 loss: 0.1084509938955307\n",
      "iteration 2280, loss value 338.4057312011719, l2 loss: 0.10845429450273514\n",
      "iteration 2281, loss value 338.40521240234375, l2 loss: 0.10845761001110077\n",
      "iteration 2282, loss value 338.4051818847656, l2 loss: 0.10846094787120819\n",
      "iteration 2283, loss value 338.4050598144531, l2 loss: 0.10846424847841263\n",
      "iteration 2284, loss value 338.4043884277344, l2 loss: 0.10846760869026184\n",
      "iteration 2285, loss value 338.4040222167969, l2 loss: 0.10847093909978867\n",
      "iteration 2286, loss value 338.40374755859375, l2 loss: 0.10847430676221848\n",
      "iteration 2287, loss value 338.4035339355469, l2 loss: 0.10847768187522888\n",
      "iteration 2288, loss value 338.4034423828125, l2 loss: 0.1084810271859169\n",
      "iteration 2289, loss value 338.4029846191406, l2 loss: 0.1084844246506691\n",
      "iteration 2290, loss value 338.4025573730469, l2 loss: 0.1084878072142601\n",
      "iteration 2291, loss value 338.402099609375, l2 loss: 0.1084912121295929\n",
      "iteration 2292, loss value 338.4014587402344, l2 loss: 0.1084945946931839\n",
      "iteration 2293, loss value 338.4011535644531, l2 loss: 0.10849801450967789\n",
      "iteration 2294, loss value 338.40093994140625, l2 loss: 0.10850145667791367\n",
      "iteration 2295, loss value 338.4010009765625, l2 loss: 0.10850488394498825\n",
      "iteration 2296, loss value 338.400146484375, l2 loss: 0.10850832611322403\n",
      "iteration 2297, loss value 338.40008544921875, l2 loss: 0.10851176828145981\n",
      "iteration 2298, loss value 338.3997497558594, l2 loss: 0.10851521790027618\n",
      "iteration 2299, loss value 338.3993835449219, l2 loss: 0.10851868987083435\n",
      "iteration 2300, loss value 338.399169921875, l2 loss: 0.10852216929197311\n",
      "iteration 2301, loss value 338.3985290527344, l2 loss: 0.10852564871311188\n",
      "iteration 2302, loss value 338.3983459472656, l2 loss: 0.10852915048599243\n",
      "iteration 2303, loss value 338.3976745605469, l2 loss: 0.10853264480829239\n",
      "iteration 2304, loss value 338.3974304199219, l2 loss: 0.10853615403175354\n",
      "iteration 2305, loss value 338.3971862792969, l2 loss: 0.10853967070579529\n",
      "iteration 2306, loss value 338.3970031738281, l2 loss: 0.10854319483041763\n",
      "iteration 2307, loss value 338.39642333984375, l2 loss: 0.10854674130678177\n",
      "iteration 2308, loss value 338.39617919921875, l2 loss: 0.1085502877831459\n",
      "iteration 2309, loss value 338.39581298828125, l2 loss: 0.10855381935834885\n",
      "iteration 2310, loss value 338.39544677734375, l2 loss: 0.10855737328529358\n",
      "iteration 2311, loss value 338.39501953125, l2 loss: 0.1085609421133995\n",
      "iteration 2312, loss value 338.3946838378906, l2 loss: 0.10856451839208603\n",
      "iteration 2313, loss value 338.3945617675781, l2 loss: 0.10856808722019196\n",
      "iteration 2314, loss value 338.3940124511719, l2 loss: 0.10857169330120087\n",
      "iteration 2315, loss value 338.3935546875, l2 loss: 0.10857528448104858\n",
      "iteration 2316, loss value 338.39361572265625, l2 loss: 0.10857891291379929\n",
      "iteration 2317, loss value 338.39337158203125, l2 loss: 0.1085825189948082\n",
      "iteration 2318, loss value 338.3929138183594, l2 loss: 0.1085861548781395\n",
      "iteration 2319, loss value 338.39263916015625, l2 loss: 0.1085897833108902\n",
      "iteration 2320, loss value 338.3922119140625, l2 loss: 0.1085934191942215\n",
      "iteration 2321, loss value 338.39202880859375, l2 loss: 0.1085970476269722\n",
      "iteration 2322, loss value 338.3910827636719, l2 loss: 0.10860074311494827\n",
      "iteration 2323, loss value 338.3913879394531, l2 loss: 0.10860439389944077\n",
      "iteration 2324, loss value 338.39105224609375, l2 loss: 0.10860804468393326\n",
      "iteration 2325, loss value 338.3903503417969, l2 loss: 0.10861175507307053\n",
      "iteration 2326, loss value 338.3904724121094, l2 loss: 0.1086154356598854\n",
      "iteration 2327, loss value 338.3897399902344, l2 loss: 0.10861913859844208\n",
      "iteration 2328, loss value 338.38946533203125, l2 loss: 0.10862284153699875\n",
      "iteration 2329, loss value 338.3890075683594, l2 loss: 0.10862653702497482\n",
      "iteration 2330, loss value 338.38885498046875, l2 loss: 0.10863029211759567\n",
      "iteration 2331, loss value 338.3885192871094, l2 loss: 0.10863403230905533\n",
      "iteration 2332, loss value 338.3885498046875, l2 loss: 0.10863775759935379\n",
      "iteration 2333, loss value 338.38800048828125, l2 loss: 0.10864151269197464\n",
      "iteration 2334, loss value 338.3875732421875, l2 loss: 0.10864527523517609\n",
      "iteration 2335, loss value 338.3871765136719, l2 loss: 0.10864900797605515\n",
      "iteration 2336, loss value 338.3866882324219, l2 loss: 0.108652763068676\n",
      "iteration 2337, loss value 338.3865661621094, l2 loss: 0.10865657031536102\n",
      "iteration 2338, loss value 338.3863525390625, l2 loss: 0.10866034775972366\n",
      "iteration 2339, loss value 338.38641357421875, l2 loss: 0.1086641252040863\n",
      "iteration 2340, loss value 338.3852233886719, l2 loss: 0.10866791754961014\n",
      "iteration 2341, loss value 338.3849792480469, l2 loss: 0.10867174714803696\n",
      "iteration 2342, loss value 338.38470458984375, l2 loss: 0.10867555439472198\n",
      "iteration 2343, loss value 338.38458251953125, l2 loss: 0.10867932438850403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2344, loss value 338.3833923339844, l2 loss: 0.10868317633867264\n",
      "iteration 2345, loss value 338.38385009765625, l2 loss: 0.10868700593709946\n",
      "iteration 2346, loss value 338.383544921875, l2 loss: 0.10869084298610687\n",
      "iteration 2347, loss value 338.3827819824219, l2 loss: 0.10869469493627548\n",
      "iteration 2348, loss value 338.38262939453125, l2 loss: 0.10869856923818588\n",
      "iteration 2349, loss value 338.3826599121094, l2 loss: 0.1087024137377739\n",
      "iteration 2350, loss value 338.3819580078125, l2 loss: 0.1087062805891037\n",
      "iteration 2351, loss value 338.3816833496094, l2 loss: 0.10871016979217529\n",
      "iteration 2352, loss value 338.3815612792969, l2 loss: 0.10871405154466629\n",
      "iteration 2353, loss value 338.38092041015625, l2 loss: 0.10871795564889908\n",
      "iteration 2354, loss value 338.381103515625, l2 loss: 0.10872185975313187\n",
      "iteration 2355, loss value 338.38079833984375, l2 loss: 0.10872578620910645\n",
      "iteration 2356, loss value 338.38067626953125, l2 loss: 0.10872966051101685\n",
      "iteration 2357, loss value 338.3797302246094, l2 loss: 0.10873359441757202\n",
      "iteration 2358, loss value 338.37939453125, l2 loss: 0.1087375283241272\n",
      "iteration 2359, loss value 338.37933349609375, l2 loss: 0.10874146968126297\n",
      "iteration 2360, loss value 338.3791809082031, l2 loss: 0.10874539613723755\n",
      "iteration 2361, loss value 338.37841796875, l2 loss: 0.10874935239553452\n",
      "iteration 2362, loss value 338.3781433105469, l2 loss: 0.10875330865383148\n",
      "iteration 2363, loss value 338.3780822753906, l2 loss: 0.10875727981328964\n",
      "iteration 2364, loss value 338.37774658203125, l2 loss: 0.1087612584233284\n",
      "iteration 2365, loss value 338.37738037109375, l2 loss: 0.10876524448394775\n",
      "iteration 2366, loss value 338.37725830078125, l2 loss: 0.10876923054456711\n",
      "iteration 2367, loss value 338.3765869140625, l2 loss: 0.10877320915460587\n",
      "iteration 2368, loss value 338.3764343261719, l2 loss: 0.1087772399187088\n",
      "iteration 2369, loss value 338.3761291503906, l2 loss: 0.10878122597932816\n",
      "iteration 2370, loss value 338.3757019042969, l2 loss: 0.1087852343916893\n",
      "iteration 2371, loss value 338.37530517578125, l2 loss: 0.10878928750753403\n",
      "iteration 2372, loss value 338.37493896484375, l2 loss: 0.10879331082105637\n",
      "iteration 2373, loss value 338.3747253417969, l2 loss: 0.1087973341345787\n",
      "iteration 2374, loss value 338.3741760253906, l2 loss: 0.10880137979984283\n",
      "iteration 2375, loss value 338.3739013671875, l2 loss: 0.10880544036626816\n",
      "iteration 2376, loss value 338.37408447265625, l2 loss: 0.10880949348211288\n",
      "iteration 2377, loss value 338.3737487792969, l2 loss: 0.10881353914737701\n",
      "iteration 2378, loss value 338.3730773925781, l2 loss: 0.10881762951612473\n",
      "iteration 2379, loss value 338.3731994628906, l2 loss: 0.10882169753313065\n",
      "iteration 2380, loss value 338.37225341796875, l2 loss: 0.10882578045129776\n",
      "iteration 2381, loss value 338.37237548828125, l2 loss: 0.10882987082004547\n",
      "iteration 2382, loss value 338.3719177246094, l2 loss: 0.10883399099111557\n",
      "iteration 2383, loss value 338.3716735839844, l2 loss: 0.1088380515575409\n",
      "iteration 2384, loss value 338.37127685546875, l2 loss: 0.10884217917919159\n",
      "iteration 2385, loss value 338.3708190917969, l2 loss: 0.10884632170200348\n",
      "iteration 2386, loss value 338.37066650390625, l2 loss: 0.10885044187307358\n",
      "iteration 2387, loss value 338.3700866699219, l2 loss: 0.10885456949472427\n",
      "iteration 2388, loss value 338.3700866699219, l2 loss: 0.10885871201753616\n",
      "iteration 2389, loss value 338.3695373535156, l2 loss: 0.10886285454034805\n",
      "iteration 2390, loss value 338.3697814941406, l2 loss: 0.10886700451374054\n",
      "iteration 2391, loss value 338.369384765625, l2 loss: 0.10887116193771362\n",
      "iteration 2392, loss value 338.36895751953125, l2 loss: 0.10887531191110611\n",
      "iteration 2393, loss value 338.3688659667969, l2 loss: 0.10887950658798218\n",
      "iteration 2394, loss value 338.36846923828125, l2 loss: 0.10888365656137466\n",
      "iteration 2395, loss value 338.36767578125, l2 loss: 0.10888784378767014\n",
      "iteration 2396, loss value 338.3673095703125, l2 loss: 0.10889202356338501\n",
      "iteration 2397, loss value 338.36688232421875, l2 loss: 0.10889622569084167\n",
      "iteration 2398, loss value 338.3670959472656, l2 loss: 0.10890042781829834\n",
      "iteration 2399, loss value 338.3670349121094, l2 loss: 0.1089046373963356\n",
      "iteration 2400, loss value 338.3666076660156, l2 loss: 0.10890885442495346\n",
      "iteration 2401, loss value 338.36627197265625, l2 loss: 0.10891306400299072\n",
      "iteration 2402, loss value 338.3655090332031, l2 loss: 0.10891730338335037\n",
      "iteration 2403, loss value 338.365478515625, l2 loss: 0.10892152041196823\n",
      "iteration 2404, loss value 338.3651123046875, l2 loss: 0.10892577469348907\n",
      "iteration 2405, loss value 338.3649597167969, l2 loss: 0.10893000662326813\n",
      "iteration 2406, loss value 338.3644104003906, l2 loss: 0.10893427580595016\n",
      "iteration 2407, loss value 338.3646240234375, l2 loss: 0.1089385449886322\n",
      "iteration 2408, loss value 338.3642578125, l2 loss: 0.10894279181957245\n",
      "iteration 2409, loss value 338.3637390136719, l2 loss: 0.10894706845283508\n",
      "iteration 2410, loss value 338.3636474609375, l2 loss: 0.10895133763551712\n",
      "iteration 2411, loss value 338.36309814453125, l2 loss: 0.10895560681819916\n",
      "iteration 2412, loss value 338.3622741699219, l2 loss: 0.10895992070436478\n",
      "iteration 2413, loss value 338.36279296875, l2 loss: 0.10896419733762741\n",
      "iteration 2414, loss value 338.36224365234375, l2 loss: 0.10896847397089005\n",
      "iteration 2415, loss value 338.36138916015625, l2 loss: 0.10897282510995865\n",
      "iteration 2416, loss value 338.36175537109375, l2 loss: 0.10897712409496307\n",
      "iteration 2417, loss value 338.36126708984375, l2 loss: 0.1089814305305481\n",
      "iteration 2418, loss value 338.3608093261719, l2 loss: 0.10898574441671371\n",
      "iteration 2419, loss value 338.3605651855469, l2 loss: 0.10899008810520172\n",
      "iteration 2420, loss value 338.3603515625, l2 loss: 0.10899445414543152\n",
      "iteration 2421, loss value 338.3601989746094, l2 loss: 0.10899876803159714\n",
      "iteration 2422, loss value 338.3599548339844, l2 loss: 0.10900313407182693\n",
      "iteration 2423, loss value 338.35980224609375, l2 loss: 0.10900746285915375\n",
      "iteration 2424, loss value 338.35858154296875, l2 loss: 0.10901181399822235\n",
      "iteration 2425, loss value 338.35845947265625, l2 loss: 0.10901620239019394\n",
      "iteration 2426, loss value 338.3585510253906, l2 loss: 0.10902057588100433\n",
      "iteration 2427, loss value 338.35821533203125, l2 loss: 0.10902494192123413\n",
      "iteration 2428, loss value 338.35809326171875, l2 loss: 0.10902930796146393\n",
      "iteration 2429, loss value 338.3577880859375, l2 loss: 0.10903369635343552\n",
      "iteration 2430, loss value 338.35797119140625, l2 loss: 0.10903806984424591\n",
      "iteration 2431, loss value 338.3571472167969, l2 loss: 0.10904249548912048\n",
      "iteration 2432, loss value 338.35699462890625, l2 loss: 0.10904688388109207\n",
      "iteration 2433, loss value 338.35650634765625, l2 loss: 0.10905129462480545\n",
      "iteration 2434, loss value 338.35626220703125, l2 loss: 0.10905566811561584\n",
      "iteration 2435, loss value 338.35577392578125, l2 loss: 0.10906010121107101\n",
      "iteration 2436, loss value 338.35565185546875, l2 loss: 0.10906454175710678\n",
      "iteration 2437, loss value 338.35546875, l2 loss: 0.10906895995140076\n",
      "iteration 2438, loss value 338.3549499511719, l2 loss: 0.10907338559627533\n",
      "iteration 2439, loss value 338.3548278808594, l2 loss: 0.10907779633998871\n",
      "iteration 2440, loss value 338.3542175292969, l2 loss: 0.10908226668834686\n",
      "iteration 2441, loss value 338.3541564941406, l2 loss: 0.10908671468496323\n",
      "iteration 2442, loss value 338.3536376953125, l2 loss: 0.10909117013216019\n",
      "iteration 2443, loss value 338.35369873046875, l2 loss: 0.10909561067819595\n",
      "iteration 2444, loss value 338.352783203125, l2 loss: 0.1091000884771347\n",
      "iteration 2445, loss value 338.3524475097656, l2 loss: 0.10910456627607346\n",
      "iteration 2446, loss value 338.3526611328125, l2 loss: 0.1091090515255928\n",
      "iteration 2447, loss value 338.3525695800781, l2 loss: 0.10911352187395096\n",
      "iteration 2448, loss value 338.3518981933594, l2 loss: 0.1091180294752121\n",
      "iteration 2449, loss value 338.3518371582031, l2 loss: 0.10912252217531204\n",
      "iteration 2450, loss value 338.3516540527344, l2 loss: 0.1091269999742508\n",
      "iteration 2451, loss value 338.3510437011719, l2 loss: 0.10913149267435074\n",
      "iteration 2452, loss value 338.3511657714844, l2 loss: 0.10913601517677307\n",
      "iteration 2453, loss value 338.3506164550781, l2 loss: 0.10914052277803421\n",
      "iteration 2454, loss value 338.3504943847656, l2 loss: 0.10914504528045654\n",
      "iteration 2455, loss value 338.3500671386719, l2 loss: 0.10914958268404007\n",
      "iteration 2456, loss value 338.3500671386719, l2 loss: 0.109154112637043\n",
      "iteration 2457, loss value 338.3494567871094, l2 loss: 0.10915863513946533\n",
      "iteration 2458, loss value 338.34906005859375, l2 loss: 0.10916317999362946\n",
      "iteration 2459, loss value 338.3492126464844, l2 loss: 0.10916770994663239\n",
      "iteration 2460, loss value 338.3486022949219, l2 loss: 0.1091722846031189\n",
      "iteration 2461, loss value 338.3486022949219, l2 loss: 0.10917683690786362\n",
      "iteration 2462, loss value 338.34796142578125, l2 loss: 0.10918139666318893\n",
      "iteration 2463, loss value 338.3473815917969, l2 loss: 0.10918592661619186\n",
      "iteration 2464, loss value 338.3477478027344, l2 loss: 0.10919052362442017\n",
      "iteration 2465, loss value 338.34716796875, l2 loss: 0.10919509083032608\n",
      "iteration 2466, loss value 338.34710693359375, l2 loss: 0.10919967293739319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2467, loss value 338.3468017578125, l2 loss: 0.1092042624950409\n",
      "iteration 2468, loss value 338.3463439941406, l2 loss: 0.1092088371515274\n",
      "iteration 2469, loss value 338.3461608886719, l2 loss: 0.1092134416103363\n",
      "iteration 2470, loss value 338.34576416015625, l2 loss: 0.1092180386185646\n",
      "iteration 2471, loss value 338.3455505371094, l2 loss: 0.10922261327505112\n",
      "iteration 2472, loss value 338.344970703125, l2 loss: 0.10922723263502121\n",
      "iteration 2473, loss value 338.3450622558594, l2 loss: 0.1092318445444107\n",
      "iteration 2474, loss value 338.3448791503906, l2 loss: 0.1092364490032196\n",
      "iteration 2475, loss value 338.3442687988281, l2 loss: 0.10924109071493149\n",
      "iteration 2476, loss value 338.34466552734375, l2 loss: 0.1092456728219986\n",
      "iteration 2477, loss value 338.3435974121094, l2 loss: 0.10925033688545227\n",
      "iteration 2478, loss value 338.34356689453125, l2 loss: 0.10925498604774475\n",
      "iteration 2479, loss value 338.3433532714844, l2 loss: 0.10925960540771484\n",
      "iteration 2480, loss value 338.3426513671875, l2 loss: 0.10926424711942673\n",
      "iteration 2481, loss value 338.3427429199219, l2 loss: 0.1092689111828804\n",
      "iteration 2482, loss value 338.34246826171875, l2 loss: 0.10927354544401169\n",
      "iteration 2483, loss value 338.3421325683594, l2 loss: 0.10927820205688477\n",
      "iteration 2484, loss value 338.3418884277344, l2 loss: 0.10928286612033844\n",
      "iteration 2485, loss value 338.3417663574219, l2 loss: 0.10928754508495331\n",
      "iteration 2486, loss value 338.34130859375, l2 loss: 0.10929220169782639\n",
      "iteration 2487, loss value 338.3410949707031, l2 loss: 0.10929688811302185\n",
      "iteration 2488, loss value 338.34112548828125, l2 loss: 0.10930157452821732\n",
      "iteration 2489, loss value 338.34051513671875, l2 loss: 0.10930623859167099\n",
      "iteration 2490, loss value 338.3401794433594, l2 loss: 0.10931094735860825\n",
      "iteration 2491, loss value 338.3401794433594, l2 loss: 0.10931563377380371\n",
      "iteration 2492, loss value 338.3399658203125, l2 loss: 0.10932030528783798\n",
      "iteration 2493, loss value 338.33953857421875, l2 loss: 0.10932499915361404\n",
      "iteration 2494, loss value 338.33892822265625, l2 loss: 0.1093297004699707\n",
      "iteration 2495, loss value 338.3389587402344, l2 loss: 0.10933438688516617\n",
      "iteration 2496, loss value 338.3382263183594, l2 loss: 0.10933911800384521\n",
      "iteration 2497, loss value 338.3382873535156, l2 loss: 0.10934381932020187\n",
      "iteration 2498, loss value 338.3379821777344, l2 loss: 0.10934855788946152\n",
      "iteration 2499, loss value 338.33819580078125, l2 loss: 0.10935327410697937\n",
      "iteration 2500, loss value 338.3375244140625, l2 loss: 0.10935796797275543\n",
      "iteration 2501, loss value 338.3369140625, l2 loss: 0.10936272889375687\n",
      "iteration 2502, loss value 338.3370056152344, l2 loss: 0.10936745256185532\n",
      "iteration 2503, loss value 338.3363037109375, l2 loss: 0.10937218368053436\n",
      "iteration 2504, loss value 338.3360290527344, l2 loss: 0.1093769520521164\n",
      "iteration 2505, loss value 338.3365783691406, l2 loss: 0.10938166826963425\n",
      "iteration 2506, loss value 338.33563232421875, l2 loss: 0.10938643664121628\n",
      "iteration 2507, loss value 338.3355712890625, l2 loss: 0.10939116775989532\n",
      "iteration 2508, loss value 338.33551025390625, l2 loss: 0.10939593613147736\n",
      "iteration 2509, loss value 338.33489990234375, l2 loss: 0.10940070450305939\n",
      "iteration 2510, loss value 338.334716796875, l2 loss: 0.10940544307231903\n",
      "iteration 2511, loss value 338.3346862792969, l2 loss: 0.10941023379564285\n",
      "iteration 2512, loss value 338.33447265625, l2 loss: 0.10941500216722488\n",
      "iteration 2513, loss value 338.3336486816406, l2 loss: 0.1094198003411293\n",
      "iteration 2514, loss value 338.33404541015625, l2 loss: 0.10942458361387253\n",
      "iteration 2515, loss value 338.3335876464844, l2 loss: 0.10942932963371277\n",
      "iteration 2516, loss value 338.3331298828125, l2 loss: 0.10943412780761719\n",
      "iteration 2517, loss value 338.3329162597656, l2 loss: 0.1094389259815216\n",
      "iteration 2518, loss value 338.3329772949219, l2 loss: 0.10944370925426483\n",
      "iteration 2519, loss value 338.33245849609375, l2 loss: 0.10944849252700806\n",
      "iteration 2520, loss value 338.3319091796875, l2 loss: 0.10945332050323486\n",
      "iteration 2521, loss value 338.33184814453125, l2 loss: 0.10945814847946167\n",
      "iteration 2522, loss value 338.3317565917969, l2 loss: 0.10946290194988251\n",
      "iteration 2523, loss value 338.3306579589844, l2 loss: 0.10946774482727051\n",
      "iteration 2524, loss value 338.3310852050781, l2 loss: 0.10947257280349731\n",
      "iteration 2525, loss value 338.3313293457031, l2 loss: 0.10947737097740173\n",
      "iteration 2526, loss value 338.3302307128906, l2 loss: 0.10948221385478973\n",
      "iteration 2527, loss value 338.33050537109375, l2 loss: 0.10948704928159714\n",
      "iteration 2528, loss value 338.3299560546875, l2 loss: 0.10949183255434036\n",
      "iteration 2529, loss value 338.329833984375, l2 loss: 0.10949669778347015\n",
      "iteration 2530, loss value 338.32940673828125, l2 loss: 0.10950153321027756\n",
      "iteration 2531, loss value 338.3290710449219, l2 loss: 0.10950636863708496\n",
      "iteration 2532, loss value 338.3291931152344, l2 loss: 0.10951117426156998\n",
      "iteration 2533, loss value 338.328369140625, l2 loss: 0.10951605439186096\n",
      "iteration 2534, loss value 338.3281555175781, l2 loss: 0.10952090471982956\n",
      "iteration 2535, loss value 338.3283386230469, l2 loss: 0.10952576249837875\n",
      "iteration 2536, loss value 338.32794189453125, l2 loss: 0.10953060537576675\n",
      "iteration 2537, loss value 338.3279724121094, l2 loss: 0.10953546315431595\n",
      "iteration 2538, loss value 338.3271484375, l2 loss: 0.10954033583402634\n",
      "iteration 2539, loss value 338.32733154296875, l2 loss: 0.10954520106315613\n",
      "iteration 2540, loss value 338.3269958496094, l2 loss: 0.10955005139112473\n",
      "iteration 2541, loss value 338.32672119140625, l2 loss: 0.10955492407083511\n",
      "iteration 2542, loss value 338.3262023925781, l2 loss: 0.10955982655286789\n",
      "iteration 2543, loss value 338.326416015625, l2 loss: 0.10956468433141708\n",
      "iteration 2544, loss value 338.3258056640625, l2 loss: 0.10956953465938568\n",
      "iteration 2545, loss value 338.3254089355469, l2 loss: 0.10957445204257965\n",
      "iteration 2546, loss value 338.32550048828125, l2 loss: 0.10957933962345123\n",
      "iteration 2547, loss value 338.3252868652344, l2 loss: 0.10958422720432281\n",
      "iteration 2548, loss value 338.32489013671875, l2 loss: 0.10958914458751678\n",
      "iteration 2549, loss value 338.32476806640625, l2 loss: 0.10959399491548538\n",
      "iteration 2550, loss value 338.32403564453125, l2 loss: 0.10959891974925995\n",
      "iteration 2551, loss value 338.32421875, l2 loss: 0.10960380733013153\n",
      "iteration 2552, loss value 338.3233947753906, l2 loss: 0.1096087172627449\n",
      "iteration 2553, loss value 338.32366943359375, l2 loss: 0.10961362719535828\n",
      "iteration 2554, loss value 338.3233337402344, l2 loss: 0.10961854457855225\n",
      "iteration 2555, loss value 338.32330322265625, l2 loss: 0.10962342470884323\n",
      "iteration 2556, loss value 338.3226013183594, l2 loss: 0.10962836444377899\n",
      "iteration 2557, loss value 338.3222351074219, l2 loss: 0.10963327437639236\n",
      "iteration 2558, loss value 338.322509765625, l2 loss: 0.10963819175958633\n",
      "iteration 2559, loss value 338.32232666015625, l2 loss: 0.1096431240439415\n",
      "iteration 2560, loss value 338.32171630859375, l2 loss: 0.10964805632829666\n",
      "iteration 2561, loss value 338.3212585449219, l2 loss: 0.10965298861265182\n",
      "iteration 2562, loss value 338.32122802734375, l2 loss: 0.10965791344642639\n",
      "iteration 2563, loss value 338.32049560546875, l2 loss: 0.10966287553310394\n",
      "iteration 2564, loss value 338.32086181640625, l2 loss: 0.10966779291629791\n",
      "iteration 2565, loss value 338.320556640625, l2 loss: 0.10967275500297546\n",
      "iteration 2566, loss value 338.3201599121094, l2 loss: 0.10967767983675003\n",
      "iteration 2567, loss value 338.3200378417969, l2 loss: 0.10968264192342758\n",
      "iteration 2568, loss value 338.32000732421875, l2 loss: 0.10968760401010513\n",
      "iteration 2569, loss value 338.3199157714844, l2 loss: 0.1096925362944603\n",
      "iteration 2570, loss value 338.3198547363281, l2 loss: 0.10969747602939606\n",
      "iteration 2571, loss value 338.3191833496094, l2 loss: 0.10970241576433182\n",
      "iteration 2572, loss value 338.3189392089844, l2 loss: 0.10970739275217056\n",
      "iteration 2573, loss value 338.3184814453125, l2 loss: 0.10971236228942871\n",
      "iteration 2574, loss value 338.3185729980469, l2 loss: 0.10971731692552567\n",
      "iteration 2575, loss value 338.31805419921875, l2 loss: 0.10972227901220322\n",
      "iteration 2576, loss value 338.3177185058594, l2 loss: 0.10972726345062256\n",
      "iteration 2577, loss value 338.31781005859375, l2 loss: 0.10973222553730011\n",
      "iteration 2578, loss value 338.3168640136719, l2 loss: 0.10973718762397766\n",
      "iteration 2579, loss value 338.31695556640625, l2 loss: 0.1097421646118164\n",
      "iteration 2580, loss value 338.3170166015625, l2 loss: 0.10974711924791336\n",
      "iteration 2581, loss value 338.3161926269531, l2 loss: 0.10975213348865509\n",
      "iteration 2582, loss value 338.31683349609375, l2 loss: 0.10975711792707443\n",
      "iteration 2583, loss value 338.3163757324219, l2 loss: 0.10976207256317139\n",
      "iteration 2584, loss value 338.31585693359375, l2 loss: 0.10976707935333252\n",
      "iteration 2585, loss value 338.31573486328125, l2 loss: 0.10977205634117126\n",
      "iteration 2586, loss value 338.3156433105469, l2 loss: 0.10977703332901001\n",
      "iteration 2587, loss value 338.31488037109375, l2 loss: 0.10978201031684875\n",
      "iteration 2588, loss value 338.3150329589844, l2 loss: 0.10978701710700989\n",
      "iteration 2589, loss value 338.3148193359375, l2 loss: 0.10979200899600983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2590, loss value 338.3144226074219, l2 loss: 0.10979699343442917\n",
      "iteration 2591, loss value 338.3140563964844, l2 loss: 0.1098020076751709\n",
      "iteration 2592, loss value 338.3141174316406, l2 loss: 0.10980700701475143\n",
      "iteration 2593, loss value 338.3136291503906, l2 loss: 0.10981200635433197\n",
      "iteration 2594, loss value 338.31329345703125, l2 loss: 0.10981698334217072\n",
      "iteration 2595, loss value 338.3129577636719, l2 loss: 0.10982199013233185\n",
      "iteration 2596, loss value 338.31317138671875, l2 loss: 0.10982703417539597\n",
      "iteration 2597, loss value 338.31292724609375, l2 loss: 0.10983201116323471\n",
      "iteration 2598, loss value 338.3124084472656, l2 loss: 0.10983704030513763\n",
      "iteration 2599, loss value 338.3120422363281, l2 loss: 0.10984204709529877\n",
      "iteration 2600, loss value 338.3121032714844, l2 loss: 0.10984708368778229\n",
      "iteration 2601, loss value 338.31182861328125, l2 loss: 0.10985208302736282\n",
      "iteration 2602, loss value 338.3111877441406, l2 loss: 0.10985712707042694\n",
      "iteration 2603, loss value 338.3114318847656, l2 loss: 0.10986213386058807\n",
      "iteration 2604, loss value 338.3109130859375, l2 loss: 0.1098671555519104\n",
      "iteration 2605, loss value 338.31060791015625, l2 loss: 0.10987219214439392\n",
      "iteration 2606, loss value 338.3108215332031, l2 loss: 0.10987719893455505\n",
      "iteration 2607, loss value 338.3103942871094, l2 loss: 0.10988223552703857\n",
      "iteration 2608, loss value 338.3101501464844, l2 loss: 0.10988727957010269\n",
      "iteration 2609, loss value 338.3101501464844, l2 loss: 0.1098923310637474\n",
      "iteration 2610, loss value 338.309814453125, l2 loss: 0.10989736765623093\n",
      "iteration 2611, loss value 338.30963134765625, l2 loss: 0.10990237444639206\n",
      "iteration 2612, loss value 338.3089294433594, l2 loss: 0.10990741103887558\n",
      "iteration 2613, loss value 338.30902099609375, l2 loss: 0.1099124476313591\n",
      "iteration 2614, loss value 338.3085632324219, l2 loss: 0.10991747677326202\n",
      "iteration 2615, loss value 338.30792236328125, l2 loss: 0.10992255061864853\n",
      "iteration 2616, loss value 338.30865478515625, l2 loss: 0.10992757230997086\n",
      "iteration 2617, loss value 338.3079833984375, l2 loss: 0.10993261635303497\n",
      "iteration 2618, loss value 338.3074035644531, l2 loss: 0.10993766784667969\n",
      "iteration 2619, loss value 338.3072204589844, l2 loss: 0.1099427118897438\n",
      "iteration 2620, loss value 338.3074645996094, l2 loss: 0.10994776338338852\n",
      "iteration 2621, loss value 338.3070983886719, l2 loss: 0.10995281487703323\n",
      "iteration 2622, loss value 338.30682373046875, l2 loss: 0.10995788872241974\n",
      "iteration 2623, loss value 338.30682373046875, l2 loss: 0.10996294766664505\n",
      "iteration 2624, loss value 338.3063659667969, l2 loss: 0.10996799916028976\n",
      "iteration 2625, loss value 338.3061218261719, l2 loss: 0.10997305810451508\n",
      "iteration 2626, loss value 338.3059997558594, l2 loss: 0.10997813194990158\n",
      "iteration 2627, loss value 338.3059387207031, l2 loss: 0.10998319834470749\n",
      "iteration 2628, loss value 338.30535888671875, l2 loss: 0.1099882423877716\n",
      "iteration 2629, loss value 338.3050231933594, l2 loss: 0.10999330133199692\n",
      "iteration 2630, loss value 338.30487060546875, l2 loss: 0.10999836027622223\n",
      "iteration 2631, loss value 338.304443359375, l2 loss: 0.11000343412160873\n",
      "iteration 2632, loss value 338.30419921875, l2 loss: 0.11000850796699524\n",
      "iteration 2633, loss value 338.3042907714844, l2 loss: 0.11001358181238174\n",
      "iteration 2634, loss value 338.3043518066406, l2 loss: 0.11001864820718765\n",
      "iteration 2635, loss value 338.3038024902344, l2 loss: 0.11002372950315475\n",
      "iteration 2636, loss value 338.3039245605469, l2 loss: 0.11002878844738007\n",
      "iteration 2637, loss value 338.3029479980469, l2 loss: 0.11003384739160538\n",
      "iteration 2638, loss value 338.3030700683594, l2 loss: 0.11003893613815308\n",
      "iteration 2639, loss value 338.3025817871094, l2 loss: 0.11004400998353958\n",
      "iteration 2640, loss value 338.30255126953125, l2 loss: 0.11004910618066788\n",
      "iteration 2641, loss value 338.3026123046875, l2 loss: 0.11005416512489319\n",
      "iteration 2642, loss value 338.3019714355469, l2 loss: 0.11005929112434387\n",
      "iteration 2643, loss value 338.3018798828125, l2 loss: 0.11006434261798859\n",
      "iteration 2644, loss value 338.30157470703125, l2 loss: 0.11006943881511688\n",
      "iteration 2645, loss value 338.3014831542969, l2 loss: 0.1100744977593422\n",
      "iteration 2646, loss value 338.30108642578125, l2 loss: 0.1100795716047287\n",
      "iteration 2647, loss value 338.3008117675781, l2 loss: 0.1100846603512764\n",
      "iteration 2648, loss value 338.3009948730469, l2 loss: 0.1100897416472435\n",
      "iteration 2649, loss value 338.300537109375, l2 loss: 0.1100948303937912\n",
      "iteration 2650, loss value 338.3006286621094, l2 loss: 0.1100999116897583\n",
      "iteration 2651, loss value 338.3004455566406, l2 loss: 0.110105000436306\n",
      "iteration 2652, loss value 338.3001403808594, l2 loss: 0.1101100891828537\n",
      "iteration 2653, loss value 338.29974365234375, l2 loss: 0.1101151704788208\n",
      "iteration 2654, loss value 338.2995300292969, l2 loss: 0.11012028157711029\n",
      "iteration 2655, loss value 338.29888916015625, l2 loss: 0.11012537032365799\n",
      "iteration 2656, loss value 338.2991638183594, l2 loss: 0.11013045907020569\n",
      "iteration 2657, loss value 338.2988586425781, l2 loss: 0.11013555526733398\n",
      "iteration 2658, loss value 338.2986145019531, l2 loss: 0.11014064401388168\n",
      "iteration 2659, loss value 338.2983703613281, l2 loss: 0.11014573276042938\n",
      "iteration 2660, loss value 338.2976989746094, l2 loss: 0.11015085130929947\n",
      "iteration 2661, loss value 338.29827880859375, l2 loss: 0.11015591770410538\n",
      "iteration 2662, loss value 338.2974548339844, l2 loss: 0.11016102880239487\n",
      "iteration 2663, loss value 338.2973937988281, l2 loss: 0.11016615480184555\n",
      "iteration 2664, loss value 338.29754638671875, l2 loss: 0.11017124354839325\n",
      "iteration 2665, loss value 338.29681396484375, l2 loss: 0.11017633229494095\n",
      "iteration 2666, loss value 338.2966003417969, l2 loss: 0.11018144339323044\n",
      "iteration 2667, loss value 338.2966003417969, l2 loss: 0.11018656939268112\n",
      "iteration 2668, loss value 338.2967224121094, l2 loss: 0.11019165813922882\n",
      "iteration 2669, loss value 338.2962341308594, l2 loss: 0.11019674688577652\n",
      "iteration 2670, loss value 338.29620361328125, l2 loss: 0.1102018654346466\n",
      "iteration 2671, loss value 338.2960510253906, l2 loss: 0.1102069839835167\n",
      "iteration 2672, loss value 338.29571533203125, l2 loss: 0.11021207273006439\n",
      "iteration 2673, loss value 338.2953796386719, l2 loss: 0.11021717637777328\n",
      "iteration 2674, loss value 338.2947692871094, l2 loss: 0.11022228747606277\n",
      "iteration 2675, loss value 338.2948303222656, l2 loss: 0.11022739112377167\n",
      "iteration 2676, loss value 338.2943420410156, l2 loss: 0.11023247987031937\n",
      "iteration 2677, loss value 338.2939758300781, l2 loss: 0.11023760586977005\n",
      "iteration 2678, loss value 338.29400634765625, l2 loss: 0.11024268716573715\n",
      "iteration 2679, loss value 338.29364013671875, l2 loss: 0.11024780571460724\n",
      "iteration 2680, loss value 338.29364013671875, l2 loss: 0.11025290191173553\n",
      "iteration 2681, loss value 338.2939758300781, l2 loss: 0.11025802791118622\n",
      "iteration 2682, loss value 338.29376220703125, l2 loss: 0.11026311665773392\n",
      "iteration 2683, loss value 338.2928161621094, l2 loss: 0.11026822030544281\n",
      "iteration 2684, loss value 338.2925720214844, l2 loss: 0.11027330905199051\n",
      "iteration 2685, loss value 338.2922058105469, l2 loss: 0.1102784126996994\n",
      "iteration 2686, loss value 338.2920837402344, l2 loss: 0.1102835163474083\n",
      "iteration 2687, loss value 338.2917175292969, l2 loss: 0.11028863489627838\n",
      "iteration 2688, loss value 338.29168701171875, l2 loss: 0.11029373854398727\n",
      "iteration 2689, loss value 338.29168701171875, l2 loss: 0.11029882729053497\n",
      "iteration 2690, loss value 338.2916564941406, l2 loss: 0.11030394583940506\n",
      "iteration 2691, loss value 338.2912292480469, l2 loss: 0.11030904948711395\n",
      "iteration 2692, loss value 338.2912292480469, l2 loss: 0.11031412333250046\n",
      "iteration 2693, loss value 338.2906188964844, l2 loss: 0.11031924933195114\n",
      "iteration 2694, loss value 338.29083251953125, l2 loss: 0.11032436788082123\n",
      "iteration 2695, loss value 338.29083251953125, l2 loss: 0.11032945662736893\n",
      "iteration 2696, loss value 338.2901306152344, l2 loss: 0.11033457517623901\n",
      "iteration 2697, loss value 338.2900695800781, l2 loss: 0.11033966392278671\n",
      "iteration 2698, loss value 338.2895812988281, l2 loss: 0.11034480482339859\n",
      "iteration 2699, loss value 338.2894592285156, l2 loss: 0.11034990102052689\n",
      "iteration 2700, loss value 338.28924560546875, l2 loss: 0.11035501956939697\n",
      "iteration 2701, loss value 338.28912353515625, l2 loss: 0.11036015301942825\n",
      "iteration 2702, loss value 338.28924560546875, l2 loss: 0.11036524921655655\n",
      "iteration 2703, loss value 338.2884826660156, l2 loss: 0.11037036776542664\n",
      "iteration 2704, loss value 338.2887878417969, l2 loss: 0.11037546396255493\n",
      "iteration 2705, loss value 338.28802490234375, l2 loss: 0.11038060486316681\n",
      "iteration 2706, loss value 338.28826904296875, l2 loss: 0.1103857085108757\n",
      "iteration 2707, loss value 338.28778076171875, l2 loss: 0.11039081960916519\n",
      "iteration 2708, loss value 338.2875671386719, l2 loss: 0.11039595305919647\n",
      "iteration 2709, loss value 338.28753662109375, l2 loss: 0.11040107160806656\n",
      "iteration 2710, loss value 338.2870788574219, l2 loss: 0.11040620505809784\n",
      "iteration 2711, loss value 338.28741455078125, l2 loss: 0.11041129380464554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2712, loss value 338.2866516113281, l2 loss: 0.11041642725467682\n",
      "iteration 2713, loss value 338.28656005859375, l2 loss: 0.11042153835296631\n",
      "iteration 2714, loss value 338.2864074707031, l2 loss: 0.1104266345500946\n",
      "iteration 2715, loss value 338.2859191894531, l2 loss: 0.11043178290128708\n",
      "iteration 2716, loss value 338.2859802246094, l2 loss: 0.11043687909841537\n",
      "iteration 2717, loss value 338.2853698730469, l2 loss: 0.11044200509786606\n",
      "iteration 2718, loss value 338.2857971191406, l2 loss: 0.11044713109731674\n",
      "iteration 2719, loss value 338.2857360839844, l2 loss: 0.11045224964618683\n",
      "iteration 2720, loss value 338.2854309082031, l2 loss: 0.11045736074447632\n",
      "iteration 2721, loss value 338.284912109375, l2 loss: 0.110462486743927\n",
      "iteration 2722, loss value 338.28521728515625, l2 loss: 0.1104675903916359\n",
      "iteration 2723, loss value 338.2846374511719, l2 loss: 0.11047270148992538\n",
      "iteration 2724, loss value 338.28448486328125, l2 loss: 0.11047780513763428\n",
      "iteration 2725, loss value 338.28411865234375, l2 loss: 0.11048291623592377\n",
      "iteration 2726, loss value 338.28375244140625, l2 loss: 0.11048804968595505\n",
      "iteration 2727, loss value 338.2835388183594, l2 loss: 0.11049313843250275\n",
      "iteration 2728, loss value 338.2832946777344, l2 loss: 0.11049824953079224\n",
      "iteration 2729, loss value 338.28289794921875, l2 loss: 0.11050337553024292\n",
      "iteration 2730, loss value 338.28289794921875, l2 loss: 0.11050847917795181\n",
      "iteration 2731, loss value 338.2828369140625, l2 loss: 0.1105135828256607\n",
      "iteration 2732, loss value 338.2826232910156, l2 loss: 0.11051870137453079\n",
      "iteration 2733, loss value 338.2824401855469, l2 loss: 0.11052383482456207\n",
      "iteration 2734, loss value 338.2827453613281, l2 loss: 0.11052890866994858\n",
      "iteration 2735, loss value 338.28155517578125, l2 loss: 0.11053403466939926\n",
      "iteration 2736, loss value 338.28179931640625, l2 loss: 0.11053914576768875\n",
      "iteration 2737, loss value 338.28179931640625, l2 loss: 0.11054424941539764\n",
      "iteration 2738, loss value 338.2817077636719, l2 loss: 0.11054934561252594\n",
      "iteration 2739, loss value 338.2815856933594, l2 loss: 0.11055443435907364\n",
      "iteration 2740, loss value 338.281005859375, l2 loss: 0.11055954545736313\n",
      "iteration 2741, loss value 338.2810974121094, l2 loss: 0.11056463420391083\n",
      "iteration 2742, loss value 338.28045654296875, l2 loss: 0.11056974530220032\n",
      "iteration 2743, loss value 338.2804870605469, l2 loss: 0.11057481169700623\n",
      "iteration 2744, loss value 338.28009033203125, l2 loss: 0.11057993769645691\n",
      "iteration 2745, loss value 338.27984619140625, l2 loss: 0.1105850487947464\n",
      "iteration 2746, loss value 338.2801208496094, l2 loss: 0.1105901375412941\n",
      "iteration 2747, loss value 338.2791748046875, l2 loss: 0.1105952337384224\n",
      "iteration 2748, loss value 338.279541015625, l2 loss: 0.11060033738613129\n",
      "iteration 2749, loss value 338.27960205078125, l2 loss: 0.11060544848442078\n",
      "iteration 2750, loss value 338.27886962890625, l2 loss: 0.11061053723096848\n",
      "iteration 2751, loss value 338.2790222167969, l2 loss: 0.11061561852693558\n",
      "iteration 2752, loss value 338.2785339355469, l2 loss: 0.11062072962522507\n",
      "iteration 2753, loss value 338.27838134765625, l2 loss: 0.11062581837177277\n",
      "iteration 2754, loss value 338.2782897949219, l2 loss: 0.11063091456890106\n",
      "iteration 2755, loss value 338.2777404785156, l2 loss: 0.11063601821660995\n",
      "iteration 2756, loss value 338.2777099609375, l2 loss: 0.11064109206199646\n",
      "iteration 2757, loss value 338.27728271484375, l2 loss: 0.11064617335796356\n",
      "iteration 2758, loss value 338.27703857421875, l2 loss: 0.11065127700567245\n",
      "iteration 2759, loss value 338.27685546875, l2 loss: 0.11065637320280075\n",
      "iteration 2760, loss value 338.27716064453125, l2 loss: 0.11066143214702606\n",
      "iteration 2761, loss value 338.2768249511719, l2 loss: 0.11066655069589615\n",
      "iteration 2762, loss value 338.276611328125, l2 loss: 0.11067163944244385\n",
      "iteration 2763, loss value 338.2768249511719, l2 loss: 0.11067671328783035\n",
      "iteration 2764, loss value 338.27655029296875, l2 loss: 0.11068178713321686\n",
      "iteration 2765, loss value 338.27618408203125, l2 loss: 0.11068690568208694\n",
      "iteration 2766, loss value 338.27606201171875, l2 loss: 0.11069195717573166\n",
      "iteration 2767, loss value 338.275634765625, l2 loss: 0.11069704592227936\n",
      "iteration 2768, loss value 338.2754211425781, l2 loss: 0.11070214211940765\n",
      "iteration 2769, loss value 338.2750244140625, l2 loss: 0.11070722341537476\n",
      "iteration 2770, loss value 338.27508544921875, l2 loss: 0.11071230471134186\n",
      "iteration 2771, loss value 338.27496337890625, l2 loss: 0.11071739345788956\n",
      "iteration 2772, loss value 338.27471923828125, l2 loss: 0.11072244495153427\n",
      "iteration 2773, loss value 338.27435302734375, l2 loss: 0.11072752624750137\n",
      "iteration 2774, loss value 338.2742004394531, l2 loss: 0.11073262989521027\n",
      "iteration 2775, loss value 338.2747802734375, l2 loss: 0.11073765158653259\n",
      "iteration 2776, loss value 338.27374267578125, l2 loss: 0.11074274778366089\n",
      "iteration 2777, loss value 338.2737731933594, l2 loss: 0.1107478141784668\n",
      "iteration 2778, loss value 338.2740478515625, l2 loss: 0.1107529029250145\n",
      "iteration 2779, loss value 338.2734069824219, l2 loss: 0.1107579842209816\n",
      "iteration 2780, loss value 338.2735595703125, l2 loss: 0.11076299846172333\n",
      "iteration 2781, loss value 338.272705078125, l2 loss: 0.11076807975769043\n",
      "iteration 2782, loss value 338.27301025390625, l2 loss: 0.11077314615249634\n",
      "iteration 2783, loss value 338.2723693847656, l2 loss: 0.11077818274497986\n",
      "iteration 2784, loss value 338.2722473144531, l2 loss: 0.11078327894210815\n",
      "iteration 2785, loss value 338.2726135253906, l2 loss: 0.11078834533691406\n",
      "iteration 2786, loss value 338.2723388671875, l2 loss: 0.11079338192939758\n",
      "iteration 2787, loss value 338.2718505859375, l2 loss: 0.1107984334230423\n",
      "iteration 2788, loss value 338.271728515625, l2 loss: 0.11080349236726761\n",
      "iteration 2789, loss value 338.27203369140625, l2 loss: 0.11080856621265411\n",
      "iteration 2790, loss value 338.2716979980469, l2 loss: 0.11081360280513763\n",
      "iteration 2791, loss value 338.271240234375, l2 loss: 0.11081865429878235\n",
      "iteration 2792, loss value 338.27093505859375, l2 loss: 0.11082369089126587\n",
      "iteration 2793, loss value 338.2704772949219, l2 loss: 0.11082874238491058\n",
      "iteration 2794, loss value 338.2705078125, l2 loss: 0.1108337864279747\n",
      "iteration 2795, loss value 338.27020263671875, l2 loss: 0.11083883792161942\n",
      "iteration 2796, loss value 338.2701721191406, l2 loss: 0.11084387451410294\n",
      "iteration 2797, loss value 338.26983642578125, l2 loss: 0.11084891855716705\n",
      "iteration 2798, loss value 338.2701721191406, l2 loss: 0.11085396260023117\n",
      "iteration 2799, loss value 338.26971435546875, l2 loss: 0.11085901409387589\n",
      "iteration 2800, loss value 338.26947021484375, l2 loss: 0.1108640506863594\n",
      "iteration 2801, loss value 338.2690124511719, l2 loss: 0.11086905747652054\n",
      "iteration 2802, loss value 338.2685241699219, l2 loss: 0.11087411642074585\n",
      "iteration 2803, loss value 338.2687072753906, l2 loss: 0.11087917536497116\n",
      "iteration 2804, loss value 338.2687683105469, l2 loss: 0.1108841821551323\n",
      "iteration 2805, loss value 338.2689208984375, l2 loss: 0.11088923364877701\n",
      "iteration 2806, loss value 338.2682800292969, l2 loss: 0.11089424788951874\n",
      "iteration 2807, loss value 338.2679138183594, l2 loss: 0.11089929938316345\n",
      "iteration 2808, loss value 338.267578125, l2 loss: 0.11090430617332458\n",
      "iteration 2809, loss value 338.2677307128906, l2 loss: 0.11090932041406631\n",
      "iteration 2810, loss value 338.2675476074219, l2 loss: 0.11091433465480804\n",
      "iteration 2811, loss value 338.26678466796875, l2 loss: 0.11091936379671097\n",
      "iteration 2812, loss value 338.2672424316406, l2 loss: 0.1109243854880333\n",
      "iteration 2813, loss value 338.2666931152344, l2 loss: 0.11092941462993622\n",
      "iteration 2814, loss value 338.26690673828125, l2 loss: 0.11093443632125854\n",
      "iteration 2815, loss value 338.26690673828125, l2 loss: 0.11093943566083908\n",
      "iteration 2816, loss value 338.2666320800781, l2 loss: 0.11094443500041962\n",
      "iteration 2817, loss value 338.2658386230469, l2 loss: 0.11094945669174194\n",
      "iteration 2818, loss value 338.2660217285156, l2 loss: 0.11095444113016129\n",
      "iteration 2819, loss value 338.2654724121094, l2 loss: 0.1109594851732254\n",
      "iteration 2820, loss value 338.26605224609375, l2 loss: 0.11096445471048355\n",
      "iteration 2821, loss value 338.265380859375, l2 loss: 0.11096946895122528\n",
      "iteration 2822, loss value 338.2649841308594, l2 loss: 0.11097446829080582\n",
      "iteration 2823, loss value 338.26513671875, l2 loss: 0.11097945272922516\n",
      "iteration 2824, loss value 338.2645263671875, l2 loss: 0.1109844446182251\n",
      "iteration 2825, loss value 338.26483154296875, l2 loss: 0.11098944395780563\n",
      "iteration 2826, loss value 338.26434326171875, l2 loss: 0.11099444329738617\n",
      "iteration 2827, loss value 338.2644958496094, l2 loss: 0.1109994500875473\n",
      "iteration 2828, loss value 338.26416015625, l2 loss: 0.11100441962480545\n",
      "iteration 2829, loss value 338.2640075683594, l2 loss: 0.11100941151380539\n",
      "iteration 2830, loss value 338.26385498046875, l2 loss: 0.11101443320512772\n",
      "iteration 2831, loss value 338.263671875, l2 loss: 0.11101938039064407\n",
      "iteration 2832, loss value 338.2630920410156, l2 loss: 0.11102437227964401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2833, loss value 338.26348876953125, l2 loss: 0.11102935671806335\n",
      "iteration 2834, loss value 338.2633972167969, l2 loss: 0.1110343262553215\n",
      "iteration 2835, loss value 338.26287841796875, l2 loss: 0.11103934049606323\n",
      "iteration 2836, loss value 338.2628173828125, l2 loss: 0.11104430258274078\n",
      "iteration 2837, loss value 338.26251220703125, l2 loss: 0.11104930192232132\n",
      "iteration 2838, loss value 338.2626953125, l2 loss: 0.11105425655841827\n",
      "iteration 2839, loss value 338.2620544433594, l2 loss: 0.11105921864509583\n",
      "iteration 2840, loss value 338.2618713378906, l2 loss: 0.11106421798467636\n",
      "iteration 2841, loss value 338.2623291015625, l2 loss: 0.11106915771961212\n",
      "iteration 2842, loss value 338.26171875, l2 loss: 0.11107414215803146\n",
      "iteration 2843, loss value 338.2619323730469, l2 loss: 0.11107908934354782\n",
      "iteration 2844, loss value 338.2613525390625, l2 loss: 0.11108405143022537\n",
      "iteration 2845, loss value 338.2611999511719, l2 loss: 0.11108901351690292\n",
      "iteration 2846, loss value 338.26104736328125, l2 loss: 0.11109396815299988\n",
      "iteration 2847, loss value 338.2608337402344, l2 loss: 0.11109894514083862\n",
      "iteration 2848, loss value 338.2610778808594, l2 loss: 0.11110389232635498\n",
      "iteration 2849, loss value 338.2608642578125, l2 loss: 0.11110883206129074\n",
      "iteration 2850, loss value 338.2602233886719, l2 loss: 0.1111137866973877\n",
      "iteration 2851, loss value 338.2601013183594, l2 loss: 0.11111874133348465\n",
      "iteration 2852, loss value 338.260009765625, l2 loss: 0.11112368106842041\n",
      "iteration 2853, loss value 338.25982666015625, l2 loss: 0.11112861335277557\n",
      "iteration 2854, loss value 338.2596435546875, l2 loss: 0.11113357543945312\n",
      "iteration 2855, loss value 338.2597351074219, l2 loss: 0.11113850027322769\n",
      "iteration 2856, loss value 338.2593688964844, l2 loss: 0.11114344000816345\n",
      "iteration 2857, loss value 338.2591247558594, l2 loss: 0.11114836484193802\n",
      "iteration 2858, loss value 338.25885009765625, l2 loss: 0.11115331202745438\n",
      "iteration 2859, loss value 338.2585144042969, l2 loss: 0.11115822196006775\n",
      "iteration 2860, loss value 338.25848388671875, l2 loss: 0.11116313189268112\n",
      "iteration 2861, loss value 338.2584228515625, l2 loss: 0.11116807907819748\n",
      "iteration 2862, loss value 338.2583312988281, l2 loss: 0.11117300391197205\n",
      "iteration 2863, loss value 338.2583923339844, l2 loss: 0.11117790639400482\n",
      "iteration 2864, loss value 338.2580261230469, l2 loss: 0.11118283122777939\n",
      "iteration 2865, loss value 338.2577819824219, l2 loss: 0.11118771880865097\n",
      "iteration 2866, loss value 338.25775146484375, l2 loss: 0.11119265854358673\n",
      "iteration 2867, loss value 338.2577819824219, l2 loss: 0.11119753867387772\n",
      "iteration 2868, loss value 338.2571716308594, l2 loss: 0.11120246350765228\n",
      "iteration 2869, loss value 338.2569580078125, l2 loss: 0.11120736598968506\n",
      "iteration 2870, loss value 338.25701904296875, l2 loss: 0.11121224611997604\n",
      "iteration 2871, loss value 338.2565002441406, l2 loss: 0.11121716350317001\n",
      "iteration 2872, loss value 338.2565612792969, l2 loss: 0.1112220510840416\n",
      "iteration 2873, loss value 338.25592041015625, l2 loss: 0.11122695356607437\n",
      "iteration 2874, loss value 338.2562255859375, l2 loss: 0.11123181134462357\n",
      "iteration 2875, loss value 338.25592041015625, l2 loss: 0.11123672872781754\n",
      "iteration 2876, loss value 338.2561950683594, l2 loss: 0.11124160140752792\n",
      "iteration 2877, loss value 338.25555419921875, l2 loss: 0.1112464889883995\n",
      "iteration 2878, loss value 338.255615234375, l2 loss: 0.11125136911869049\n",
      "iteration 2879, loss value 338.2552490234375, l2 loss: 0.11125624924898148\n",
      "iteration 2880, loss value 338.255126953125, l2 loss: 0.11126111447811127\n",
      "iteration 2881, loss value 338.2550048828125, l2 loss: 0.11126597970724106\n",
      "iteration 2882, loss value 338.2547302246094, l2 loss: 0.11127085238695145\n",
      "iteration 2883, loss value 338.2541809082031, l2 loss: 0.11127573251724243\n",
      "iteration 2884, loss value 338.2544860839844, l2 loss: 0.11128058284521103\n",
      "iteration 2885, loss value 338.2542419433594, l2 loss: 0.11128546297550201\n",
      "iteration 2886, loss value 338.2540283203125, l2 loss: 0.11129031330347061\n",
      "iteration 2887, loss value 338.2539978027344, l2 loss: 0.1112951785326004\n",
      "iteration 2888, loss value 338.25360107421875, l2 loss: 0.1113000363111496\n",
      "iteration 2889, loss value 338.2537536621094, l2 loss: 0.1113048866391182\n",
      "iteration 2890, loss value 338.2532653808594, l2 loss: 0.11130974441766739\n",
      "iteration 2891, loss value 338.2532653808594, l2 loss: 0.1113145723938942\n",
      "iteration 2892, loss value 338.25262451171875, l2 loss: 0.11131942272186279\n",
      "iteration 2893, loss value 338.2532043457031, l2 loss: 0.1113242581486702\n",
      "iteration 2894, loss value 338.2530212402344, l2 loss: 0.1113291084766388\n",
      "iteration 2895, loss value 338.2528076171875, l2 loss: 0.1113339513540268\n",
      "iteration 2896, loss value 338.25244140625, l2 loss: 0.1113387793302536\n",
      "iteration 2897, loss value 338.25262451171875, l2 loss: 0.11134359985589981\n",
      "iteration 2898, loss value 338.2524108886719, l2 loss: 0.11134843528270721\n",
      "iteration 2899, loss value 338.2519226074219, l2 loss: 0.11135326325893402\n",
      "iteration 2900, loss value 338.2518005371094, l2 loss: 0.11135808378458023\n",
      "iteration 2901, loss value 338.2513122558594, l2 loss: 0.11136291921138763\n",
      "iteration 2902, loss value 338.25152587890625, l2 loss: 0.11136773973703384\n",
      "iteration 2903, loss value 338.25140380859375, l2 loss: 0.11137255281209946\n",
      "iteration 2904, loss value 338.251220703125, l2 loss: 0.11137738078832626\n",
      "iteration 2905, loss value 338.2512512207031, l2 loss: 0.11138215661048889\n",
      "iteration 2906, loss value 338.25091552734375, l2 loss: 0.1113869920372963\n",
      "iteration 2907, loss value 338.2511901855469, l2 loss: 0.11139180511236191\n",
      "iteration 2908, loss value 338.2505187988281, l2 loss: 0.11139661073684692\n",
      "iteration 2909, loss value 338.250244140625, l2 loss: 0.11140140146017075\n",
      "iteration 2910, loss value 338.2500305175781, l2 loss: 0.11140618473291397\n",
      "iteration 2911, loss value 338.24981689453125, l2 loss: 0.11141099780797958\n",
      "iteration 2912, loss value 338.2497863769531, l2 loss: 0.111415795981884\n",
      "iteration 2913, loss value 338.249755859375, l2 loss: 0.11142058670520782\n",
      "iteration 2914, loss value 338.24969482421875, l2 loss: 0.11142534762620926\n",
      "iteration 2915, loss value 338.2497253417969, l2 loss: 0.11143013089895248\n",
      "iteration 2916, loss value 338.2492370605469, l2 loss: 0.1114349290728569\n",
      "iteration 2917, loss value 338.248779296875, l2 loss: 0.11143968999385834\n",
      "iteration 2918, loss value 338.24859619140625, l2 loss: 0.11144448071718216\n",
      "iteration 2919, loss value 338.2486572265625, l2 loss: 0.11144925653934479\n",
      "iteration 2920, loss value 338.2486267089844, l2 loss: 0.11145401000976562\n",
      "iteration 2921, loss value 338.2486267089844, l2 loss: 0.11145881563425064\n",
      "iteration 2922, loss value 338.2487487792969, l2 loss: 0.11146357655525208\n",
      "iteration 2923, loss value 338.24835205078125, l2 loss: 0.11146831512451172\n",
      "iteration 2924, loss value 338.24798583984375, l2 loss: 0.11147308349609375\n",
      "iteration 2925, loss value 338.2477722167969, l2 loss: 0.11147785186767578\n",
      "iteration 2926, loss value 338.247802734375, l2 loss: 0.11148257553577423\n",
      "iteration 2927, loss value 338.2474060058594, l2 loss: 0.11148736625909805\n",
      "iteration 2928, loss value 338.2476501464844, l2 loss: 0.1114921122789383\n",
      "iteration 2929, loss value 338.24749755859375, l2 loss: 0.11149685084819794\n",
      "iteration 2930, loss value 338.2469787597656, l2 loss: 0.11150157451629639\n",
      "iteration 2931, loss value 338.2468566894531, l2 loss: 0.11150632053613663\n",
      "iteration 2932, loss value 338.2464599609375, l2 loss: 0.11151108145713806\n",
      "iteration 2933, loss value 338.24676513671875, l2 loss: 0.11151577532291412\n",
      "iteration 2934, loss value 338.24627685546875, l2 loss: 0.11152052879333496\n",
      "iteration 2935, loss value 338.2464294433594, l2 loss: 0.11152523756027222\n",
      "iteration 2936, loss value 338.2456970214844, l2 loss: 0.11152997612953186\n",
      "iteration 2937, loss value 338.2461853027344, l2 loss: 0.11153469234704971\n",
      "iteration 2938, loss value 338.2456970214844, l2 loss: 0.11153943091630936\n",
      "iteration 2939, loss value 338.2455749511719, l2 loss: 0.11154411733150482\n",
      "iteration 2940, loss value 338.2455749511719, l2 loss: 0.11154884845018387\n",
      "iteration 2941, loss value 338.2453308105469, l2 loss: 0.11155355721712112\n",
      "iteration 2942, loss value 338.2449645996094, l2 loss: 0.11155826598405838\n",
      "iteration 2943, loss value 338.2449645996094, l2 loss: 0.11156294494867325\n",
      "iteration 2944, loss value 338.24456787109375, l2 loss: 0.1115676611661911\n",
      "iteration 2945, loss value 338.2445068359375, l2 loss: 0.11157234013080597\n",
      "iteration 2946, loss value 338.2445068359375, l2 loss: 0.11157704144716263\n",
      "iteration 2947, loss value 338.2442626953125, l2 loss: 0.1115817278623581\n",
      "iteration 2948, loss value 338.24444580078125, l2 loss: 0.11158643662929535\n",
      "iteration 2949, loss value 338.24432373046875, l2 loss: 0.11159108579158783\n",
      "iteration 2950, loss value 338.2435302734375, l2 loss: 0.11159578710794449\n",
      "iteration 2951, loss value 338.24359130859375, l2 loss: 0.11160047352313995\n",
      "iteration 2952, loss value 338.24346923828125, l2 loss: 0.11160514503717422\n",
      "iteration 2953, loss value 338.2431945800781, l2 loss: 0.1116098165512085\n",
      "iteration 2954, loss value 338.24359130859375, l2 loss: 0.11161446571350098\n",
      "iteration 2955, loss value 338.24310302734375, l2 loss: 0.11161912232637405\n",
      "iteration 2956, loss value 338.24285888671875, l2 loss: 0.11162378638982773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2957, loss value 338.2425231933594, l2 loss: 0.11162843555212021\n",
      "iteration 2958, loss value 338.2426452636719, l2 loss: 0.1116330698132515\n",
      "iteration 2959, loss value 338.2424621582031, l2 loss: 0.11163772642612457\n",
      "iteration 2960, loss value 338.2418518066406, l2 loss: 0.11164238303899765\n",
      "iteration 2961, loss value 338.2422790527344, l2 loss: 0.11164702475070953\n",
      "iteration 2962, loss value 338.2419128417969, l2 loss: 0.11165165901184082\n",
      "iteration 2963, loss value 338.24200439453125, l2 loss: 0.1116563007235527\n",
      "iteration 2964, loss value 338.2416687011719, l2 loss: 0.11166094243526459\n",
      "iteration 2965, loss value 338.24200439453125, l2 loss: 0.11166556179523468\n",
      "iteration 2966, loss value 338.2414245605469, l2 loss: 0.11167019605636597\n",
      "iteration 2967, loss value 338.2414855957031, l2 loss: 0.11167485266923904\n",
      "iteration 2968, loss value 338.2413024902344, l2 loss: 0.11167944967746735\n",
      "iteration 2969, loss value 338.24114990234375, l2 loss: 0.11168403923511505\n",
      "iteration 2970, loss value 338.2406311035156, l2 loss: 0.11168865114450455\n",
      "iteration 2971, loss value 338.2407531738281, l2 loss: 0.11169328540563583\n",
      "iteration 2972, loss value 338.24078369140625, l2 loss: 0.11169788241386414\n",
      "iteration 2973, loss value 338.2404479980469, l2 loss: 0.11170249432325363\n",
      "iteration 2974, loss value 338.2401123046875, l2 loss: 0.11170709133148193\n",
      "iteration 2975, loss value 338.2403259277344, l2 loss: 0.11171167343854904\n",
      "iteration 2976, loss value 338.2397766113281, l2 loss: 0.11171628534793854\n",
      "iteration 2977, loss value 338.23992919921875, l2 loss: 0.11172086745500565\n",
      "iteration 2978, loss value 338.2394104003906, l2 loss: 0.11172546446323395\n",
      "iteration 2979, loss value 338.2393493652344, l2 loss: 0.11173004657030106\n",
      "iteration 2980, loss value 338.23974609375, l2 loss: 0.11173460632562637\n",
      "iteration 2981, loss value 338.2387390136719, l2 loss: 0.11173918843269348\n",
      "iteration 2982, loss value 338.2391052246094, l2 loss: 0.1117437556385994\n",
      "iteration 2983, loss value 338.2388000488281, l2 loss: 0.11174833029508591\n",
      "iteration 2984, loss value 338.2386169433594, l2 loss: 0.11175288259983063\n",
      "iteration 2985, loss value 338.2382507324219, l2 loss: 0.11175746470689774\n",
      "iteration 2986, loss value 338.238525390625, l2 loss: 0.11176202446222305\n",
      "iteration 2987, loss value 338.23822021484375, l2 loss: 0.11176659911870956\n",
      "iteration 2988, loss value 338.2384033203125, l2 loss: 0.1117711216211319\n",
      "iteration 2989, loss value 338.23785400390625, l2 loss: 0.11177568137645721\n",
      "iteration 2990, loss value 338.2379150390625, l2 loss: 0.11178019642829895\n",
      "iteration 2991, loss value 338.23773193359375, l2 loss: 0.11178473383188248\n",
      "iteration 2992, loss value 338.23724365234375, l2 loss: 0.11178930103778839\n",
      "iteration 2993, loss value 338.23785400390625, l2 loss: 0.11179380863904953\n",
      "iteration 2994, loss value 338.23699951171875, l2 loss: 0.11179834604263306\n",
      "iteration 2995, loss value 338.23724365234375, l2 loss: 0.11180286109447479\n",
      "iteration 2996, loss value 338.2369079589844, l2 loss: 0.11180738359689713\n",
      "iteration 2997, loss value 338.23675537109375, l2 loss: 0.11181189864873886\n",
      "iteration 2998, loss value 338.236572265625, l2 loss: 0.1118164211511612\n",
      "iteration 2999, loss value 338.23681640625, l2 loss: 0.11182095110416412\n",
      "iteration 3000, loss value 338.2364501953125, l2 loss: 0.11182545125484467\n",
      "iteration 3001, loss value 338.23614501953125, l2 loss: 0.111829973757267\n",
      "iteration 3002, loss value 338.2359619140625, l2 loss: 0.11183445900678635\n",
      "iteration 3003, loss value 338.23565673828125, l2 loss: 0.11183896660804749\n",
      "iteration 3004, loss value 338.2360534667969, l2 loss: 0.11184348165988922\n",
      "iteration 3005, loss value 338.235595703125, l2 loss: 0.11184795945882797\n",
      "iteration 3006, loss value 338.23516845703125, l2 loss: 0.11185245215892792\n",
      "iteration 3007, loss value 338.23553466796875, l2 loss: 0.11185693740844727\n",
      "iteration 3008, loss value 338.23504638671875, l2 loss: 0.11186143010854721\n",
      "iteration 3009, loss value 338.2352294921875, l2 loss: 0.11186591535806656\n",
      "iteration 3010, loss value 338.23486328125, l2 loss: 0.11187038570642471\n",
      "iteration 3011, loss value 338.2347412109375, l2 loss: 0.11187482625246048\n",
      "iteration 3012, loss value 338.2344970703125, l2 loss: 0.11187932640314102\n",
      "iteration 3013, loss value 338.2344970703125, l2 loss: 0.11188377439975739\n",
      "iteration 3014, loss value 338.2342224121094, l2 loss: 0.11188824474811554\n",
      "iteration 3015, loss value 338.234375, l2 loss: 0.1118926852941513\n",
      "iteration 3016, loss value 338.2339172363281, l2 loss: 0.11189714819192886\n",
      "iteration 3017, loss value 338.23388671875, l2 loss: 0.11190158873796463\n",
      "iteration 3018, loss value 338.23406982421875, l2 loss: 0.111906036734581\n",
      "iteration 3019, loss value 338.2336120605469, l2 loss: 0.11191047728061676\n",
      "iteration 3020, loss value 338.233642578125, l2 loss: 0.11191492527723312\n",
      "iteration 3021, loss value 338.2336120605469, l2 loss: 0.11191937327384949\n",
      "iteration 3022, loss value 338.2332763671875, l2 loss: 0.11192378401756287\n",
      "iteration 3023, loss value 338.2332458496094, l2 loss: 0.11192822456359863\n",
      "iteration 3024, loss value 338.23321533203125, l2 loss: 0.11193263530731201\n",
      "iteration 3025, loss value 338.2326965332031, l2 loss: 0.11193704605102539\n",
      "iteration 3026, loss value 338.232421875, l2 loss: 0.11194147914648056\n",
      "iteration 3027, loss value 338.23272705078125, l2 loss: 0.11194588243961334\n",
      "iteration 3028, loss value 338.2320556640625, l2 loss: 0.11195029318332672\n",
      "iteration 3029, loss value 338.2323913574219, l2 loss: 0.1119546964764595\n",
      "iteration 3030, loss value 338.23199462890625, l2 loss: 0.1119590476155281\n",
      "iteration 3031, loss value 338.2314147949219, l2 loss: 0.11196347326040268\n",
      "iteration 3032, loss value 338.23193359375, l2 loss: 0.11196784675121307\n",
      "iteration 3033, loss value 338.2314147949219, l2 loss: 0.11197222024202347\n",
      "iteration 3034, loss value 338.231201171875, l2 loss: 0.11197659373283386\n",
      "iteration 3035, loss value 338.2313537597656, l2 loss: 0.11198095232248306\n",
      "iteration 3036, loss value 338.2308044433594, l2 loss: 0.11198534071445465\n",
      "iteration 3037, loss value 338.2308044433594, l2 loss: 0.11198967695236206\n",
      "iteration 3038, loss value 338.2305603027344, l2 loss: 0.11199406534433365\n",
      "iteration 3039, loss value 338.2308349609375, l2 loss: 0.11199840903282166\n",
      "iteration 3040, loss value 338.2305603027344, l2 loss: 0.11200275272130966\n",
      "iteration 3041, loss value 338.23028564453125, l2 loss: 0.11200708895921707\n",
      "iteration 3042, loss value 338.2305908203125, l2 loss: 0.11201142519712448\n",
      "iteration 3043, loss value 338.2301940917969, l2 loss: 0.11201576888561249\n",
      "iteration 3044, loss value 338.22991943359375, l2 loss: 0.1120200827717781\n",
      "iteration 3045, loss value 338.22979736328125, l2 loss: 0.11202442646026611\n",
      "iteration 3046, loss value 338.22955322265625, l2 loss: 0.11202873289585114\n",
      "iteration 3047, loss value 338.2289733886719, l2 loss: 0.11203307658433914\n",
      "iteration 3048, loss value 338.2295837402344, l2 loss: 0.11203741282224655\n",
      "iteration 3049, loss value 338.22943115234375, l2 loss: 0.11204171180725098\n",
      "iteration 3050, loss value 338.2290954589844, l2 loss: 0.1120460107922554\n",
      "iteration 3051, loss value 338.2290344238281, l2 loss: 0.11205034703016281\n",
      "iteration 3052, loss value 338.22894287109375, l2 loss: 0.11205463111400604\n",
      "iteration 3053, loss value 338.2286376953125, l2 loss: 0.11205893009901047\n",
      "iteration 3054, loss value 338.2286071777344, l2 loss: 0.11206322908401489\n",
      "iteration 3055, loss value 338.2284851074219, l2 loss: 0.11206753551959991\n",
      "iteration 3056, loss value 338.2283630371094, l2 loss: 0.11207184940576553\n",
      "iteration 3057, loss value 338.22845458984375, l2 loss: 0.11207611113786697\n",
      "iteration 3058, loss value 338.22802734375, l2 loss: 0.1120804101228714\n",
      "iteration 3059, loss value 338.2279052734375, l2 loss: 0.11208469420671463\n",
      "iteration 3060, loss value 338.2277526855469, l2 loss: 0.11208898574113846\n",
      "iteration 3061, loss value 338.22808837890625, l2 loss: 0.1120932325720787\n",
      "iteration 3062, loss value 338.2271423339844, l2 loss: 0.11209753155708313\n",
      "iteration 3063, loss value 338.2273864746094, l2 loss: 0.11210178583860397\n",
      "iteration 3064, loss value 338.2270812988281, l2 loss: 0.1121060699224472\n",
      "iteration 3065, loss value 338.2269287109375, l2 loss: 0.11211032420396805\n",
      "iteration 3066, loss value 338.2270812988281, l2 loss: 0.11211458593606949\n",
      "iteration 3067, loss value 338.2266540527344, l2 loss: 0.11211884021759033\n",
      "iteration 3068, loss value 338.22698974609375, l2 loss: 0.11212306469678879\n",
      "iteration 3069, loss value 338.2264099121094, l2 loss: 0.11212734878063202\n",
      "iteration 3070, loss value 338.2266845703125, l2 loss: 0.11213154345750809\n",
      "iteration 3071, loss value 338.2261657714844, l2 loss: 0.11213579028844833\n",
      "iteration 3072, loss value 338.22613525390625, l2 loss: 0.11214004456996918\n",
      "iteration 3073, loss value 338.226318359375, l2 loss: 0.11214425414800644\n",
      "iteration 3074, loss value 338.22613525390625, l2 loss: 0.1121484637260437\n",
      "iteration 3075, loss value 338.22576904296875, l2 loss: 0.11215269565582275\n",
      "iteration 3076, loss value 338.22607421875, l2 loss: 0.11215690523386002\n",
      "iteration 3077, loss value 338.2256774902344, l2 loss: 0.1121610775589943\n",
      "iteration 3078, loss value 338.2252197265625, l2 loss: 0.11216527968645096\n",
      "iteration 3079, loss value 338.22491455078125, l2 loss: 0.11216951906681061\n",
      "iteration 3080, loss value 338.2249755859375, l2 loss: 0.11217370629310608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3081, loss value 338.225341796875, l2 loss: 0.11217787861824036\n",
      "iteration 3082, loss value 338.2249755859375, l2 loss: 0.11218205839395523\n",
      "iteration 3083, loss value 338.2245788574219, l2 loss: 0.1121862456202507\n",
      "iteration 3084, loss value 338.22442626953125, l2 loss: 0.11219045519828796\n",
      "iteration 3085, loss value 338.22503662109375, l2 loss: 0.11219459027051926\n",
      "iteration 3086, loss value 338.2242431640625, l2 loss: 0.11219876259565353\n",
      "iteration 3087, loss value 338.2244873046875, l2 loss: 0.11220292747020721\n",
      "iteration 3088, loss value 338.2238464355469, l2 loss: 0.1122070774435997\n",
      "iteration 3089, loss value 338.2239990234375, l2 loss: 0.11221124976873398\n",
      "iteration 3090, loss value 338.22412109375, l2 loss: 0.11221540719270706\n",
      "iteration 3091, loss value 338.2238464355469, l2 loss: 0.11221950501203537\n",
      "iteration 3092, loss value 338.22357177734375, l2 loss: 0.11222367733716965\n",
      "iteration 3093, loss value 338.2233581542969, l2 loss: 0.11222781240940094\n",
      "iteration 3094, loss value 338.2229919433594, l2 loss: 0.11223194748163223\n",
      "iteration 3095, loss value 338.22332763671875, l2 loss: 0.11223609000444412\n",
      "iteration 3096, loss value 338.2229919433594, l2 loss: 0.11224021762609482\n",
      "iteration 3097, loss value 338.2233581542969, l2 loss: 0.11224436014890671\n",
      "iteration 3098, loss value 338.22296142578125, l2 loss: 0.1122484803199768\n",
      "iteration 3099, loss value 338.2227478027344, l2 loss: 0.1122526302933693\n",
      "iteration 3100, loss value 338.2225036621094, l2 loss: 0.11225671321153641\n",
      "iteration 3101, loss value 338.2225036621094, l2 loss: 0.1122608408331871\n",
      "iteration 3102, loss value 338.2226257324219, l2 loss: 0.11226493865251541\n",
      "iteration 3103, loss value 338.2222595214844, l2 loss: 0.11226905137300491\n",
      "iteration 3104, loss value 338.2218017578125, l2 loss: 0.11227314919233322\n",
      "iteration 3105, loss value 338.2218322753906, l2 loss: 0.11227725446224213\n",
      "iteration 3106, loss value 338.22186279296875, l2 loss: 0.11228133738040924\n",
      "iteration 3107, loss value 338.22161865234375, l2 loss: 0.11228542029857635\n",
      "iteration 3108, loss value 338.221435546875, l2 loss: 0.11228950321674347\n",
      "iteration 3109, loss value 338.22149658203125, l2 loss: 0.11229357123374939\n",
      "iteration 3110, loss value 338.22149658203125, l2 loss: 0.1122976616024971\n",
      "iteration 3111, loss value 338.221435546875, l2 loss: 0.11230167746543884\n",
      "iteration 3112, loss value 338.2210388183594, l2 loss: 0.11230574548244476\n",
      "iteration 3113, loss value 338.2207946777344, l2 loss: 0.11230979859828949\n",
      "iteration 3114, loss value 338.220703125, l2 loss: 0.11231385171413422\n",
      "iteration 3115, loss value 338.2208251953125, l2 loss: 0.11231787502765656\n",
      "iteration 3116, loss value 338.22039794921875, l2 loss: 0.11232192069292068\n",
      "iteration 3117, loss value 338.22076416015625, l2 loss: 0.11232596635818481\n",
      "iteration 3118, loss value 338.22039794921875, l2 loss: 0.11233000457286835\n",
      "iteration 3119, loss value 338.22021484375, l2 loss: 0.11233402043581009\n",
      "iteration 3120, loss value 338.22021484375, l2 loss: 0.11233802139759064\n",
      "iteration 3121, loss value 338.2196960449219, l2 loss: 0.11234203726053238\n",
      "iteration 3122, loss value 338.21978759765625, l2 loss: 0.11234606802463531\n",
      "iteration 3123, loss value 338.2195129394531, l2 loss: 0.11235008388757706\n",
      "iteration 3124, loss value 338.219482421875, l2 loss: 0.11235406249761581\n",
      "iteration 3125, loss value 338.2188415527344, l2 loss: 0.11235810071229935\n",
      "iteration 3126, loss value 338.21929931640625, l2 loss: 0.11236204952001572\n",
      "iteration 3127, loss value 338.2184753417969, l2 loss: 0.11236608028411865\n",
      "iteration 3128, loss value 338.2188415527344, l2 loss: 0.11237005144357681\n",
      "iteration 3129, loss value 338.2191162109375, l2 loss: 0.11237403750419617\n",
      "iteration 3130, loss value 338.21893310546875, l2 loss: 0.11237800866365433\n",
      "iteration 3131, loss value 338.21893310546875, l2 loss: 0.1123819649219513\n",
      "iteration 3132, loss value 338.2183532714844, l2 loss: 0.11238594353199005\n",
      "iteration 3133, loss value 338.21826171875, l2 loss: 0.11238988488912582\n",
      "iteration 3134, loss value 338.2178649902344, l2 loss: 0.11239385604858398\n",
      "iteration 3135, loss value 338.21820068359375, l2 loss: 0.11239781975746155\n",
      "iteration 3136, loss value 338.2181701660156, l2 loss: 0.11240177601575851\n",
      "iteration 3137, loss value 338.2181091308594, l2 loss: 0.11240573227405548\n",
      "iteration 3138, loss value 338.2176513671875, l2 loss: 0.11240964382886887\n",
      "iteration 3139, loss value 338.21722412109375, l2 loss: 0.11241358518600464\n",
      "iteration 3140, loss value 338.2174377441406, l2 loss: 0.11241751909255981\n",
      "iteration 3141, loss value 338.2172546386719, l2 loss: 0.11242147535085678\n",
      "iteration 3142, loss value 338.2171325683594, l2 loss: 0.11242540180683136\n",
      "iteration 3143, loss value 338.21746826171875, l2 loss: 0.11242932081222534\n",
      "iteration 3144, loss value 338.21697998046875, l2 loss: 0.11243323236703873\n",
      "iteration 3145, loss value 338.2170104980469, l2 loss: 0.11243713647127151\n",
      "iteration 3146, loss value 338.2168884277344, l2 loss: 0.11244107037782669\n",
      "iteration 3147, loss value 338.2170104980469, l2 loss: 0.11244495213031769\n",
      "iteration 3148, loss value 338.2165222167969, l2 loss: 0.11244885623455048\n",
      "iteration 3149, loss value 338.2166442871094, l2 loss: 0.11245277523994446\n",
      "iteration 3150, loss value 338.2161560058594, l2 loss: 0.11245666444301605\n",
      "iteration 3151, loss value 338.2161865234375, l2 loss: 0.11246054619550705\n",
      "iteration 3152, loss value 338.21624755859375, l2 loss: 0.11246442049741745\n",
      "iteration 3153, loss value 338.2161560058594, l2 loss: 0.11246829479932785\n",
      "iteration 3154, loss value 338.21563720703125, l2 loss: 0.11247216910123825\n",
      "iteration 3155, loss value 338.2154846191406, l2 loss: 0.11247606575489044\n",
      "iteration 3156, loss value 338.2154541015625, l2 loss: 0.11247989535331726\n",
      "iteration 3157, loss value 338.2151794433594, l2 loss: 0.11248380690813065\n",
      "iteration 3158, loss value 338.2155456542969, l2 loss: 0.11248762160539627\n",
      "iteration 3159, loss value 338.2150573730469, l2 loss: 0.11249148845672607\n",
      "iteration 3160, loss value 338.2150573730469, l2 loss: 0.11249532550573349\n",
      "iteration 3161, loss value 338.2149353027344, l2 loss: 0.1124991700053215\n",
      "iteration 3162, loss value 338.2149353027344, l2 loss: 0.11250299215316772\n",
      "iteration 3163, loss value 338.2144470214844, l2 loss: 0.11250683665275574\n",
      "iteration 3164, loss value 338.2149658203125, l2 loss: 0.11251064389944077\n",
      "iteration 3165, loss value 338.2142333984375, l2 loss: 0.11251448839902878\n",
      "iteration 3166, loss value 338.2144775390625, l2 loss: 0.1125183030962944\n",
      "iteration 3167, loss value 338.214111328125, l2 loss: 0.11252211034297943\n",
      "iteration 3168, loss value 338.2142639160156, l2 loss: 0.11252592504024506\n",
      "iteration 3169, loss value 338.2137145996094, l2 loss: 0.11252971738576889\n",
      "iteration 3170, loss value 338.2137451171875, l2 loss: 0.11253351718187332\n",
      "iteration 3171, loss value 338.21356201171875, l2 loss: 0.11253733187913895\n",
      "iteration 3172, loss value 338.2137145996094, l2 loss: 0.11254110932350159\n",
      "iteration 3173, loss value 338.21368408203125, l2 loss: 0.11254488676786423\n",
      "iteration 3174, loss value 338.21343994140625, l2 loss: 0.11254869401454926\n",
      "iteration 3175, loss value 338.2134094238281, l2 loss: 0.1125524640083313\n",
      "iteration 3176, loss value 338.2131042480469, l2 loss: 0.11255624890327454\n",
      "iteration 3177, loss value 338.2129211425781, l2 loss: 0.11256003379821777\n",
      "iteration 3178, loss value 338.212890625, l2 loss: 0.11256378144025803\n",
      "iteration 3179, loss value 338.21282958984375, l2 loss: 0.11256755888462067\n",
      "iteration 3180, loss value 338.21295166015625, l2 loss: 0.11257129907608032\n",
      "iteration 3181, loss value 338.2125244140625, l2 loss: 0.11257508397102356\n",
      "iteration 3182, loss value 338.21270751953125, l2 loss: 0.11257880181074142\n",
      "iteration 3183, loss value 338.2123718261719, l2 loss: 0.11258257180452347\n",
      "iteration 3184, loss value 338.21270751953125, l2 loss: 0.11258626729249954\n",
      "iteration 3185, loss value 338.2121276855469, l2 loss: 0.1125900149345398\n",
      "iteration 3186, loss value 338.21185302734375, l2 loss: 0.11259374767541885\n",
      "iteration 3187, loss value 338.2118225097656, l2 loss: 0.11259748041629791\n",
      "iteration 3188, loss value 338.212158203125, l2 loss: 0.11260116845369339\n",
      "iteration 3189, loss value 338.2113952636719, l2 loss: 0.11260487884283066\n",
      "iteration 3190, loss value 338.2119140625, l2 loss: 0.11260858178138733\n",
      "iteration 3191, loss value 338.21112060546875, l2 loss: 0.11261231452226639\n",
      "iteration 3192, loss value 338.21142578125, l2 loss: 0.11261598020792007\n",
      "iteration 3193, loss value 338.21112060546875, l2 loss: 0.11261966824531555\n",
      "iteration 3194, loss value 338.21087646484375, l2 loss: 0.11262335628271103\n",
      "iteration 3195, loss value 338.21112060546875, l2 loss: 0.1126270592212677\n",
      "iteration 3196, loss value 338.2107849121094, l2 loss: 0.11263071745634079\n",
      "iteration 3197, loss value 338.210693359375, l2 loss: 0.11263439059257507\n",
      "iteration 3198, loss value 338.2106628417969, l2 loss: 0.11263807117938995\n",
      "iteration 3199, loss value 338.2104187011719, l2 loss: 0.11264173686504364\n",
      "iteration 3200, loss value 338.21026611328125, l2 loss: 0.11264539510011673\n",
      "iteration 3201, loss value 338.2103271484375, l2 loss: 0.11264904588460922\n",
      "iteration 3202, loss value 338.20989990234375, l2 loss: 0.1126527190208435\n",
      "iteration 3203, loss value 338.2101745605469, l2 loss: 0.112656369805336\n",
      "iteration 3204, loss value 338.2101745605469, l2 loss: 0.11266002058982849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3205, loss value 338.2095947265625, l2 loss: 0.11266366392374039\n",
      "iteration 3206, loss value 338.2096862792969, l2 loss: 0.11266729980707169\n",
      "iteration 3207, loss value 338.2095642089844, l2 loss: 0.11267094314098358\n",
      "iteration 3208, loss value 338.20916748046875, l2 loss: 0.11267457157373428\n",
      "iteration 3209, loss value 338.20941162109375, l2 loss: 0.11267821490764618\n",
      "iteration 3210, loss value 338.20947265625, l2 loss: 0.11268185824155807\n",
      "iteration 3211, loss value 338.20947265625, l2 loss: 0.1126854419708252\n",
      "iteration 3212, loss value 338.20928955078125, l2 loss: 0.1126890480518341\n",
      "iteration 3213, loss value 338.2090148925781, l2 loss: 0.11269263923168182\n",
      "iteration 3214, loss value 338.20904541015625, l2 loss: 0.11269624531269073\n",
      "iteration 3215, loss value 338.2088623046875, l2 loss: 0.11269985139369965\n",
      "iteration 3216, loss value 338.2088317871094, l2 loss: 0.11270344257354736\n",
      "iteration 3217, loss value 338.208740234375, l2 loss: 0.11270701885223389\n",
      "iteration 3218, loss value 338.2084045410156, l2 loss: 0.11271059513092041\n",
      "iteration 3219, loss value 338.2080993652344, l2 loss: 0.11271416395902634\n",
      "iteration 3220, loss value 338.2079772949219, l2 loss: 0.11271773278713226\n",
      "iteration 3221, loss value 338.2080078125, l2 loss: 0.1127212718129158\n",
      "iteration 3222, loss value 338.2073669433594, l2 loss: 0.11272482573986053\n",
      "iteration 3223, loss value 338.20745849609375, l2 loss: 0.11272839456796646\n",
      "iteration 3224, loss value 338.20745849609375, l2 loss: 0.1127319410443306\n",
      "iteration 3225, loss value 338.2075500488281, l2 loss: 0.11273549497127533\n",
      "iteration 3226, loss value 338.207763671875, l2 loss: 0.11273898184299469\n",
      "iteration 3227, loss value 338.2073669433594, l2 loss: 0.11274252086877823\n",
      "iteration 3228, loss value 338.2073669433594, l2 loss: 0.11274605244398117\n",
      "iteration 3229, loss value 338.20721435546875, l2 loss: 0.11274958401918411\n",
      "iteration 3230, loss value 338.2074890136719, l2 loss: 0.11275310069322586\n",
      "iteration 3231, loss value 338.2070007324219, l2 loss: 0.11275661736726761\n",
      "iteration 3232, loss value 338.2069091796875, l2 loss: 0.11276011168956757\n",
      "iteration 3233, loss value 338.2061767578125, l2 loss: 0.11276362836360931\n",
      "iteration 3234, loss value 338.2066345214844, l2 loss: 0.11276713013648987\n",
      "iteration 3235, loss value 338.2066650390625, l2 loss: 0.11277060955762863\n",
      "iteration 3236, loss value 338.2065124511719, l2 loss: 0.1127740815281868\n",
      "iteration 3237, loss value 338.2060546875, l2 loss: 0.11277757585048676\n",
      "iteration 3238, loss value 338.20587158203125, l2 loss: 0.11278104782104492\n",
      "iteration 3239, loss value 338.2060546875, l2 loss: 0.11278454959392548\n",
      "iteration 3240, loss value 338.2060546875, l2 loss: 0.11278799921274185\n",
      "iteration 3241, loss value 338.2057189941406, l2 loss: 0.11279145628213882\n",
      "iteration 3242, loss value 338.2056884765625, l2 loss: 0.1127949208021164\n",
      "iteration 3243, loss value 338.20574951171875, l2 loss: 0.11279836297035217\n",
      "iteration 3244, loss value 338.2053527832031, l2 loss: 0.11280181258916855\n",
      "iteration 3245, loss value 338.2057800292969, l2 loss: 0.11280528455972672\n",
      "iteration 3246, loss value 338.2057800292969, l2 loss: 0.1128087192773819\n",
      "iteration 3247, loss value 338.20526123046875, l2 loss: 0.11281215399503708\n",
      "iteration 3248, loss value 338.205078125, l2 loss: 0.11281559616327286\n",
      "iteration 3249, loss value 338.2052917480469, l2 loss: 0.11281903833150864\n",
      "iteration 3250, loss value 338.20513916015625, l2 loss: 0.11282243579626083\n",
      "iteration 3251, loss value 338.2049255371094, l2 loss: 0.11282584816217422\n",
      "iteration 3252, loss value 338.2043151855469, l2 loss: 0.11282929033041\n",
      "iteration 3253, loss value 338.2047424316406, l2 loss: 0.1128326952457428\n",
      "iteration 3254, loss value 338.20404052734375, l2 loss: 0.11283611506223679\n",
      "iteration 3255, loss value 338.2044372558594, l2 loss: 0.11283953487873077\n",
      "iteration 3256, loss value 338.20428466796875, l2 loss: 0.11284292489290237\n",
      "iteration 3257, loss value 338.20391845703125, l2 loss: 0.11284634470939636\n",
      "iteration 3258, loss value 338.2042236328125, l2 loss: 0.11284970492124557\n",
      "iteration 3259, loss value 338.20391845703125, l2 loss: 0.11285310238599777\n",
      "iteration 3260, loss value 338.2038269042969, l2 loss: 0.11285647004842758\n",
      "iteration 3261, loss value 338.20355224609375, l2 loss: 0.11285986751317978\n",
      "iteration 3262, loss value 338.2035827636719, l2 loss: 0.11286322772502899\n",
      "iteration 3263, loss value 338.2038269042969, l2 loss: 0.11286658048629761\n",
      "iteration 3264, loss value 338.2038269042969, l2 loss: 0.11286996304988861\n",
      "iteration 3265, loss value 338.203125, l2 loss: 0.11287330836057663\n",
      "iteration 3266, loss value 338.2032470703125, l2 loss: 0.11287665367126465\n",
      "iteration 3267, loss value 338.2030944824219, l2 loss: 0.11288000643253326\n",
      "iteration 3268, loss value 338.202880859375, l2 loss: 0.11288333684206009\n",
      "iteration 3269, loss value 338.2027893066406, l2 loss: 0.11288665980100632\n",
      "iteration 3270, loss value 338.2026062011719, l2 loss: 0.11288996785879135\n",
      "iteration 3271, loss value 338.2025146484375, l2 loss: 0.11289327591657639\n",
      "iteration 3272, loss value 338.2021789550781, l2 loss: 0.11289661377668381\n",
      "iteration 3273, loss value 338.2028503417969, l2 loss: 0.11289990693330765\n",
      "iteration 3274, loss value 338.2024841308594, l2 loss: 0.11290320008993149\n",
      "iteration 3275, loss value 338.20220947265625, l2 loss: 0.11290648579597473\n",
      "iteration 3276, loss value 338.2016906738281, l2 loss: 0.11290978640317917\n",
      "iteration 3277, loss value 338.2022399902344, l2 loss: 0.11291307955980301\n",
      "iteration 3278, loss value 338.20196533203125, l2 loss: 0.11291635781526566\n",
      "iteration 3279, loss value 338.20196533203125, l2 loss: 0.1129196435213089\n",
      "iteration 3280, loss value 338.20166015625, l2 loss: 0.11292292922735214\n",
      "iteration 3281, loss value 338.2017517089844, l2 loss: 0.1129261925816536\n",
      "iteration 3282, loss value 338.2008972167969, l2 loss: 0.11292947828769684\n",
      "iteration 3283, loss value 338.2017517089844, l2 loss: 0.11293275654315948\n",
      "iteration 3284, loss value 338.2010498046875, l2 loss: 0.11293603479862213\n",
      "iteration 3285, loss value 338.20147705078125, l2 loss: 0.11293925344944\n",
      "iteration 3286, loss value 338.2008056640625, l2 loss: 0.11294254660606384\n",
      "iteration 3287, loss value 338.2009582519531, l2 loss: 0.11294578015804291\n",
      "iteration 3288, loss value 338.2007751464844, l2 loss: 0.11294904351234436\n",
      "iteration 3289, loss value 338.20086669921875, l2 loss: 0.11295225471258163\n",
      "iteration 3290, loss value 338.2006530761719, l2 loss: 0.1129554957151413\n",
      "iteration 3291, loss value 338.2002258300781, l2 loss: 0.11295874416828156\n",
      "iteration 3292, loss value 338.200439453125, l2 loss: 0.11296196281909943\n",
      "iteration 3293, loss value 338.2007751464844, l2 loss: 0.1129651740193367\n",
      "iteration 3294, loss value 338.2005310058594, l2 loss: 0.11296836286783218\n",
      "iteration 3295, loss value 338.2002258300781, l2 loss: 0.11297156661748886\n",
      "iteration 3296, loss value 338.20025634765625, l2 loss: 0.11297478526830673\n",
      "iteration 3297, loss value 338.19976806640625, l2 loss: 0.11297797411680222\n",
      "iteration 3298, loss value 338.1994323730469, l2 loss: 0.11298117786645889\n",
      "iteration 3299, loss value 338.1997985839844, l2 loss: 0.11298435926437378\n",
      "iteration 3300, loss value 338.1995544433594, l2 loss: 0.11298752576112747\n",
      "iteration 3301, loss value 338.19964599609375, l2 loss: 0.11299071460962296\n",
      "iteration 3302, loss value 338.1991882324219, l2 loss: 0.11299387365579605\n",
      "iteration 3303, loss value 338.1990966796875, l2 loss: 0.11299705505371094\n",
      "iteration 3304, loss value 338.1990661621094, l2 loss: 0.11300019174814224\n",
      "iteration 3305, loss value 338.1988830566406, l2 loss: 0.11300335824489594\n",
      "iteration 3306, loss value 338.1987609863281, l2 loss: 0.11300650984048843\n",
      "iteration 3307, loss value 338.19879150390625, l2 loss: 0.11300964653491974\n",
      "iteration 3308, loss value 338.19879150390625, l2 loss: 0.11301279813051224\n",
      "iteration 3309, loss value 338.1990661621094, l2 loss: 0.11301592737436295\n",
      "iteration 3310, loss value 338.1987609863281, l2 loss: 0.11301903426647186\n",
      "iteration 3311, loss value 338.1983642578125, l2 loss: 0.11302217841148376\n",
      "iteration 3312, loss value 338.19842529296875, l2 loss: 0.11302530765533447\n",
      "iteration 3313, loss value 338.1983337402344, l2 loss: 0.11302841454744339\n",
      "iteration 3314, loss value 338.1980895996094, l2 loss: 0.1130315363407135\n",
      "iteration 3315, loss value 338.19775390625, l2 loss: 0.11303466558456421\n",
      "iteration 3316, loss value 338.19805908203125, l2 loss: 0.11303777247667313\n",
      "iteration 3317, loss value 338.19793701171875, l2 loss: 0.11304084211587906\n",
      "iteration 3318, loss value 338.1979064941406, l2 loss: 0.11304394900798798\n",
      "iteration 3319, loss value 338.1974182128906, l2 loss: 0.1130470335483551\n",
      "iteration 3320, loss value 338.1979064941406, l2 loss: 0.11305012553930283\n",
      "iteration 3321, loss value 338.19720458984375, l2 loss: 0.11305321007966995\n",
      "iteration 3322, loss value 338.1976013183594, l2 loss: 0.11305627971887589\n",
      "iteration 3323, loss value 338.19732666015625, l2 loss: 0.1130593791604042\n",
      "iteration 3324, loss value 338.1974792480469, l2 loss: 0.11306242644786835\n",
      "iteration 3325, loss value 338.1970520019531, l2 loss: 0.11306547373533249\n",
      "iteration 3326, loss value 338.19720458984375, l2 loss: 0.11306855082511902\n",
      "iteration 3327, loss value 338.1966247558594, l2 loss: 0.11307160556316376\n",
      "iteration 3328, loss value 338.1969909667969, l2 loss: 0.1130746454000473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3329, loss value 338.19732666015625, l2 loss: 0.11307770013809204\n",
      "iteration 3330, loss value 338.19671630859375, l2 loss: 0.11308072507381439\n",
      "iteration 3331, loss value 338.1966247558594, l2 loss: 0.11308374255895615\n",
      "iteration 3332, loss value 338.19635009765625, l2 loss: 0.1130867525935173\n",
      "iteration 3333, loss value 338.1958923339844, l2 loss: 0.11308977007865906\n",
      "iteration 3334, loss value 338.19622802734375, l2 loss: 0.1130928099155426\n",
      "iteration 3335, loss value 338.19610595703125, l2 loss: 0.11309581995010376\n",
      "iteration 3336, loss value 338.19622802734375, l2 loss: 0.11309882253408432\n",
      "iteration 3337, loss value 338.19598388671875, l2 loss: 0.11310180276632309\n",
      "iteration 3338, loss value 338.1958312988281, l2 loss: 0.11310482770204544\n",
      "iteration 3339, loss value 338.195556640625, l2 loss: 0.1131078377366066\n",
      "iteration 3340, loss value 338.1954345703125, l2 loss: 0.11311081051826477\n",
      "iteration 3341, loss value 338.19561767578125, l2 loss: 0.11311378329992294\n",
      "iteration 3342, loss value 338.1954040527344, l2 loss: 0.11311674863100052\n",
      "iteration 3343, loss value 338.1950378417969, l2 loss: 0.1131197139620781\n",
      "iteration 3344, loss value 338.19512939453125, l2 loss: 0.11312268674373627\n",
      "iteration 3345, loss value 338.19488525390625, l2 loss: 0.11312562227249146\n",
      "iteration 3346, loss value 338.19500732421875, l2 loss: 0.11312856525182724\n",
      "iteration 3347, loss value 338.1947021484375, l2 loss: 0.11313150078058243\n",
      "iteration 3348, loss value 338.1944580078125, l2 loss: 0.11313444375991821\n",
      "iteration 3349, loss value 338.1944885253906, l2 loss: 0.1131373792886734\n",
      "iteration 3350, loss value 338.1944885253906, l2 loss: 0.1131402999162674\n",
      "iteration 3351, loss value 338.1946716308594, l2 loss: 0.11314322799444199\n",
      "iteration 3352, loss value 338.1946105957031, l2 loss: 0.11314616352319717\n",
      "iteration 3353, loss value 338.19427490234375, l2 loss: 0.11314906924962997\n",
      "iteration 3354, loss value 338.1940002441406, l2 loss: 0.11315198987722397\n",
      "iteration 3355, loss value 338.1944580078125, l2 loss: 0.11315488815307617\n",
      "iteration 3356, loss value 338.19427490234375, l2 loss: 0.11315779387950897\n",
      "iteration 3357, loss value 338.1941833496094, l2 loss: 0.11316069960594177\n",
      "iteration 3358, loss value 338.19427490234375, l2 loss: 0.11316357553005219\n",
      "iteration 3359, loss value 338.19354248046875, l2 loss: 0.11316647380590439\n",
      "iteration 3360, loss value 338.193603515625, l2 loss: 0.1131693497300148\n",
      "iteration 3361, loss value 338.19378662109375, l2 loss: 0.11317223310470581\n",
      "iteration 3362, loss value 338.1940612792969, l2 loss: 0.11317508667707443\n",
      "iteration 3363, loss value 338.1930847167969, l2 loss: 0.11317797750234604\n",
      "iteration 3364, loss value 338.1939392089844, l2 loss: 0.11318084597587585\n",
      "iteration 3365, loss value 338.19354248046875, l2 loss: 0.11318371444940567\n",
      "iteration 3366, loss value 338.19329833984375, l2 loss: 0.1131865531206131\n",
      "iteration 3367, loss value 338.1929626464844, l2 loss: 0.11318941414356232\n",
      "iteration 3368, loss value 338.1925354003906, l2 loss: 0.11319226771593094\n",
      "iteration 3369, loss value 338.1924743652344, l2 loss: 0.11319511383771896\n",
      "iteration 3370, loss value 338.19268798828125, l2 loss: 0.11319797486066818\n",
      "iteration 3371, loss value 338.19293212890625, l2 loss: 0.1132008284330368\n",
      "iteration 3372, loss value 338.19305419921875, l2 loss: 0.11320365220308304\n",
      "iteration 3373, loss value 338.1928405761719, l2 loss: 0.11320646107196808\n",
      "iteration 3374, loss value 338.19256591796875, l2 loss: 0.1132092997431755\n",
      "iteration 3375, loss value 338.19232177734375, l2 loss: 0.11321212351322174\n",
      "iteration 3376, loss value 338.1922302246094, l2 loss: 0.11321492493152618\n",
      "iteration 3377, loss value 338.1919860839844, l2 loss: 0.11321772634983063\n",
      "iteration 3378, loss value 338.19189453125, l2 loss: 0.11322055011987686\n",
      "iteration 3379, loss value 338.1920166015625, l2 loss: 0.11322334408760071\n",
      "iteration 3380, loss value 338.1919860839844, l2 loss: 0.11322613805532455\n",
      "iteration 3381, loss value 338.191650390625, l2 loss: 0.1132289245724678\n",
      "iteration 3382, loss value 338.1912841796875, l2 loss: 0.11323170363903046\n",
      "iteration 3383, loss value 338.19146728515625, l2 loss: 0.11323446035385132\n",
      "iteration 3384, loss value 338.1910095214844, l2 loss: 0.11323723942041397\n",
      "iteration 3385, loss value 338.19140625, l2 loss: 0.11323999613523483\n",
      "iteration 3386, loss value 338.1910400390625, l2 loss: 0.11324277520179749\n",
      "iteration 3387, loss value 338.19146728515625, l2 loss: 0.11324550956487656\n",
      "iteration 3388, loss value 338.1910095214844, l2 loss: 0.11324826627969742\n",
      "iteration 3389, loss value 338.1910095214844, l2 loss: 0.11325100064277649\n",
      "iteration 3390, loss value 338.19134521484375, l2 loss: 0.11325372010469437\n",
      "iteration 3391, loss value 338.1907043457031, l2 loss: 0.11325645446777344\n",
      "iteration 3392, loss value 338.1908264160156, l2 loss: 0.11325918138027191\n",
      "iteration 3393, loss value 338.19085693359375, l2 loss: 0.11326190084218979\n",
      "iteration 3394, loss value 338.1905822753906, l2 loss: 0.11326460540294647\n",
      "iteration 3395, loss value 338.1906433105469, l2 loss: 0.11326732486486435\n",
      "iteration 3396, loss value 338.1901550292969, l2 loss: 0.11327001452445984\n",
      "iteration 3397, loss value 338.1900939941406, l2 loss: 0.11327269673347473\n",
      "iteration 3398, loss value 338.19012451171875, l2 loss: 0.11327540874481201\n",
      "iteration 3399, loss value 338.1897888183594, l2 loss: 0.11327812075614929\n",
      "iteration 3400, loss value 338.18988037109375, l2 loss: 0.11328081041574478\n",
      "iteration 3401, loss value 338.19012451171875, l2 loss: 0.11328347027301788\n",
      "iteration 3402, loss value 338.18975830078125, l2 loss: 0.11328612267971039\n",
      "iteration 3403, loss value 338.1894226074219, l2 loss: 0.11328878998756409\n",
      "iteration 3404, loss value 338.18939208984375, l2 loss: 0.11329147219657898\n",
      "iteration 3405, loss value 338.1894836425781, l2 loss: 0.11329411715269089\n",
      "iteration 3406, loss value 338.1895446777344, l2 loss: 0.1132967621088028\n",
      "iteration 3407, loss value 338.1893005371094, l2 loss: 0.11329944431781769\n",
      "iteration 3408, loss value 338.18963623046875, l2 loss: 0.1133020669221878\n",
      "iteration 3409, loss value 338.1889343261719, l2 loss: 0.1133047342300415\n",
      "iteration 3410, loss value 338.1894226074219, l2 loss: 0.11330737918615341\n",
      "iteration 3411, loss value 338.1893615722656, l2 loss: 0.11331002414226532\n",
      "iteration 3412, loss value 338.1889343261719, l2 loss: 0.11331262439489365\n",
      "iteration 3413, loss value 338.1889343261719, l2 loss: 0.11331525444984436\n",
      "iteration 3414, loss value 338.18890380859375, l2 loss: 0.11331787705421448\n",
      "iteration 3415, loss value 338.1885070800781, l2 loss: 0.113320492208004\n",
      "iteration 3416, loss value 338.1882019042969, l2 loss: 0.11332309991121292\n",
      "iteration 3417, loss value 338.18853759765625, l2 loss: 0.11332569271326065\n",
      "iteration 3418, loss value 338.18829345703125, l2 loss: 0.11332830041646957\n",
      "iteration 3419, loss value 338.1881103515625, l2 loss: 0.1133308932185173\n",
      "iteration 3420, loss value 338.1882019042969, l2 loss: 0.11333349347114563\n",
      "iteration 3421, loss value 338.188232421875, l2 loss: 0.11333609372377396\n",
      "iteration 3422, loss value 338.18804931640625, l2 loss: 0.1133386567234993\n",
      "iteration 3423, loss value 338.1879577636719, l2 loss: 0.11334123462438583\n",
      "iteration 3424, loss value 338.18804931640625, l2 loss: 0.11334379017353058\n",
      "iteration 3425, loss value 338.18792724609375, l2 loss: 0.11334634572267532\n",
      "iteration 3426, loss value 338.1875, l2 loss: 0.11334889382123947\n",
      "iteration 3427, loss value 338.1873474121094, l2 loss: 0.11335145682096481\n",
      "iteration 3428, loss value 338.18695068359375, l2 loss: 0.11335401982069016\n",
      "iteration 3429, loss value 338.18719482421875, l2 loss: 0.11335655301809311\n",
      "iteration 3430, loss value 338.1872253417969, l2 loss: 0.11335909366607666\n",
      "iteration 3431, loss value 338.1875, l2 loss: 0.11336161196231842\n",
      "iteration 3432, loss value 338.186767578125, l2 loss: 0.11336415261030197\n",
      "iteration 3433, loss value 338.18701171875, l2 loss: 0.11336665600538254\n",
      "iteration 3434, loss value 338.1870422363281, l2 loss: 0.11336918920278549\n",
      "iteration 3435, loss value 338.18682861328125, l2 loss: 0.11337166279554367\n",
      "iteration 3436, loss value 338.1864929199219, l2 loss: 0.11337420344352722\n",
      "iteration 3437, loss value 338.18682861328125, l2 loss: 0.1133766695857048\n",
      "iteration 3438, loss value 338.1867980957031, l2 loss: 0.11337918788194656\n",
      "iteration 3439, loss value 338.1866760253906, l2 loss: 0.11338164657354355\n",
      "iteration 3440, loss value 338.1864929199219, l2 loss: 0.11338413506746292\n",
      "iteration 3441, loss value 338.18646240234375, l2 loss: 0.1133866235613823\n",
      "iteration 3442, loss value 338.18634033203125, l2 loss: 0.11338913440704346\n",
      "iteration 3443, loss value 338.1864013671875, l2 loss: 0.11339160799980164\n",
      "iteration 3444, loss value 338.18609619140625, l2 loss: 0.11339403688907623\n",
      "iteration 3445, loss value 338.1858825683594, l2 loss: 0.113396555185318\n",
      "iteration 3446, loss value 338.186279296875, l2 loss: 0.11339900642633438\n",
      "iteration 3447, loss value 338.185791015625, l2 loss: 0.11340145021677017\n",
      "iteration 3448, loss value 338.18548583984375, l2 loss: 0.11340390890836716\n",
      "iteration 3449, loss value 338.1853942871094, l2 loss: 0.11340638250112534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3450, loss value 338.18609619140625, l2 loss: 0.11340880393981934\n",
      "iteration 3451, loss value 338.1856384277344, l2 loss: 0.11341123282909393\n",
      "iteration 3452, loss value 338.185302734375, l2 loss: 0.11341368407011032\n",
      "iteration 3453, loss value 338.1855163574219, l2 loss: 0.11341611295938492\n",
      "iteration 3454, loss value 338.1851806640625, l2 loss: 0.11341851949691772\n",
      "iteration 3455, loss value 338.18487548828125, l2 loss: 0.11342093348503113\n",
      "iteration 3456, loss value 338.1845397949219, l2 loss: 0.11342335492372513\n",
      "iteration 3457, loss value 338.18475341796875, l2 loss: 0.11342576146125793\n",
      "iteration 3458, loss value 338.1847229003906, l2 loss: 0.11342817544937134\n",
      "iteration 3459, loss value 338.18524169921875, l2 loss: 0.11343053728342056\n",
      "iteration 3460, loss value 338.18463134765625, l2 loss: 0.11343292146921158\n",
      "iteration 3461, loss value 338.1846923828125, l2 loss: 0.1134352833032608\n",
      "iteration 3462, loss value 338.184326171875, l2 loss: 0.11343764513731003\n",
      "iteration 3463, loss value 338.18438720703125, l2 loss: 0.11344005912542343\n",
      "iteration 3464, loss value 338.184326171875, l2 loss: 0.11344239115715027\n",
      "iteration 3465, loss value 338.1843566894531, l2 loss: 0.11344476789236069\n",
      "iteration 3466, loss value 338.1841735839844, l2 loss: 0.1134471446275711\n",
      "iteration 3467, loss value 338.18414306640625, l2 loss: 0.11344944685697556\n",
      "iteration 3468, loss value 338.1837158203125, l2 loss: 0.11345185339450836\n",
      "iteration 3469, loss value 338.18389892578125, l2 loss: 0.1134541854262352\n",
      "iteration 3470, loss value 338.18389892578125, l2 loss: 0.11345653980970383\n",
      "iteration 3471, loss value 338.1836853027344, l2 loss: 0.11345887929201126\n",
      "iteration 3472, loss value 338.1836853027344, l2 loss: 0.1134612113237381\n",
      "iteration 3473, loss value 338.1833190917969, l2 loss: 0.11346353590488434\n",
      "iteration 3474, loss value 338.1838073730469, l2 loss: 0.11346586048603058\n",
      "iteration 3475, loss value 338.1833190917969, l2 loss: 0.11346818506717682\n",
      "iteration 3476, loss value 338.1834716796875, l2 loss: 0.11347051709890366\n",
      "iteration 3477, loss value 338.1835632324219, l2 loss: 0.11347278207540512\n",
      "iteration 3478, loss value 338.18310546875, l2 loss: 0.11347512155771255\n",
      "iteration 3479, loss value 338.18316650390625, l2 loss: 0.11347739398479462\n",
      "iteration 3480, loss value 338.18267822265625, l2 loss: 0.11347965896129608\n",
      "iteration 3481, loss value 338.1827087402344, l2 loss: 0.11348196119070053\n",
      "iteration 3482, loss value 338.1827392578125, l2 loss: 0.11348424851894379\n",
      "iteration 3483, loss value 338.1830749511719, l2 loss: 0.11348651349544525\n",
      "iteration 3484, loss value 338.1827392578125, l2 loss: 0.11348876357078552\n",
      "iteration 3485, loss value 338.1824645996094, l2 loss: 0.11349104344844818\n",
      "iteration 3486, loss value 338.18267822265625, l2 loss: 0.11349326372146606\n",
      "iteration 3487, loss value 338.1820373535156, l2 loss: 0.11349552124738693\n",
      "iteration 3488, loss value 338.181884765625, l2 loss: 0.1134977713227272\n",
      "iteration 3489, loss value 338.1821594238281, l2 loss: 0.11350000649690628\n",
      "iteration 3490, loss value 338.18231201171875, l2 loss: 0.11350226402282715\n",
      "iteration 3491, loss value 338.18206787109375, l2 loss: 0.11350449174642563\n",
      "iteration 3492, loss value 338.18194580078125, l2 loss: 0.11350671947002411\n",
      "iteration 3493, loss value 338.1817932128906, l2 loss: 0.11350896209478378\n",
      "iteration 3494, loss value 338.1817626953125, l2 loss: 0.11351117491722107\n",
      "iteration 3495, loss value 338.1814880371094, l2 loss: 0.11351338028907776\n",
      "iteration 3496, loss value 338.1814270019531, l2 loss: 0.11351557075977325\n",
      "iteration 3497, loss value 338.1814270019531, l2 loss: 0.11351778358221054\n",
      "iteration 3498, loss value 338.1816101074219, l2 loss: 0.11351998895406723\n",
      "iteration 3499, loss value 338.18157958984375, l2 loss: 0.11352217942476273\n",
      "iteration 3500, loss value 338.18133544921875, l2 loss: 0.11352437734603882\n",
      "iteration 3501, loss value 338.1811828613281, l2 loss: 0.11352658271789551\n",
      "iteration 3502, loss value 338.18121337890625, l2 loss: 0.11352875828742981\n",
      "iteration 3503, loss value 338.18109130859375, l2 loss: 0.11353093385696411\n",
      "iteration 3504, loss value 338.18096923828125, l2 loss: 0.11353311687707901\n",
      "iteration 3505, loss value 338.1805419921875, l2 loss: 0.11353527009487152\n",
      "iteration 3506, loss value 338.1806640625, l2 loss: 0.11353744566440582\n",
      "iteration 3507, loss value 338.1806335449219, l2 loss: 0.11353962123394012\n",
      "iteration 3508, loss value 338.1806640625, l2 loss: 0.11354177445173264\n",
      "iteration 3509, loss value 338.1806335449219, l2 loss: 0.11354392021894455\n",
      "iteration 3510, loss value 338.1806335449219, l2 loss: 0.11354604363441467\n",
      "iteration 3511, loss value 338.1805114746094, l2 loss: 0.1135481745004654\n",
      "iteration 3512, loss value 338.18011474609375, l2 loss: 0.11355030536651611\n",
      "iteration 3513, loss value 338.1805419921875, l2 loss: 0.11355243623256683\n",
      "iteration 3514, loss value 338.18011474609375, l2 loss: 0.11355453729629517\n",
      "iteration 3515, loss value 338.1801452636719, l2 loss: 0.11355666071176529\n",
      "iteration 3516, loss value 338.17999267578125, l2 loss: 0.11355876922607422\n",
      "iteration 3517, loss value 338.1798095703125, l2 loss: 0.11356086283922195\n",
      "iteration 3518, loss value 338.179931640625, l2 loss: 0.11356294900178909\n",
      "iteration 3519, loss value 338.17950439453125, l2 loss: 0.11356508731842041\n",
      "iteration 3520, loss value 338.1796875, l2 loss: 0.11356715857982635\n",
      "iteration 3521, loss value 338.17938232421875, l2 loss: 0.11356928199529648\n",
      "iteration 3522, loss value 338.17950439453125, l2 loss: 0.11357133835554123\n",
      "iteration 3523, loss value 338.1791687011719, l2 loss: 0.11357343196868896\n",
      "iteration 3524, loss value 338.1794128417969, l2 loss: 0.11357549577951431\n",
      "iteration 3525, loss value 338.1796875, l2 loss: 0.11357758939266205\n",
      "iteration 3526, loss value 338.17919921875, l2 loss: 0.1135796457529068\n",
      "iteration 3527, loss value 338.17938232421875, l2 loss: 0.11358170211315155\n",
      "iteration 3528, loss value 338.17926025390625, l2 loss: 0.11358373612165451\n",
      "iteration 3529, loss value 338.1787414550781, l2 loss: 0.11358580738306046\n",
      "iteration 3530, loss value 338.1792297363281, l2 loss: 0.1135878637433052\n",
      "iteration 3531, loss value 338.17901611328125, l2 loss: 0.11358990520238876\n",
      "iteration 3532, loss value 338.1788635253906, l2 loss: 0.11359194666147232\n",
      "iteration 3533, loss value 338.17889404296875, l2 loss: 0.11359396576881409\n",
      "iteration 3534, loss value 338.1789245605469, l2 loss: 0.11359598487615585\n",
      "iteration 3535, loss value 338.17822265625, l2 loss: 0.11359799653291702\n",
      "iteration 3536, loss value 338.1781005859375, l2 loss: 0.11360001564025879\n",
      "iteration 3537, loss value 338.1780700683594, l2 loss: 0.11360199749469757\n",
      "iteration 3538, loss value 338.1779479980469, l2 loss: 0.11360402405261993\n",
      "iteration 3539, loss value 338.17822265625, l2 loss: 0.11360601335763931\n",
      "iteration 3540, loss value 338.17791748046875, l2 loss: 0.11360800266265869\n",
      "iteration 3541, loss value 338.1780700683594, l2 loss: 0.11360999196767807\n",
      "iteration 3542, loss value 338.1780700683594, l2 loss: 0.11361195147037506\n",
      "iteration 3543, loss value 338.1779479980469, l2 loss: 0.11361396312713623\n",
      "iteration 3544, loss value 338.1783142089844, l2 loss: 0.11361589282751083\n",
      "iteration 3545, loss value 338.1777648925781, l2 loss: 0.11361788958311081\n",
      "iteration 3546, loss value 338.17767333984375, l2 loss: 0.1136198565363884\n",
      "iteration 3547, loss value 338.17803955078125, l2 loss: 0.113621786236763\n",
      "iteration 3548, loss value 338.1775817871094, l2 loss: 0.1136237382888794\n",
      "iteration 3549, loss value 338.1776123046875, l2 loss: 0.11362568289041519\n",
      "iteration 3550, loss value 338.1769714355469, l2 loss: 0.11362763494253159\n",
      "iteration 3551, loss value 338.1773681640625, l2 loss: 0.11362956464290619\n",
      "iteration 3552, loss value 338.177001953125, l2 loss: 0.11363149434328079\n",
      "iteration 3553, loss value 338.17718505859375, l2 loss: 0.1136334240436554\n",
      "iteration 3554, loss value 338.17694091796875, l2 loss: 0.11363535374403\n",
      "iteration 3555, loss value 338.17694091796875, l2 loss: 0.1136372834444046\n",
      "iteration 3556, loss value 338.17669677734375, l2 loss: 0.11363919824361801\n",
      "iteration 3557, loss value 338.1773376464844, l2 loss: 0.11364108324050903\n",
      "iteration 3558, loss value 338.1767883300781, l2 loss: 0.11364300549030304\n",
      "iteration 3559, loss value 338.17694091796875, l2 loss: 0.11364489793777466\n",
      "iteration 3560, loss value 338.1764831542969, l2 loss: 0.11364681273698807\n",
      "iteration 3561, loss value 338.17681884765625, l2 loss: 0.11364870518445969\n",
      "iteration 3562, loss value 338.1763610839844, l2 loss: 0.1136505976319313\n",
      "iteration 3563, loss value 338.17657470703125, l2 loss: 0.11365249007940292\n",
      "iteration 3564, loss value 338.1766357421875, l2 loss: 0.11365436017513275\n",
      "iteration 3565, loss value 338.1759948730469, l2 loss: 0.11365626752376556\n",
      "iteration 3566, loss value 338.1763610839844, l2 loss: 0.1136581152677536\n",
      "iteration 3567, loss value 338.17620849609375, l2 loss: 0.11365997046232224\n",
      "iteration 3568, loss value 338.1757507324219, l2 loss: 0.11366184800863266\n",
      "iteration 3569, loss value 338.17608642578125, l2 loss: 0.1136636883020401\n",
      "iteration 3570, loss value 338.1756286621094, l2 loss: 0.11366555094718933\n",
      "iteration 3571, loss value 338.17578125, l2 loss: 0.11366739869117737\n",
      "iteration 3572, loss value 338.17578125, l2 loss: 0.1136692464351654\n",
      "iteration 3573, loss value 338.1757507324219, l2 loss: 0.11367105692625046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3574, loss value 338.17498779296875, l2 loss: 0.11367291212081909\n",
      "iteration 3575, loss value 338.17547607421875, l2 loss: 0.11367473751306534\n",
      "iteration 3576, loss value 338.175537109375, l2 loss: 0.11367657035589218\n",
      "iteration 3577, loss value 338.175537109375, l2 loss: 0.11367837339639664\n",
      "iteration 3578, loss value 338.17529296875, l2 loss: 0.1136801689863205\n",
      "iteration 3579, loss value 338.1752624511719, l2 loss: 0.11368198692798615\n",
      "iteration 3580, loss value 338.17523193359375, l2 loss: 0.11368378251791\n",
      "iteration 3581, loss value 338.17523193359375, l2 loss: 0.11368555575609207\n",
      "iteration 3582, loss value 338.17437744140625, l2 loss: 0.11368738114833832\n",
      "iteration 3583, loss value 338.17498779296875, l2 loss: 0.11368916928768158\n",
      "iteration 3584, loss value 338.17462158203125, l2 loss: 0.11369096487760544\n",
      "iteration 3585, loss value 338.1748962402344, l2 loss: 0.1136927455663681\n",
      "iteration 3586, loss value 338.1746826171875, l2 loss: 0.11369448155164719\n",
      "iteration 3587, loss value 338.1742248535156, l2 loss: 0.11369625478982925\n",
      "iteration 3588, loss value 338.1740417480469, l2 loss: 0.11369800567626953\n",
      "iteration 3589, loss value 338.1739501953125, l2 loss: 0.11369973421096802\n",
      "iteration 3590, loss value 338.1737060546875, l2 loss: 0.11370152235031128\n",
      "iteration 3591, loss value 338.17413330078125, l2 loss: 0.11370325088500977\n",
      "iteration 3592, loss value 338.1737060546875, l2 loss: 0.11370500922203064\n",
      "iteration 3593, loss value 338.174072265625, l2 loss: 0.11370672285556793\n",
      "iteration 3594, loss value 338.1736145019531, l2 loss: 0.11370846629142761\n",
      "iteration 3595, loss value 338.1741638183594, l2 loss: 0.11371017247438431\n",
      "iteration 3596, loss value 338.1734619140625, l2 loss: 0.1137118712067604\n",
      "iteration 3597, loss value 338.1735534667969, l2 loss: 0.11371362209320068\n",
      "iteration 3598, loss value 338.1741943359375, l2 loss: 0.1137152910232544\n",
      "iteration 3599, loss value 338.1736755371094, l2 loss: 0.11371701210737228\n",
      "iteration 3600, loss value 338.1736755371094, l2 loss: 0.11371871083974838\n",
      "iteration 3601, loss value 338.1737060546875, l2 loss: 0.11372040957212448\n",
      "iteration 3602, loss value 338.1733703613281, l2 loss: 0.11372209340333939\n",
      "iteration 3603, loss value 338.1734619140625, l2 loss: 0.1137237697839737\n",
      "iteration 3604, loss value 338.17327880859375, l2 loss: 0.113725446164608\n",
      "iteration 3605, loss value 338.17327880859375, l2 loss: 0.11372711509466171\n",
      "iteration 3606, loss value 338.1729431152344, l2 loss: 0.11372880637645721\n",
      "iteration 3607, loss value 338.1730651855469, l2 loss: 0.11373045295476913\n",
      "iteration 3608, loss value 338.1729431152344, l2 loss: 0.11373209953308105\n",
      "iteration 3609, loss value 338.1727600097656, l2 loss: 0.11373376101255417\n",
      "iteration 3610, loss value 338.1723937988281, l2 loss: 0.11373542994260788\n",
      "iteration 3611, loss value 338.1726379394531, l2 loss: 0.1137370690703392\n",
      "iteration 3612, loss value 338.1725769042969, l2 loss: 0.11373872309923172\n",
      "iteration 3613, loss value 338.1725769042969, l2 loss: 0.11374033242464066\n",
      "iteration 3614, loss value 338.17291259765625, l2 loss: 0.11374197900295258\n",
      "iteration 3615, loss value 338.1725769042969, l2 loss: 0.1137436181306839\n",
      "iteration 3616, loss value 338.1727294921875, l2 loss: 0.11374522745609283\n",
      "iteration 3617, loss value 338.1719970703125, l2 loss: 0.11374683678150177\n",
      "iteration 3618, loss value 338.17193603515625, l2 loss: 0.1137484759092331\n",
      "iteration 3619, loss value 338.17242431640625, l2 loss: 0.11375007033348083\n",
      "iteration 3620, loss value 338.1722412109375, l2 loss: 0.11375166475772858\n",
      "iteration 3621, loss value 338.17205810546875, l2 loss: 0.11375325173139572\n",
      "iteration 3622, loss value 338.1719665527344, l2 loss: 0.11375486105680466\n",
      "iteration 3623, loss value 338.1719970703125, l2 loss: 0.1137564405798912\n",
      "iteration 3624, loss value 338.171875, l2 loss: 0.11375799775123596\n",
      "iteration 3625, loss value 338.171630859375, l2 loss: 0.11375956237316132\n",
      "iteration 3626, loss value 338.17144775390625, l2 loss: 0.11376114189624786\n",
      "iteration 3627, loss value 338.1713562011719, l2 loss: 0.11376272141933441\n",
      "iteration 3628, loss value 338.17156982421875, l2 loss: 0.11376430094242096\n",
      "iteration 3629, loss value 338.17181396484375, l2 loss: 0.11376582831144333\n",
      "iteration 3630, loss value 338.17144775390625, l2 loss: 0.11376738548278809\n",
      "iteration 3631, loss value 338.17144775390625, l2 loss: 0.11376893520355225\n",
      "iteration 3632, loss value 338.17144775390625, l2 loss: 0.11377047002315521\n",
      "iteration 3633, loss value 338.1711120605469, l2 loss: 0.11377202719449997\n",
      "iteration 3634, loss value 338.17132568359375, l2 loss: 0.11377358436584473\n",
      "iteration 3635, loss value 338.1712646484375, l2 loss: 0.1137750968337059\n",
      "iteration 3636, loss value 338.1707458496094, l2 loss: 0.11377663910388947\n",
      "iteration 3637, loss value 338.17108154296875, l2 loss: 0.11377813667058945\n",
      "iteration 3638, loss value 338.1700744628906, l2 loss: 0.11377967149019241\n",
      "iteration 3639, loss value 338.17071533203125, l2 loss: 0.1137811467051506\n",
      "iteration 3640, loss value 338.1705017089844, l2 loss: 0.11378266662359238\n",
      "iteration 3641, loss value 338.17095947265625, l2 loss: 0.11378415673971176\n",
      "iteration 3642, loss value 338.1703796386719, l2 loss: 0.11378566175699234\n",
      "iteration 3643, loss value 338.1705322265625, l2 loss: 0.11378712207078934\n",
      "iteration 3644, loss value 338.1702575683594, l2 loss: 0.1137886643409729\n",
      "iteration 3645, loss value 338.1710510253906, l2 loss: 0.1137901097536087\n",
      "iteration 3646, loss value 338.1700439453125, l2 loss: 0.11379159986972809\n",
      "iteration 3647, loss value 338.1703186035156, l2 loss: 0.11379309743642807\n",
      "iteration 3648, loss value 338.17022705078125, l2 loss: 0.11379455029964447\n",
      "iteration 3649, loss value 338.1697692871094, l2 loss: 0.11379601061344147\n",
      "iteration 3650, loss value 338.16961669921875, l2 loss: 0.11379749327898026\n",
      "iteration 3651, loss value 338.1696472167969, l2 loss: 0.11379892379045486\n",
      "iteration 3652, loss value 338.16949462890625, l2 loss: 0.11380040645599365\n",
      "iteration 3653, loss value 338.16998291015625, l2 loss: 0.11380184441804886\n",
      "iteration 3654, loss value 338.1699523925781, l2 loss: 0.11380329728126526\n",
      "iteration 3655, loss value 338.1696472167969, l2 loss: 0.11380472034215927\n",
      "iteration 3656, loss value 338.1695861816406, l2 loss: 0.11380612850189209\n",
      "iteration 3657, loss value 338.1693420410156, l2 loss: 0.11380758881568909\n",
      "iteration 3658, loss value 338.16949462890625, l2 loss: 0.1138090193271637\n",
      "iteration 3659, loss value 338.16937255859375, l2 loss: 0.11381043493747711\n",
      "iteration 3660, loss value 338.16943359375, l2 loss: 0.11381186544895172\n",
      "iteration 3661, loss value 338.16937255859375, l2 loss: 0.11381327360868454\n",
      "iteration 3662, loss value 338.1695251464844, l2 loss: 0.11381466686725616\n",
      "iteration 3663, loss value 338.1689147949219, l2 loss: 0.11381609737873077\n",
      "iteration 3664, loss value 338.1690368652344, l2 loss: 0.11381751298904419\n",
      "iteration 3665, loss value 338.16888427734375, l2 loss: 0.11381889879703522\n",
      "iteration 3666, loss value 338.1685791015625, l2 loss: 0.11382030695676804\n",
      "iteration 3667, loss value 338.16888427734375, l2 loss: 0.11382168531417847\n",
      "iteration 3668, loss value 338.16888427734375, l2 loss: 0.1138230636715889\n",
      "iteration 3669, loss value 338.1688232421875, l2 loss: 0.11382441967725754\n",
      "iteration 3670, loss value 338.16845703125, l2 loss: 0.11382579803466797\n",
      "iteration 3671, loss value 338.1687927246094, l2 loss: 0.1138271614909172\n",
      "iteration 3672, loss value 338.1681213378906, l2 loss: 0.11382848024368286\n",
      "iteration 3673, loss value 338.1680908203125, l2 loss: 0.11382986605167389\n",
      "iteration 3674, loss value 338.1680603027344, l2 loss: 0.11383120715618134\n",
      "iteration 3675, loss value 338.16827392578125, l2 loss: 0.11383254826068878\n",
      "iteration 3676, loss value 338.16815185546875, l2 loss: 0.11383385211229324\n",
      "iteration 3677, loss value 338.1678161621094, l2 loss: 0.11383519321680069\n",
      "iteration 3678, loss value 338.1676330566406, l2 loss: 0.11383653432130814\n",
      "iteration 3679, loss value 338.1678466796875, l2 loss: 0.1138378232717514\n",
      "iteration 3680, loss value 338.1679992675781, l2 loss: 0.11383914202451706\n",
      "iteration 3681, loss value 338.16802978515625, l2 loss: 0.11384043842554092\n",
      "iteration 3682, loss value 338.1676940917969, l2 loss: 0.11384174227714539\n",
      "iteration 3683, loss value 338.1674499511719, l2 loss: 0.11384303867816925\n",
      "iteration 3684, loss value 338.1673278808594, l2 loss: 0.1138443574309349\n",
      "iteration 3685, loss value 338.1677551269531, l2 loss: 0.11384565383195877\n",
      "iteration 3686, loss value 338.1674499511719, l2 loss: 0.11384691298007965\n",
      "iteration 3687, loss value 338.1672668457031, l2 loss: 0.11384822428226471\n",
      "iteration 3688, loss value 338.1676025390625, l2 loss: 0.11384949833154678\n",
      "iteration 3689, loss value 338.1673278808594, l2 loss: 0.11385073512792587\n",
      "iteration 3690, loss value 338.16717529296875, l2 loss: 0.11385202407836914\n",
      "iteration 3691, loss value 338.1673278808594, l2 loss: 0.11385327577590942\n",
      "iteration 3692, loss value 338.1672668457031, l2 loss: 0.11385452747344971\n",
      "iteration 3693, loss value 338.166748046875, l2 loss: 0.11385580152273178\n",
      "iteration 3694, loss value 338.16693115234375, l2 loss: 0.11385706067085266\n",
      "iteration 3695, loss value 338.166748046875, l2 loss: 0.11385827511548996\n",
      "iteration 3696, loss value 338.1665344238281, l2 loss: 0.11385955661535263\n",
      "iteration 3697, loss value 338.16693115234375, l2 loss: 0.11386077851057053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3698, loss value 338.1662902832031, l2 loss: 0.11386203020811081\n",
      "iteration 3699, loss value 338.1664123535156, l2 loss: 0.1138632595539093\n",
      "iteration 3700, loss value 338.1665344238281, l2 loss: 0.1138644888997078\n",
      "iteration 3701, loss value 338.1663513183594, l2 loss: 0.1138656958937645\n",
      "iteration 3702, loss value 338.1663513183594, l2 loss: 0.11386692523956299\n",
      "iteration 3703, loss value 338.1664123535156, l2 loss: 0.11386813968420029\n",
      "iteration 3704, loss value 338.16619873046875, l2 loss: 0.11386936902999878\n",
      "iteration 3705, loss value 338.1663513183594, l2 loss: 0.11387056857347488\n",
      "iteration 3706, loss value 338.166259765625, l2 loss: 0.1138717383146286\n",
      "iteration 3707, loss value 338.1661071777344, l2 loss: 0.11387293040752411\n",
      "iteration 3708, loss value 338.1659851074219, l2 loss: 0.11387411504983902\n",
      "iteration 3709, loss value 338.1658630371094, l2 loss: 0.11387532949447632\n",
      "iteration 3710, loss value 338.1663513183594, l2 loss: 0.11387652903795242\n",
      "iteration 3711, loss value 338.1663513183594, l2 loss: 0.11387767642736435\n",
      "iteration 3712, loss value 338.165771484375, l2 loss: 0.11387886106967926\n",
      "iteration 3713, loss value 338.16583251953125, l2 loss: 0.11388003081083298\n",
      "iteration 3714, loss value 338.1656799316406, l2 loss: 0.1138811931014061\n",
      "iteration 3715, loss value 338.1656799316406, l2 loss: 0.11388234794139862\n",
      "iteration 3716, loss value 338.16522216796875, l2 loss: 0.11388351023197174\n",
      "iteration 3717, loss value 338.1658935546875, l2 loss: 0.11388462036848068\n",
      "iteration 3718, loss value 338.1651611328125, l2 loss: 0.1138857752084732\n",
      "iteration 3719, loss value 338.16534423828125, l2 loss: 0.11388689279556274\n",
      "iteration 3720, loss value 338.1652526855469, l2 loss: 0.11388801783323288\n",
      "iteration 3721, loss value 338.1651306152344, l2 loss: 0.11388911306858063\n",
      "iteration 3722, loss value 338.1650695800781, l2 loss: 0.11389027535915375\n",
      "iteration 3723, loss value 338.1652526855469, l2 loss: 0.1138913482427597\n",
      "iteration 3724, loss value 338.16510009765625, l2 loss: 0.11389248818159103\n",
      "iteration 3725, loss value 338.1650390625, l2 loss: 0.11389357596635818\n",
      "iteration 3726, loss value 338.16485595703125, l2 loss: 0.11389465630054474\n",
      "iteration 3727, loss value 338.1646423339844, l2 loss: 0.1138957291841507\n",
      "iteration 3728, loss value 338.1646423339844, l2 loss: 0.11389680951833725\n",
      "iteration 3729, loss value 338.16448974609375, l2 loss: 0.1138979122042656\n",
      "iteration 3730, loss value 338.1650085449219, l2 loss: 0.11389897018671036\n",
      "iteration 3731, loss value 338.1643981933594, l2 loss: 0.11390004307031631\n",
      "iteration 3732, loss value 338.16473388671875, l2 loss: 0.11390110850334167\n",
      "iteration 3733, loss value 338.1640625, l2 loss: 0.11390216648578644\n",
      "iteration 3734, loss value 338.16448974609375, l2 loss: 0.11390320211648941\n",
      "iteration 3735, loss value 338.1643371582031, l2 loss: 0.11390426009893417\n",
      "iteration 3736, loss value 338.16424560546875, l2 loss: 0.11390531808137894\n",
      "iteration 3737, loss value 338.164306640625, l2 loss: 0.1139063686132431\n",
      "iteration 3738, loss value 338.1640319824219, l2 loss: 0.11390742659568787\n",
      "iteration 3739, loss value 338.16400146484375, l2 loss: 0.11390845477581024\n",
      "iteration 3740, loss value 338.1637878417969, l2 loss: 0.11390948295593262\n",
      "iteration 3741, loss value 338.1637878417969, l2 loss: 0.11391054838895798\n",
      "iteration 3742, loss value 338.16412353515625, l2 loss: 0.11391155421733856\n",
      "iteration 3743, loss value 338.1637268066406, l2 loss: 0.11391258239746094\n",
      "iteration 3744, loss value 338.16363525390625, l2 loss: 0.11391360312700272\n",
      "iteration 3745, loss value 338.1634216308594, l2 loss: 0.1139146089553833\n",
      "iteration 3746, loss value 338.1633605957031, l2 loss: 0.11391562968492508\n",
      "iteration 3747, loss value 338.1634826660156, l2 loss: 0.11391666531562805\n",
      "iteration 3748, loss value 338.16351318359375, l2 loss: 0.11391765624284744\n",
      "iteration 3749, loss value 338.16357421875, l2 loss: 0.11391865462064743\n",
      "iteration 3750, loss value 338.16339111328125, l2 loss: 0.11391963064670563\n",
      "iteration 3751, loss value 338.1632995605469, l2 loss: 0.11392062157392502\n",
      "iteration 3752, loss value 338.16351318359375, l2 loss: 0.11392160505056381\n",
      "iteration 3753, loss value 338.16302490234375, l2 loss: 0.11392256617546082\n",
      "iteration 3754, loss value 338.1627197265625, l2 loss: 0.1139235645532608\n",
      "iteration 3755, loss value 338.1629638671875, l2 loss: 0.11392451822757721\n",
      "iteration 3756, loss value 338.1624450683594, l2 loss: 0.113925501704216\n",
      "iteration 3757, loss value 338.16290283203125, l2 loss: 0.11392643302679062\n",
      "iteration 3758, loss value 338.16290283203125, l2 loss: 0.11392739415168762\n",
      "iteration 3759, loss value 338.1628723144531, l2 loss: 0.11392834782600403\n",
      "iteration 3760, loss value 338.1626892089844, l2 loss: 0.11392929404973984\n",
      "iteration 3761, loss value 338.16241455078125, l2 loss: 0.11393024027347565\n",
      "iteration 3762, loss value 338.16259765625, l2 loss: 0.11393115669488907\n",
      "iteration 3763, loss value 338.1621398925781, l2 loss: 0.11393210291862488\n",
      "iteration 3764, loss value 338.1625671386719, l2 loss: 0.1139330267906189\n",
      "iteration 3765, loss value 338.16241455078125, l2 loss: 0.11393395066261292\n",
      "iteration 3766, loss value 338.16217041015625, l2 loss: 0.11393487453460693\n",
      "iteration 3767, loss value 338.1626281738281, l2 loss: 0.11393578350543976\n",
      "iteration 3768, loss value 338.1624450683594, l2 loss: 0.11393669992685318\n",
      "iteration 3769, loss value 338.1620788574219, l2 loss: 0.11393760144710541\n",
      "iteration 3770, loss value 338.1622009277344, l2 loss: 0.11393848806619644\n",
      "iteration 3771, loss value 338.16168212890625, l2 loss: 0.11393939703702927\n",
      "iteration 3772, loss value 338.16180419921875, l2 loss: 0.1139402985572815\n",
      "iteration 3773, loss value 338.1618957519531, l2 loss: 0.11394117772579193\n",
      "iteration 3774, loss value 338.16156005859375, l2 loss: 0.11394204944372177\n",
      "iteration 3775, loss value 338.1615905761719, l2 loss: 0.11394292861223221\n",
      "iteration 3776, loss value 338.1614990234375, l2 loss: 0.11394380778074265\n",
      "iteration 3777, loss value 338.1614685058594, l2 loss: 0.11394466459751129\n",
      "iteration 3778, loss value 338.16192626953125, l2 loss: 0.11394549906253815\n",
      "iteration 3779, loss value 338.16162109375, l2 loss: 0.11394638568162918\n",
      "iteration 3780, loss value 338.1615905761719, l2 loss: 0.11394724994897842\n",
      "iteration 3781, loss value 338.1614990234375, l2 loss: 0.11394806206226349\n",
      "iteration 3782, loss value 338.1610107421875, l2 loss: 0.11394894123077393\n",
      "iteration 3783, loss value 338.1617126464844, l2 loss: 0.11394978314638138\n",
      "iteration 3784, loss value 338.1613464355469, l2 loss: 0.11395058780908585\n",
      "iteration 3785, loss value 338.16064453125, l2 loss: 0.11395145952701569\n",
      "iteration 3786, loss value 338.16107177734375, l2 loss: 0.11395227909088135\n",
      "iteration 3787, loss value 338.1607666015625, l2 loss: 0.11395309865474701\n",
      "iteration 3788, loss value 338.1613464355469, l2 loss: 0.11395388841629028\n",
      "iteration 3789, loss value 338.1606140136719, l2 loss: 0.11395471543073654\n",
      "iteration 3790, loss value 338.160888671875, l2 loss: 0.11395551264286041\n",
      "iteration 3791, loss value 338.1601867675781, l2 loss: 0.11395632475614548\n",
      "iteration 3792, loss value 338.16094970703125, l2 loss: 0.11395712196826935\n",
      "iteration 3793, loss value 338.16046142578125, l2 loss: 0.11395792663097382\n",
      "iteration 3794, loss value 338.1605224609375, l2 loss: 0.1139586940407753\n",
      "iteration 3795, loss value 338.1604309082031, l2 loss: 0.11395949125289917\n",
      "iteration 3796, loss value 338.1605224609375, l2 loss: 0.11396027356386185\n",
      "iteration 3797, loss value 338.1601867675781, l2 loss: 0.11396104097366333\n",
      "iteration 3798, loss value 338.1601257324219, l2 loss: 0.11396182328462601\n",
      "iteration 3799, loss value 338.1603698730469, l2 loss: 0.1139625832438469\n",
      "iteration 3800, loss value 338.16015625, l2 loss: 0.11396332830190659\n",
      "iteration 3801, loss value 338.1604309082031, l2 loss: 0.11396409571170807\n",
      "iteration 3802, loss value 338.1602478027344, l2 loss: 0.11396484076976776\n",
      "iteration 3803, loss value 338.1595458984375, l2 loss: 0.11396556347608566\n",
      "iteration 3804, loss value 338.1596374511719, l2 loss: 0.11396635323762894\n",
      "iteration 3805, loss value 338.16033935546875, l2 loss: 0.11396708339452744\n",
      "iteration 3806, loss value 338.1597900390625, l2 loss: 0.11396781355142593\n",
      "iteration 3807, loss value 338.15972900390625, l2 loss: 0.11396850645542145\n",
      "iteration 3808, loss value 338.1597900390625, l2 loss: 0.11396924406290054\n",
      "iteration 3809, loss value 338.1597595214844, l2 loss: 0.11396996676921844\n",
      "iteration 3810, loss value 338.15966796875, l2 loss: 0.11397068947553635\n",
      "iteration 3811, loss value 338.1593933105469, l2 loss: 0.11397140473127365\n",
      "iteration 3812, loss value 338.1596984863281, l2 loss: 0.11397210508584976\n",
      "iteration 3813, loss value 338.1591491699219, l2 loss: 0.11397282034158707\n",
      "iteration 3814, loss value 338.15948486328125, l2 loss: 0.11397353559732437\n",
      "iteration 3815, loss value 338.1593017578125, l2 loss: 0.11397422850131989\n",
      "iteration 3816, loss value 338.15966796875, l2 loss: 0.1139749214053154\n",
      "iteration 3817, loss value 338.1590576171875, l2 loss: 0.11397561430931091\n",
      "iteration 3818, loss value 338.15936279296875, l2 loss: 0.11397629231214523\n",
      "iteration 3819, loss value 338.15899658203125, l2 loss: 0.11397695541381836\n",
      "iteration 3820, loss value 338.1587829589844, l2 loss: 0.11397762596607208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3821, loss value 338.15887451171875, l2 loss: 0.113978311419487\n",
      "iteration 3822, loss value 338.1590881347656, l2 loss: 0.11397895216941833\n",
      "iteration 3823, loss value 338.15863037109375, l2 loss: 0.11397962272167206\n",
      "iteration 3824, loss value 338.1588134765625, l2 loss: 0.1139802485704422\n",
      "iteration 3825, loss value 338.1584167480469, l2 loss: 0.11398089677095413\n",
      "iteration 3826, loss value 338.1584777832031, l2 loss: 0.11398155987262726\n",
      "iteration 3827, loss value 338.15863037109375, l2 loss: 0.1139821708202362\n",
      "iteration 3828, loss value 338.1585388183594, l2 loss: 0.11398279666900635\n",
      "iteration 3829, loss value 338.1585388183594, l2 loss: 0.11398342996835709\n",
      "iteration 3830, loss value 338.15814208984375, l2 loss: 0.11398404836654663\n",
      "iteration 3831, loss value 338.158447265625, l2 loss: 0.11398464441299438\n",
      "iteration 3832, loss value 338.1581726074219, l2 loss: 0.11398527771234512\n",
      "iteration 3833, loss value 338.1581726074219, l2 loss: 0.11398588120937347\n",
      "iteration 3834, loss value 338.157958984375, l2 loss: 0.11398649960756302\n",
      "iteration 3835, loss value 338.15826416015625, l2 loss: 0.11398710310459137\n",
      "iteration 3836, loss value 338.1586608886719, l2 loss: 0.11398769170045853\n",
      "iteration 3837, loss value 338.15814208984375, l2 loss: 0.11398832499980927\n",
      "iteration 3838, loss value 338.1582946777344, l2 loss: 0.11398891359567642\n",
      "iteration 3839, loss value 338.1578063964844, l2 loss: 0.11398950964212418\n",
      "iteration 3840, loss value 338.1576843261719, l2 loss: 0.11399010568857193\n",
      "iteration 3841, loss value 338.15802001953125, l2 loss: 0.1139906793832779\n",
      "iteration 3842, loss value 338.1576843261719, l2 loss: 0.11399128288030624\n",
      "iteration 3843, loss value 338.1578369140625, l2 loss: 0.1139918714761734\n",
      "iteration 3844, loss value 338.15771484375, l2 loss: 0.11399242281913757\n",
      "iteration 3845, loss value 338.1573486328125, l2 loss: 0.11399298906326294\n",
      "iteration 3846, loss value 338.15716552734375, l2 loss: 0.1139935702085495\n",
      "iteration 3847, loss value 338.1575622558594, l2 loss: 0.11399410665035248\n",
      "iteration 3848, loss value 338.1574401855469, l2 loss: 0.11399466544389725\n",
      "iteration 3849, loss value 338.1573181152344, l2 loss: 0.11399518698453903\n",
      "iteration 3850, loss value 338.1571044921875, l2 loss: 0.11399573087692261\n",
      "iteration 3851, loss value 338.1575622558594, l2 loss: 0.11399625241756439\n",
      "iteration 3852, loss value 338.1571350097656, l2 loss: 0.11399678885936737\n",
      "iteration 3853, loss value 338.156982421875, l2 loss: 0.11399729549884796\n",
      "iteration 3854, loss value 338.1570129394531, l2 loss: 0.11399777978658676\n",
      "iteration 3855, loss value 338.1568298339844, l2 loss: 0.11399833112955093\n",
      "iteration 3856, loss value 338.15692138671875, l2 loss: 0.11399883031845093\n",
      "iteration 3857, loss value 338.1569519042969, l2 loss: 0.11399933695793152\n",
      "iteration 3858, loss value 338.1568298339844, l2 loss: 0.11399982124567032\n",
      "iteration 3859, loss value 338.1567077636719, l2 loss: 0.11400032043457031\n",
      "iteration 3860, loss value 338.15679931640625, l2 loss: 0.11400080472230911\n",
      "iteration 3861, loss value 338.156494140625, l2 loss: 0.11400127410888672\n",
      "iteration 3862, loss value 338.1557922363281, l2 loss: 0.11400177329778671\n",
      "iteration 3863, loss value 338.15606689453125, l2 loss: 0.11400222778320312\n",
      "iteration 3864, loss value 338.156005859375, l2 loss: 0.11400270462036133\n",
      "iteration 3865, loss value 338.1562805175781, l2 loss: 0.11400316655635834\n",
      "iteration 3866, loss value 338.1562805175781, l2 loss: 0.11400359869003296\n",
      "iteration 3867, loss value 338.1559753417969, l2 loss: 0.11400406062602997\n",
      "iteration 3868, loss value 338.15618896484375, l2 loss: 0.11400452256202698\n",
      "iteration 3869, loss value 338.1558837890625, l2 loss: 0.1140049397945404\n",
      "iteration 3870, loss value 338.1560974121094, l2 loss: 0.11400536447763443\n",
      "iteration 3871, loss value 338.15576171875, l2 loss: 0.11400582641363144\n",
      "iteration 3872, loss value 338.156005859375, l2 loss: 0.11400624364614487\n",
      "iteration 3873, loss value 338.1557922363281, l2 loss: 0.11400667577981949\n",
      "iteration 3874, loss value 338.15570068359375, l2 loss: 0.11400707811117172\n",
      "iteration 3875, loss value 338.1556091308594, l2 loss: 0.11400748044252396\n",
      "iteration 3876, loss value 338.15557861328125, l2 loss: 0.11400792002677917\n",
      "iteration 3877, loss value 338.1560363769531, l2 loss: 0.11400831490755081\n",
      "iteration 3878, loss value 338.15557861328125, l2 loss: 0.11400873959064484\n",
      "iteration 3879, loss value 338.1558532714844, l2 loss: 0.11400911211967468\n",
      "iteration 3880, loss value 338.155517578125, l2 loss: 0.11400952935218811\n",
      "iteration 3881, loss value 338.15582275390625, l2 loss: 0.11400991678237915\n",
      "iteration 3882, loss value 338.1554870605469, l2 loss: 0.114010289311409\n",
      "iteration 3883, loss value 338.155029296875, l2 loss: 0.11401069909334183\n",
      "iteration 3884, loss value 338.15545654296875, l2 loss: 0.11401109397411346\n",
      "iteration 3885, loss value 338.155517578125, l2 loss: 0.11401144415140152\n",
      "iteration 3886, loss value 338.1551208496094, l2 loss: 0.11401183903217316\n",
      "iteration 3887, loss value 338.1556091308594, l2 loss: 0.11401218920946121\n",
      "iteration 3888, loss value 338.15460205078125, l2 loss: 0.11401257663965225\n",
      "iteration 3889, loss value 338.1549072265625, l2 loss: 0.1140129566192627\n",
      "iteration 3890, loss value 338.15478515625, l2 loss: 0.11401333659887314\n",
      "iteration 3891, loss value 338.1551208496094, l2 loss: 0.1140136867761612\n",
      "iteration 3892, loss value 338.1549987792969, l2 loss: 0.11401402950286865\n",
      "iteration 3893, loss value 338.1549072265625, l2 loss: 0.1140143871307373\n",
      "iteration 3894, loss value 338.155029296875, l2 loss: 0.11401469260454178\n",
      "iteration 3895, loss value 338.1544189453125, l2 loss: 0.11401508003473282\n",
      "iteration 3896, loss value 338.15478515625, l2 loss: 0.11401541531085968\n",
      "iteration 3897, loss value 338.15533447265625, l2 loss: 0.11401571333408356\n",
      "iteration 3898, loss value 338.1541748046875, l2 loss: 0.11401607096195221\n",
      "iteration 3899, loss value 338.15484619140625, l2 loss: 0.11401639133691788\n",
      "iteration 3900, loss value 338.1543884277344, l2 loss: 0.11401670426130295\n",
      "iteration 3901, loss value 338.1544494628906, l2 loss: 0.11401702463626862\n",
      "iteration 3902, loss value 338.1542053222656, l2 loss: 0.11401736736297607\n",
      "iteration 3903, loss value 338.1545104980469, l2 loss: 0.11401765793561935\n",
      "iteration 3904, loss value 338.1543273925781, l2 loss: 0.11401797831058502\n",
      "iteration 3905, loss value 338.15423583984375, l2 loss: 0.11401830613613129\n",
      "iteration 3906, loss value 338.1541748046875, l2 loss: 0.11401858180761337\n",
      "iteration 3907, loss value 338.1541748046875, l2 loss: 0.11401887238025665\n",
      "iteration 3908, loss value 338.15423583984375, l2 loss: 0.11401914805173874\n",
      "iteration 3909, loss value 338.1540222167969, l2 loss: 0.11401945352554321\n",
      "iteration 3910, loss value 338.154052734375, l2 loss: 0.11401968449354172\n",
      "iteration 3911, loss value 338.1535949707031, l2 loss: 0.11401993781328201\n",
      "iteration 3912, loss value 338.1535339355469, l2 loss: 0.11402022838592529\n",
      "iteration 3913, loss value 338.1537780761719, l2 loss: 0.11402047425508499\n",
      "iteration 3914, loss value 338.1536560058594, l2 loss: 0.1140207052230835\n",
      "iteration 3915, loss value 338.1540222167969, l2 loss: 0.11402091383934021\n",
      "iteration 3916, loss value 338.153076171875, l2 loss: 0.1140211820602417\n",
      "iteration 3917, loss value 338.1534729003906, l2 loss: 0.1140214130282402\n",
      "iteration 3918, loss value 338.1534118652344, l2 loss: 0.1140216588973999\n",
      "iteration 3919, loss value 338.1535339355469, l2 loss: 0.11402186006307602\n",
      "iteration 3920, loss value 338.15338134765625, l2 loss: 0.11402209103107452\n",
      "iteration 3921, loss value 338.15313720703125, l2 loss: 0.11402229964733124\n",
      "iteration 3922, loss value 338.15277099609375, l2 loss: 0.11402253806591034\n",
      "iteration 3923, loss value 338.153076171875, l2 loss: 0.11402271687984467\n",
      "iteration 3924, loss value 338.1528625488281, l2 loss: 0.11402296274900436\n",
      "iteration 3925, loss value 338.1531982421875, l2 loss: 0.11402317136526108\n",
      "iteration 3926, loss value 338.1535339355469, l2 loss: 0.1140233650803566\n",
      "iteration 3927, loss value 338.1532897949219, l2 loss: 0.11402356624603271\n",
      "iteration 3928, loss value 338.1526794433594, l2 loss: 0.11402375251054764\n",
      "iteration 3929, loss value 338.1522521972656, l2 loss: 0.11402395367622375\n",
      "iteration 3930, loss value 338.1526794433594, l2 loss: 0.11402414739131927\n",
      "iteration 3931, loss value 338.1524658203125, l2 loss: 0.1140243336558342\n",
      "iteration 3932, loss value 338.1524353027344, l2 loss: 0.11402451246976852\n",
      "iteration 3933, loss value 338.1527404785156, l2 loss: 0.11402470618486404\n",
      "iteration 3934, loss value 338.1529235839844, l2 loss: 0.11402485519647598\n",
      "iteration 3935, loss value 338.15240478515625, l2 loss: 0.1140250414609909\n",
      "iteration 3936, loss value 338.152587890625, l2 loss: 0.11402519792318344\n",
      "iteration 3937, loss value 338.1524963378906, l2 loss: 0.11402538418769836\n",
      "iteration 3938, loss value 338.1523132324219, l2 loss: 0.1140255406498909\n",
      "iteration 3939, loss value 338.1521911621094, l2 loss: 0.11402570456266403\n",
      "iteration 3940, loss value 338.1519470214844, l2 loss: 0.11402586847543716\n",
      "iteration 3941, loss value 338.1524353027344, l2 loss: 0.11402599513530731\n",
      "iteration 3942, loss value 338.15228271484375, l2 loss: 0.11402613669633865\n",
      "iteration 3943, loss value 338.1519470214844, l2 loss: 0.1140262633562088\n",
      "iteration 3944, loss value 338.15167236328125, l2 loss: 0.11402640491724014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3945, loss value 338.1523132324219, l2 loss: 0.11402653902769089\n",
      "iteration 3946, loss value 338.1518249511719, l2 loss: 0.11402665078639984\n",
      "iteration 3947, loss value 338.1519470214844, l2 loss: 0.11402676999568939\n",
      "iteration 3948, loss value 338.15191650390625, l2 loss: 0.11402688175439835\n",
      "iteration 3949, loss value 338.152099609375, l2 loss: 0.11402697116136551\n",
      "iteration 3950, loss value 338.1515808105469, l2 loss: 0.11402709037065506\n",
      "iteration 3951, loss value 338.1518249511719, l2 loss: 0.11402717977762222\n",
      "iteration 3952, loss value 338.15130615234375, l2 loss: 0.11402726918458939\n",
      "iteration 3953, loss value 338.151611328125, l2 loss: 0.11402736604213715\n",
      "iteration 3954, loss value 338.1516418457031, l2 loss: 0.11402743309736252\n",
      "iteration 3955, loss value 338.1513366699219, l2 loss: 0.11402754485607147\n",
      "iteration 3956, loss value 338.1512145996094, l2 loss: 0.11402760446071625\n",
      "iteration 3957, loss value 338.15118408203125, l2 loss: 0.11402767151594162\n",
      "iteration 3958, loss value 338.1514587402344, l2 loss: 0.1140277311205864\n",
      "iteration 3959, loss value 338.15142822265625, l2 loss: 0.11402779817581177\n",
      "iteration 3960, loss value 338.15118408203125, l2 loss: 0.11402785032987595\n",
      "iteration 3961, loss value 338.15093994140625, l2 loss: 0.11402789503335953\n",
      "iteration 3962, loss value 338.15093994140625, l2 loss: 0.11402790993452072\n",
      "iteration 3963, loss value 338.1507873535156, l2 loss: 0.1140279546380043\n",
      "iteration 3964, loss value 338.15093994140625, l2 loss: 0.11402798444032669\n",
      "iteration 3965, loss value 338.1509704589844, l2 loss: 0.11402799934148788\n",
      "iteration 3966, loss value 338.1505126953125, l2 loss: 0.11402801424264908\n",
      "iteration 3967, loss value 338.15087890625, l2 loss: 0.11402801424264908\n",
      "iteration 3968, loss value 338.1507568359375, l2 loss: 0.11402804404497147\n",
      "iteration 3969, loss value 338.15093994140625, l2 loss: 0.11402806639671326\n",
      "iteration 3970, loss value 338.15106201171875, l2 loss: 0.11402804404497147\n",
      "iteration 3971, loss value 338.1507568359375, l2 loss: 0.11402802169322968\n",
      "iteration 3972, loss value 338.15032958984375, l2 loss: 0.11402805149555206\n",
      "iteration 3973, loss value 338.1506042480469, l2 loss: 0.11402803659439087\n",
      "iteration 3974, loss value 338.15045166015625, l2 loss: 0.11402802169322968\n",
      "iteration 3975, loss value 338.1504821777344, l2 loss: 0.11402798444032669\n",
      "iteration 3976, loss value 338.150146484375, l2 loss: 0.11402798444032669\n",
      "iteration 3977, loss value 338.1502990722656, l2 loss: 0.11402798444032669\n",
      "iteration 3978, loss value 338.15008544921875, l2 loss: 0.1140279471874237\n",
      "iteration 3979, loss value 338.150146484375, l2 loss: 0.11402791738510132\n",
      "iteration 3980, loss value 338.1504211425781, l2 loss: 0.11402791738510132\n",
      "iteration 3981, loss value 338.1504211425781, l2 loss: 0.11402787268161774\n",
      "iteration 3982, loss value 338.1500244140625, l2 loss: 0.11402782052755356\n",
      "iteration 3983, loss value 338.15008544921875, l2 loss: 0.11402777582406998\n",
      "iteration 3984, loss value 338.1498718261719, l2 loss: 0.11402773857116699\n",
      "iteration 3985, loss value 338.14984130859375, l2 loss: 0.11402767151594162\n",
      "iteration 3986, loss value 338.14996337890625, l2 loss: 0.11402763426303864\n",
      "iteration 3987, loss value 338.1495361328125, l2 loss: 0.11402756720781326\n",
      "iteration 3988, loss value 338.1496276855469, l2 loss: 0.11402750015258789\n",
      "iteration 3989, loss value 338.14971923828125, l2 loss: 0.11402744054794312\n",
      "iteration 3990, loss value 338.1497497558594, l2 loss: 0.11402735114097595\n",
      "iteration 3991, loss value 338.1495056152344, l2 loss: 0.11402729898691177\n",
      "iteration 3992, loss value 338.1492614746094, l2 loss: 0.1140272244811058\n",
      "iteration 3993, loss value 338.1495361328125, l2 loss: 0.11402714997529984\n",
      "iteration 3994, loss value 338.14947509765625, l2 loss: 0.11402706056833267\n",
      "iteration 3995, loss value 338.14935302734375, l2 loss: 0.1140269786119461\n",
      "iteration 3996, loss value 338.14984130859375, l2 loss: 0.11402686685323715\n",
      "iteration 3997, loss value 338.14947509765625, l2 loss: 0.1140267550945282\n",
      "iteration 3998, loss value 338.1492004394531, l2 loss: 0.11402668058872223\n",
      "iteration 3999, loss value 338.1492614746094, l2 loss: 0.11402657628059387\n",
      "iteration 4000, loss value 338.1492614746094, l2 loss: 0.11402647197246552\n",
      "iteration 4001, loss value 338.1489562988281, l2 loss: 0.11402635276317596\n",
      "iteration 4002, loss value 338.14910888671875, l2 loss: 0.11402624100446701\n",
      "iteration 4003, loss value 338.1488952636719, l2 loss: 0.11402612179517746\n",
      "iteration 4004, loss value 338.1489562988281, l2 loss: 0.11402595788240433\n",
      "iteration 4005, loss value 338.1487731933594, l2 loss: 0.11402583867311478\n",
      "iteration 4006, loss value 338.14849853515625, l2 loss: 0.11402569711208344\n",
      "iteration 4007, loss value 338.1485595703125, l2 loss: 0.1140255406498909\n",
      "iteration 4008, loss value 338.1486511230469, l2 loss: 0.11402538418769836\n",
      "iteration 4009, loss value 338.14849853515625, l2 loss: 0.11402523517608643\n",
      "iteration 4010, loss value 338.1485595703125, l2 loss: 0.1140250563621521\n",
      "iteration 4011, loss value 338.1482849121094, l2 loss: 0.11402489989995956\n",
      "iteration 4012, loss value 338.1485595703125, l2 loss: 0.11402471363544464\n",
      "iteration 4013, loss value 338.1483154296875, l2 loss: 0.11402453482151031\n",
      "iteration 4014, loss value 338.1481628417969, l2 loss: 0.11402439326047897\n",
      "iteration 4015, loss value 338.14837646484375, l2 loss: 0.11402420699596405\n",
      "iteration 4016, loss value 338.1483459472656, l2 loss: 0.11402402818202972\n",
      "iteration 4017, loss value 338.1484069824219, l2 loss: 0.1140238344669342\n",
      "iteration 4018, loss value 338.1481628417969, l2 loss: 0.11402366310358047\n",
      "iteration 4019, loss value 338.1482238769531, l2 loss: 0.11402347683906555\n",
      "iteration 4020, loss value 338.1483154296875, l2 loss: 0.11402329802513123\n",
      "iteration 4021, loss value 338.14849853515625, l2 loss: 0.11402306705713272\n",
      "iteration 4022, loss value 338.14801025390625, l2 loss: 0.11402284353971481\n",
      "iteration 4023, loss value 338.14752197265625, l2 loss: 0.1140226498246193\n",
      "iteration 4024, loss value 338.14752197265625, l2 loss: 0.11402244865894318\n",
      "iteration 4025, loss value 338.1477966308594, l2 loss: 0.11402223259210587\n",
      "iteration 4026, loss value 338.14801025390625, l2 loss: 0.11402199417352676\n",
      "iteration 4027, loss value 338.14739990234375, l2 loss: 0.11402180045843124\n",
      "iteration 4028, loss value 338.14788818359375, l2 loss: 0.11402155458927155\n",
      "iteration 4029, loss value 338.1471862792969, l2 loss: 0.11402132362127304\n",
      "iteration 4030, loss value 338.1473693847656, l2 loss: 0.11402111500501633\n",
      "iteration 4031, loss value 338.14764404296875, l2 loss: 0.11402086168527603\n",
      "iteration 4032, loss value 338.1473388671875, l2 loss: 0.11402060836553574\n",
      "iteration 4033, loss value 338.1472473144531, l2 loss: 0.11402037739753723\n",
      "iteration 4034, loss value 338.1473083496094, l2 loss: 0.11402010172605515\n",
      "iteration 4035, loss value 338.14703369140625, l2 loss: 0.11401984095573425\n",
      "iteration 4036, loss value 338.1466979980469, l2 loss: 0.11401958018541336\n",
      "iteration 4037, loss value 338.1474914550781, l2 loss: 0.11401929706335068\n",
      "iteration 4038, loss value 338.1473083496094, l2 loss: 0.11401902139186859\n",
      "iteration 4039, loss value 338.1470031738281, l2 loss: 0.1140187457203865\n",
      "iteration 4040, loss value 338.1471252441406, l2 loss: 0.11401844769716263\n",
      "iteration 4041, loss value 338.1470642089844, l2 loss: 0.11401813477277756\n",
      "iteration 4042, loss value 338.1470031738281, l2 loss: 0.11401785165071487\n",
      "iteration 4043, loss value 338.1470642089844, l2 loss: 0.1140175461769104\n",
      "iteration 4044, loss value 338.1468505859375, l2 loss: 0.11401723325252533\n",
      "iteration 4045, loss value 338.1468505859375, l2 loss: 0.11401691287755966\n",
      "iteration 4046, loss value 338.1465759277344, l2 loss: 0.1140165776014328\n",
      "iteration 4047, loss value 338.1465148925781, l2 loss: 0.11401626467704773\n",
      "iteration 4048, loss value 338.1463928222656, l2 loss: 0.11401595175266266\n",
      "iteration 4049, loss value 338.1468200683594, l2 loss: 0.11401563137769699\n",
      "iteration 4050, loss value 338.1465759277344, l2 loss: 0.11401530355215073\n",
      "iteration 4051, loss value 338.1468811035156, l2 loss: 0.11401496827602386\n",
      "iteration 4052, loss value 338.14678955078125, l2 loss: 0.11401461809873581\n",
      "iteration 4053, loss value 338.1466064453125, l2 loss: 0.11401429027318954\n",
      "iteration 4054, loss value 338.1463317871094, l2 loss: 0.1140139251947403\n",
      "iteration 4055, loss value 338.1461486816406, l2 loss: 0.11401358991861343\n",
      "iteration 4056, loss value 338.14642333984375, l2 loss: 0.11401323974132538\n",
      "iteration 4057, loss value 338.14617919921875, l2 loss: 0.11401287466287613\n",
      "iteration 4058, loss value 338.14617919921875, l2 loss: 0.11401254683732986\n",
      "iteration 4059, loss value 338.1462097167969, l2 loss: 0.11401217430830002\n",
      "iteration 4060, loss value 338.1462097167969, l2 loss: 0.11401182413101196\n",
      "iteration 4061, loss value 338.1464538574219, l2 loss: 0.11401144415140152\n",
      "iteration 4062, loss value 338.1459655761719, l2 loss: 0.11401107162237167\n",
      "iteration 4063, loss value 338.1460266113281, l2 loss: 0.11401072889566422\n",
      "iteration 4064, loss value 338.14642333984375, l2 loss: 0.11401034146547318\n",
      "iteration 4065, loss value 338.1462097167969, l2 loss: 0.11400997638702393\n",
      "iteration 4066, loss value 338.1456604003906, l2 loss: 0.11400957405567169\n",
      "iteration 4067, loss value 338.1455993652344, l2 loss: 0.11400920897722244\n",
      "iteration 4068, loss value 338.145751953125, l2 loss: 0.1140088140964508\n",
      "iteration 4069, loss value 338.1456604003906, l2 loss: 0.11400839686393738\n",
      "iteration 4070, loss value 338.14532470703125, l2 loss: 0.11400800943374634\n",
      "iteration 4071, loss value 338.1457824707031, l2 loss: 0.11400759220123291\n",
      "iteration 4072, loss value 338.1455993652344, l2 loss: 0.11400716751813889\n",
      "iteration 4073, loss value 338.14581298828125, l2 loss: 0.11400675773620605\n",
      "iteration 4074, loss value 338.1456604003906, l2 loss: 0.11400635540485382\n",
      "iteration 4075, loss value 338.1455078125, l2 loss: 0.1140059158205986\n",
      "iteration 4076, loss value 338.1451110839844, l2 loss: 0.11400552839040756\n",
      "iteration 4077, loss value 338.1451110839844, l2 loss: 0.11400508135557175\n",
      "iteration 4078, loss value 338.14569091796875, l2 loss: 0.11400461196899414\n",
      "iteration 4079, loss value 338.1453552246094, l2 loss: 0.11400418728590012\n",
      "iteration 4080, loss value 338.1453552246094, l2 loss: 0.11400371044874191\n",
      "iteration 4081, loss value 338.1452941894531, l2 loss: 0.1140032634139061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4082, loss value 338.1454772949219, l2 loss: 0.1140027791261673\n",
      "iteration 4083, loss value 338.1452331542969, l2 loss: 0.11400233954191208\n",
      "iteration 4084, loss value 338.1452331542969, l2 loss: 0.11400184780359268\n",
      "iteration 4085, loss value 338.1449890136719, l2 loss: 0.11400134861469269\n",
      "iteration 4086, loss value 338.1445007324219, l2 loss: 0.11400091648101807\n",
      "iteration 4087, loss value 338.14508056640625, l2 loss: 0.11400041729211807\n",
      "iteration 4088, loss value 338.1445007324219, l2 loss: 0.11399994045495987\n",
      "iteration 4089, loss value 338.1443176269531, l2 loss: 0.11399947851896286\n",
      "iteration 4090, loss value 338.14453125, l2 loss: 0.11399898678064346\n",
      "iteration 4091, loss value 338.1447448730469, l2 loss: 0.11399848759174347\n",
      "iteration 4092, loss value 338.1446228027344, l2 loss: 0.11399798095226288\n",
      "iteration 4093, loss value 338.1442565917969, l2 loss: 0.11399748176336288\n",
      "iteration 4094, loss value 338.1441650390625, l2 loss: 0.1139969676733017\n",
      "iteration 4095, loss value 338.14434814453125, l2 loss: 0.11399645358324051\n",
      "iteration 4096, loss value 338.1446533203125, l2 loss: 0.11399591714143753\n",
      "iteration 4097, loss value 338.1441345214844, l2 loss: 0.11399540305137634\n",
      "iteration 4098, loss value 338.1442565917969, l2 loss: 0.11399485170841217\n",
      "iteration 4099, loss value 338.1440734863281, l2 loss: 0.11399435251951218\n",
      "iteration 4100, loss value 338.1445007324219, l2 loss: 0.11399383842945099\n",
      "iteration 4101, loss value 338.14447021484375, l2 loss: 0.11399330943822861\n",
      "iteration 4102, loss value 338.1443786621094, l2 loss: 0.11399275809526443\n",
      "iteration 4103, loss value 338.14410400390625, l2 loss: 0.11399219185113907\n",
      "iteration 4104, loss value 338.1439208984375, l2 loss: 0.11399167031049728\n",
      "iteration 4105, loss value 338.14404296875, l2 loss: 0.11399112641811371\n",
      "iteration 4106, loss value 338.14422607421875, l2 loss: 0.11399058252573013\n",
      "iteration 4107, loss value 338.14422607421875, l2 loss: 0.11399002373218536\n",
      "iteration 4108, loss value 338.14385986328125, l2 loss: 0.1139894500374794\n",
      "iteration 4109, loss value 338.1436462402344, l2 loss: 0.11398890614509583\n",
      "iteration 4110, loss value 338.1440734863281, l2 loss: 0.11398833990097046\n",
      "iteration 4111, loss value 338.1440124511719, l2 loss: 0.1139877587556839\n",
      "iteration 4112, loss value 338.1435546875, l2 loss: 0.11398716270923615\n",
      "iteration 4113, loss value 338.1431579589844, l2 loss: 0.11398659646511078\n",
      "iteration 4114, loss value 338.1434631347656, l2 loss: 0.11398599296808243\n",
      "iteration 4115, loss value 338.1432189941406, l2 loss: 0.11398541927337646\n",
      "iteration 4116, loss value 338.1432800292969, l2 loss: 0.11398482322692871\n",
      "iteration 4117, loss value 338.1434326171875, l2 loss: 0.11398420482873917\n",
      "iteration 4118, loss value 338.1429138183594, l2 loss: 0.1139836385846138\n",
      "iteration 4119, loss value 338.143310546875, l2 loss: 0.11398301273584366\n",
      "iteration 4120, loss value 338.1431579589844, l2 loss: 0.1139824241399765\n",
      "iteration 4121, loss value 338.1432189941406, l2 loss: 0.11398182064294815\n",
      "iteration 4122, loss value 338.143310546875, l2 loss: 0.11398117989301682\n",
      "iteration 4123, loss value 338.1429443359375, l2 loss: 0.11398059129714966\n",
      "iteration 4124, loss value 338.14312744140625, l2 loss: 0.11397997289896011\n",
      "iteration 4125, loss value 338.14276123046875, l2 loss: 0.11397933959960938\n",
      "iteration 4126, loss value 338.14276123046875, l2 loss: 0.11397872865200043\n",
      "iteration 4127, loss value 338.1429138183594, l2 loss: 0.11397808790206909\n",
      "iteration 4128, loss value 338.1428527832031, l2 loss: 0.11397745460271835\n",
      "iteration 4129, loss value 338.1423034667969, l2 loss: 0.11397680640220642\n",
      "iteration 4130, loss value 338.1427917480469, l2 loss: 0.11397619545459747\n",
      "iteration 4131, loss value 338.1427917480469, l2 loss: 0.11397555470466614\n",
      "iteration 4132, loss value 338.14300537109375, l2 loss: 0.11397489905357361\n",
      "iteration 4133, loss value 338.142822265625, l2 loss: 0.11397422105073929\n",
      "iteration 4134, loss value 338.1426086425781, l2 loss: 0.11397358030080795\n",
      "iteration 4135, loss value 338.1424255371094, l2 loss: 0.11397290974855423\n",
      "iteration 4136, loss value 338.1424560546875, l2 loss: 0.1139722391963005\n",
      "iteration 4137, loss value 338.14251708984375, l2 loss: 0.11397159099578857\n",
      "iteration 4138, loss value 338.1425476074219, l2 loss: 0.11397090554237366\n",
      "iteration 4139, loss value 338.14227294921875, l2 loss: 0.11397022753953934\n",
      "iteration 4140, loss value 338.1424255371094, l2 loss: 0.11396953463554382\n",
      "iteration 4141, loss value 338.1424255371094, l2 loss: 0.11396883428096771\n",
      "iteration 4142, loss value 338.1424865722656, l2 loss: 0.11396816372871399\n",
      "iteration 4143, loss value 338.14276123046875, l2 loss: 0.11396745592355728\n",
      "iteration 4144, loss value 338.14239501953125, l2 loss: 0.11396673321723938\n",
      "iteration 4145, loss value 338.142333984375, l2 loss: 0.11396603286266327\n",
      "iteration 4146, loss value 338.1424560546875, l2 loss: 0.11396531015634537\n",
      "iteration 4147, loss value 338.14227294921875, l2 loss: 0.11396460980176926\n",
      "iteration 4148, loss value 338.14227294921875, l2 loss: 0.11396387964487076\n",
      "iteration 4149, loss value 338.1418762207031, l2 loss: 0.11396314948797226\n",
      "iteration 4150, loss value 338.1414489746094, l2 loss: 0.11396244168281555\n",
      "iteration 4151, loss value 338.14202880859375, l2 loss: 0.11396168172359467\n",
      "iteration 4152, loss value 338.1413269042969, l2 loss: 0.11396095901727676\n",
      "iteration 4153, loss value 338.1418762207031, l2 loss: 0.11396020650863647\n",
      "iteration 4154, loss value 338.1410827636719, l2 loss: 0.11395946145057678\n",
      "iteration 4155, loss value 338.1414489746094, l2 loss: 0.1139586940407753\n",
      "iteration 4156, loss value 338.14141845703125, l2 loss: 0.1139579638838768\n",
      "iteration 4157, loss value 338.1417236328125, l2 loss: 0.11395717412233353\n",
      "iteration 4158, loss value 338.1415100097656, l2 loss: 0.11395641416311264\n",
      "iteration 4159, loss value 338.1416931152344, l2 loss: 0.11395564675331116\n",
      "iteration 4160, loss value 338.1418151855469, l2 loss: 0.11395485699176788\n",
      "iteration 4161, loss value 338.1412353515625, l2 loss: 0.1139541044831276\n",
      "iteration 4162, loss value 338.14141845703125, l2 loss: 0.11395329982042313\n",
      "iteration 4163, loss value 338.14105224609375, l2 loss: 0.11395254731178284\n",
      "iteration 4164, loss value 338.14117431640625, l2 loss: 0.11395174264907837\n",
      "iteration 4165, loss value 338.1410827636719, l2 loss: 0.1139509454369545\n",
      "iteration 4166, loss value 338.14117431640625, l2 loss: 0.11395015567541122\n",
      "iteration 4167, loss value 338.1407165527344, l2 loss: 0.11394933611154556\n",
      "iteration 4168, loss value 338.1411437988281, l2 loss: 0.11394857615232468\n",
      "iteration 4169, loss value 338.1413269042969, l2 loss: 0.11394772678613663\n",
      "iteration 4170, loss value 338.1409606933594, l2 loss: 0.11394693702459335\n",
      "iteration 4171, loss value 338.1412048339844, l2 loss: 0.1139461025595665\n",
      "iteration 4172, loss value 338.140625, l2 loss: 0.11394529044628143\n",
      "iteration 4173, loss value 338.1408386230469, l2 loss: 0.11394447833299637\n",
      "iteration 4174, loss value 338.1409912109375, l2 loss: 0.11394362896680832\n",
      "iteration 4175, loss value 338.1403503417969, l2 loss: 0.11394281685352325\n",
      "iteration 4176, loss value 338.1407165527344, l2 loss: 0.1139419749379158\n",
      "iteration 4177, loss value 338.14068603515625, l2 loss: 0.11394114047288895\n",
      "iteration 4178, loss value 338.1407165527344, l2 loss: 0.11394033581018448\n",
      "iteration 4179, loss value 338.1410217285156, l2 loss: 0.11393950879573822\n",
      "iteration 4180, loss value 338.1404724121094, l2 loss: 0.11393867433071136\n",
      "iteration 4181, loss value 338.140380859375, l2 loss: 0.11393781751394272\n",
      "iteration 4182, loss value 338.1405944824219, l2 loss: 0.11393696069717407\n",
      "iteration 4183, loss value 338.1405944824219, l2 loss: 0.11393614113330841\n",
      "iteration 4184, loss value 338.14044189453125, l2 loss: 0.11393529176712036\n",
      "iteration 4185, loss value 338.14044189453125, l2 loss: 0.11393441259860992\n",
      "iteration 4186, loss value 338.139892578125, l2 loss: 0.11393357068300247\n",
      "iteration 4187, loss value 338.1405944824219, l2 loss: 0.11393269896507263\n",
      "iteration 4188, loss value 338.1402587890625, l2 loss: 0.1139317974448204\n",
      "iteration 4189, loss value 338.1400451660156, l2 loss: 0.11393093317747116\n",
      "iteration 4190, loss value 338.1402587890625, l2 loss: 0.11393004655838013\n",
      "iteration 4191, loss value 338.14007568359375, l2 loss: 0.1139291450381279\n",
      "iteration 4192, loss value 338.1396789550781, l2 loss: 0.11392825096845627\n",
      "iteration 4193, loss value 338.1397399902344, l2 loss: 0.11392735689878464\n",
      "iteration 4194, loss value 338.1402282714844, l2 loss: 0.11392644047737122\n",
      "iteration 4195, loss value 338.139892578125, l2 loss: 0.11392553895711899\n",
      "iteration 4196, loss value 338.1399841308594, l2 loss: 0.11392462253570557\n",
      "iteration 4197, loss value 338.13983154296875, l2 loss: 0.11392367631196976\n",
      "iteration 4198, loss value 338.1394348144531, l2 loss: 0.11392276734113693\n",
      "iteration 4199, loss value 338.1398010253906, l2 loss: 0.11392184346914291\n",
      "iteration 4200, loss value 338.13970947265625, l2 loss: 0.1139209121465683\n",
      "iteration 4201, loss value 338.13934326171875, l2 loss: 0.11391996592283249\n",
      "iteration 4202, loss value 338.1394348144531, l2 loss: 0.11391901969909668\n",
      "iteration 4203, loss value 338.13970947265625, l2 loss: 0.11391808092594147\n",
      "iteration 4204, loss value 338.1395568847656, l2 loss: 0.11391714215278625\n",
      "iteration 4205, loss value 338.1394958496094, l2 loss: 0.11391618102788925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4206, loss value 338.1392517089844, l2 loss: 0.11391520500183105\n",
      "iteration 4207, loss value 338.1391296386719, l2 loss: 0.11391426622867584\n",
      "iteration 4208, loss value 338.1390075683594, l2 loss: 0.11391332745552063\n",
      "iteration 4209, loss value 338.1392517089844, l2 loss: 0.11391236633062363\n",
      "iteration 4210, loss value 338.1399841308594, l2 loss: 0.11391142010688782\n",
      "iteration 4211, loss value 338.1393737792969, l2 loss: 0.11391045153141022\n",
      "iteration 4212, loss value 338.13922119140625, l2 loss: 0.11390949040651321\n",
      "iteration 4213, loss value 338.1390380859375, l2 loss: 0.11390849202871323\n",
      "iteration 4214, loss value 338.13897705078125, l2 loss: 0.11390752345323563\n",
      "iteration 4215, loss value 338.1390075683594, l2 loss: 0.11390653997659683\n",
      "iteration 4216, loss value 338.1391296386719, l2 loss: 0.11390554159879684\n",
      "iteration 4217, loss value 338.13909912109375, l2 loss: 0.11390455812215805\n",
      "iteration 4218, loss value 338.139404296875, l2 loss: 0.11390355974435806\n",
      "iteration 4219, loss value 338.1392822265625, l2 loss: 0.11390255391597748\n",
      "iteration 4220, loss value 338.13897705078125, l2 loss: 0.1139015480875969\n",
      "iteration 4221, loss value 338.1387939453125, l2 loss: 0.11390052735805511\n",
      "iteration 4222, loss value 338.13873291015625, l2 loss: 0.11389952152967453\n",
      "iteration 4223, loss value 338.1387634277344, l2 loss: 0.11389851570129395\n",
      "iteration 4224, loss value 338.1387939453125, l2 loss: 0.11389748752117157\n",
      "iteration 4225, loss value 338.1387634277344, l2 loss: 0.11389647424221039\n",
      "iteration 4226, loss value 338.1387939453125, l2 loss: 0.1138954758644104\n",
      "iteration 4227, loss value 338.138916015625, l2 loss: 0.11389444023370743\n",
      "iteration 4228, loss value 338.13897705078125, l2 loss: 0.11389338225126266\n",
      "iteration 4229, loss value 338.1381530761719, l2 loss: 0.11389237642288208\n",
      "iteration 4230, loss value 338.1385498046875, l2 loss: 0.11389133334159851\n",
      "iteration 4231, loss value 338.1383361816406, l2 loss: 0.11389027535915375\n",
      "iteration 4232, loss value 338.1380920410156, l2 loss: 0.11388924717903137\n",
      "iteration 4233, loss value 338.1383972167969, l2 loss: 0.1138882115483284\n",
      "iteration 4234, loss value 338.138427734375, l2 loss: 0.11388714611530304\n",
      "iteration 4235, loss value 338.1381530761719, l2 loss: 0.11388609558343887\n",
      "iteration 4236, loss value 338.1383056640625, l2 loss: 0.11388501524925232\n",
      "iteration 4237, loss value 338.1381530761719, l2 loss: 0.11388395726680756\n",
      "iteration 4238, loss value 338.1381530761719, l2 loss: 0.1138828694820404\n",
      "iteration 4239, loss value 338.13775634765625, l2 loss: 0.11388183385133743\n",
      "iteration 4240, loss value 338.1383972167969, l2 loss: 0.11388072371482849\n",
      "iteration 4241, loss value 338.1376647949219, l2 loss: 0.11387963593006134\n",
      "iteration 4242, loss value 338.1378173828125, l2 loss: 0.1138785183429718\n",
      "iteration 4243, loss value 338.1373596191406, l2 loss: 0.11387746036052704\n",
      "iteration 4244, loss value 338.1376647949219, l2 loss: 0.1138763278722763\n",
      "iteration 4245, loss value 338.1374206542969, l2 loss: 0.11387524008750916\n",
      "iteration 4246, loss value 338.13787841796875, l2 loss: 0.113874152302742\n",
      "iteration 4247, loss value 338.1376647949219, l2 loss: 0.11387303471565247\n",
      "iteration 4248, loss value 338.1376953125, l2 loss: 0.11387192457914352\n",
      "iteration 4249, loss value 338.1375427246094, l2 loss: 0.11387081444263458\n",
      "iteration 4250, loss value 338.1377868652344, l2 loss: 0.11386971175670624\n",
      "iteration 4251, loss value 338.1370849609375, l2 loss: 0.1138685941696167\n",
      "iteration 4252, loss value 338.1374206542969, l2 loss: 0.11386747658252716\n",
      "iteration 4253, loss value 338.13714599609375, l2 loss: 0.11386638134717941\n",
      "iteration 4254, loss value 338.1378173828125, l2 loss: 0.11386523395776749\n",
      "iteration 4255, loss value 338.1369934082031, l2 loss: 0.11386412382125854\n",
      "iteration 4256, loss value 338.13751220703125, l2 loss: 0.11386299133300781\n",
      "iteration 4257, loss value 338.13739013671875, l2 loss: 0.11386182904243469\n",
      "iteration 4258, loss value 338.1371765136719, l2 loss: 0.11386071145534515\n",
      "iteration 4259, loss value 338.13714599609375, l2 loss: 0.11385954916477203\n",
      "iteration 4260, loss value 338.1368713378906, l2 loss: 0.1138584092259407\n",
      "iteration 4261, loss value 338.13720703125, l2 loss: 0.11385726183652878\n",
      "iteration 4262, loss value 338.13720703125, l2 loss: 0.11385606974363327\n",
      "iteration 4263, loss value 338.13653564453125, l2 loss: 0.11385492980480194\n",
      "iteration 4264, loss value 338.1367492675781, l2 loss: 0.11385375261306763\n",
      "iteration 4265, loss value 338.1368103027344, l2 loss: 0.11385257542133331\n",
      "iteration 4266, loss value 338.13690185546875, l2 loss: 0.11385137587785721\n",
      "iteration 4267, loss value 338.13671875, l2 loss: 0.1138501912355423\n",
      "iteration 4268, loss value 338.13671875, l2 loss: 0.11384901404380798\n",
      "iteration 4269, loss value 338.1368103027344, l2 loss: 0.11384782940149307\n",
      "iteration 4270, loss value 338.1368713378906, l2 loss: 0.11384662240743637\n",
      "iteration 4271, loss value 338.1365051269531, l2 loss: 0.11384546756744385\n",
      "iteration 4272, loss value 338.13677978515625, l2 loss: 0.11384430527687073\n",
      "iteration 4273, loss value 338.13665771484375, l2 loss: 0.11384309083223343\n",
      "iteration 4274, loss value 338.13677978515625, l2 loss: 0.11384189873933792\n",
      "iteration 4275, loss value 338.1368103027344, l2 loss: 0.11384068429470062\n",
      "iteration 4276, loss value 338.13641357421875, l2 loss: 0.1138395220041275\n",
      "iteration 4277, loss value 338.1368713378906, l2 loss: 0.11383828520774841\n",
      "iteration 4278, loss value 338.1363525390625, l2 loss: 0.11383708566427231\n",
      "iteration 4279, loss value 338.13623046875, l2 loss: 0.1138358861207962\n",
      "iteration 4280, loss value 338.13641357421875, l2 loss: 0.1138346716761589\n",
      "iteration 4281, loss value 338.1365966796875, l2 loss: 0.11383341997861862\n",
      "iteration 4282, loss value 338.1362609863281, l2 loss: 0.11383220553398132\n",
      "iteration 4283, loss value 338.13604736328125, l2 loss: 0.11383094638586044\n",
      "iteration 4284, loss value 338.13604736328125, l2 loss: 0.11382970213890076\n",
      "iteration 4285, loss value 338.13592529296875, l2 loss: 0.11382847279310226\n",
      "iteration 4286, loss value 338.1361999511719, l2 loss: 0.11382720619440079\n",
      "iteration 4287, loss value 338.1357727050781, l2 loss: 0.1138259768486023\n",
      "iteration 4288, loss value 338.13616943359375, l2 loss: 0.11382470279932022\n",
      "iteration 4289, loss value 338.1358642578125, l2 loss: 0.11382345110177994\n",
      "iteration 4290, loss value 338.1360778808594, l2 loss: 0.11382215470075607\n",
      "iteration 4291, loss value 338.1353759765625, l2 loss: 0.1138208881020546\n",
      "iteration 4292, loss value 338.13580322265625, l2 loss: 0.11381962150335312\n",
      "iteration 4293, loss value 338.1357727050781, l2 loss: 0.11381832510232925\n",
      "iteration 4294, loss value 338.13555908203125, l2 loss: 0.11381705850362778\n",
      "iteration 4295, loss value 338.1356201171875, l2 loss: 0.11381575465202332\n",
      "iteration 4296, loss value 338.1354675292969, l2 loss: 0.11381448060274124\n",
      "iteration 4297, loss value 338.1359558105469, l2 loss: 0.11381321400403976\n",
      "iteration 4298, loss value 338.13555908203125, l2 loss: 0.1138119101524353\n",
      "iteration 4299, loss value 338.135498046875, l2 loss: 0.11381062865257263\n",
      "iteration 4300, loss value 338.1357116699219, l2 loss: 0.11380933970212936\n",
      "iteration 4301, loss value 338.1354675292969, l2 loss: 0.11380801349878311\n",
      "iteration 4302, loss value 338.13531494140625, l2 loss: 0.11380673199892044\n",
      "iteration 4303, loss value 338.1354675292969, l2 loss: 0.11380542814731598\n",
      "iteration 4304, loss value 338.1357421875, l2 loss: 0.11380410939455032\n",
      "iteration 4305, loss value 338.13519287109375, l2 loss: 0.11380279809236526\n",
      "iteration 4306, loss value 338.13519287109375, l2 loss: 0.1138014942407608\n",
      "iteration 4307, loss value 338.1353454589844, l2 loss: 0.11380016058683395\n",
      "iteration 4308, loss value 338.1350402832031, l2 loss: 0.1137988343834877\n",
      "iteration 4309, loss value 338.13519287109375, l2 loss: 0.11379752308130264\n",
      "iteration 4310, loss value 338.1349792480469, l2 loss: 0.1137961894273758\n",
      "iteration 4311, loss value 338.13543701171875, l2 loss: 0.11379483342170715\n",
      "iteration 4312, loss value 338.1352233886719, l2 loss: 0.1137935072183609\n",
      "iteration 4313, loss value 338.1347961425781, l2 loss: 0.11379214376211166\n",
      "iteration 4314, loss value 338.13507080078125, l2 loss: 0.11379078030586243\n",
      "iteration 4315, loss value 338.13470458984375, l2 loss: 0.11378944665193558\n",
      "iteration 4316, loss value 338.1349182128906, l2 loss: 0.11378809809684753\n",
      "iteration 4317, loss value 338.1347351074219, l2 loss: 0.1137867346405983\n",
      "iteration 4318, loss value 338.134765625, l2 loss: 0.11378538608551025\n",
      "iteration 4319, loss value 338.13494873046875, l2 loss: 0.11378400772809982\n",
      "iteration 4320, loss value 338.1346130371094, l2 loss: 0.11378263682126999\n",
      "iteration 4321, loss value 338.1348571777344, l2 loss: 0.11378125846385956\n",
      "iteration 4322, loss value 338.1344299316406, l2 loss: 0.11377986520528793\n",
      "iteration 4323, loss value 338.1344909667969, l2 loss: 0.1137785017490387\n",
      "iteration 4324, loss value 338.134521484375, l2 loss: 0.11377708613872528\n",
      "iteration 4325, loss value 338.13470458984375, l2 loss: 0.11377568542957306\n",
      "iteration 4326, loss value 338.13470458984375, l2 loss: 0.11377428472042084\n",
      "iteration 4327, loss value 338.1344909667969, l2 loss: 0.11377287656068802\n",
      "iteration 4328, loss value 338.13458251953125, l2 loss: 0.1137714833021164\n",
      "iteration 4329, loss value 338.134765625, l2 loss: 0.11377006024122238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4330, loss value 338.13421630859375, l2 loss: 0.11376862227916718\n",
      "iteration 4331, loss value 338.1335144042969, l2 loss: 0.11376725137233734\n",
      "iteration 4332, loss value 338.13433837890625, l2 loss: 0.11376583576202393\n",
      "iteration 4333, loss value 338.1342468261719, l2 loss: 0.11376439034938812\n",
      "iteration 4334, loss value 338.1336975097656, l2 loss: 0.1137629896402359\n",
      "iteration 4335, loss value 338.1340026855469, l2 loss: 0.11376155912876129\n",
      "iteration 4336, loss value 338.1338806152344, l2 loss: 0.11376012116670609\n",
      "iteration 4337, loss value 338.13385009765625, l2 loss: 0.11375872045755386\n",
      "iteration 4338, loss value 338.134033203125, l2 loss: 0.11375726759433746\n",
      "iteration 4339, loss value 338.1338806152344, l2 loss: 0.11375582963228226\n",
      "iteration 4340, loss value 338.1338806152344, l2 loss: 0.11375439167022705\n",
      "iteration 4341, loss value 338.1337585449219, l2 loss: 0.11375296860933304\n",
      "iteration 4342, loss value 338.1341247558594, l2 loss: 0.11375148594379425\n",
      "iteration 4343, loss value 338.1339416503906, l2 loss: 0.11375005543231964\n",
      "iteration 4344, loss value 338.13421630859375, l2 loss: 0.11374856531620026\n",
      "iteration 4345, loss value 338.1337585449219, l2 loss: 0.11374712735414505\n",
      "iteration 4346, loss value 338.13348388671875, l2 loss: 0.11374564468860626\n",
      "iteration 4347, loss value 338.1334533691406, l2 loss: 0.11374418437480927\n",
      "iteration 4348, loss value 338.1335144042969, l2 loss: 0.11374267935752869\n",
      "iteration 4349, loss value 338.13299560546875, l2 loss: 0.11374121904373169\n",
      "iteration 4350, loss value 338.1334533691406, l2 loss: 0.1137397363781929\n",
      "iteration 4351, loss value 338.1331481933594, l2 loss: 0.11373826861381531\n",
      "iteration 4352, loss value 338.1333923339844, l2 loss: 0.11373675614595413\n",
      "iteration 4353, loss value 338.1333312988281, l2 loss: 0.11373528838157654\n",
      "iteration 4354, loss value 338.1336364746094, l2 loss: 0.11373376846313477\n",
      "iteration 4355, loss value 338.1333923339844, l2 loss: 0.11373225599527359\n",
      "iteration 4356, loss value 338.13323974609375, l2 loss: 0.1137307733297348\n",
      "iteration 4357, loss value 338.1330261230469, l2 loss: 0.11372924596071243\n",
      "iteration 4358, loss value 338.1333923339844, l2 loss: 0.11372771859169006\n",
      "iteration 4359, loss value 338.1332702636719, l2 loss: 0.11372619867324829\n",
      "iteration 4360, loss value 338.13323974609375, l2 loss: 0.11372470110654831\n",
      "iteration 4361, loss value 338.13330078125, l2 loss: 0.11372318118810654\n",
      "iteration 4362, loss value 338.13323974609375, l2 loss: 0.11372163891792297\n",
      "iteration 4363, loss value 338.1330261230469, l2 loss: 0.1137201339006424\n",
      "iteration 4364, loss value 338.13311767578125, l2 loss: 0.11371859908103943\n",
      "iteration 4365, loss value 338.13311767578125, l2 loss: 0.11371708661317825\n",
      "iteration 4366, loss value 338.1330871582031, l2 loss: 0.11371556669473648\n",
      "iteration 4367, loss value 338.1329040527344, l2 loss: 0.11371403932571411\n",
      "iteration 4368, loss value 338.13262939453125, l2 loss: 0.11371248215436935\n",
      "iteration 4369, loss value 338.1322937011719, l2 loss: 0.11371096968650818\n",
      "iteration 4370, loss value 338.13262939453125, l2 loss: 0.11370944231748581\n",
      "iteration 4371, loss value 338.1324462890625, l2 loss: 0.11370789259672165\n",
      "iteration 4372, loss value 338.132568359375, l2 loss: 0.1137063279747963\n",
      "iteration 4373, loss value 338.1325378417969, l2 loss: 0.11370477825403214\n",
      "iteration 4374, loss value 338.1326599121094, l2 loss: 0.11370322853326797\n",
      "iteration 4375, loss value 338.13287353515625, l2 loss: 0.11370166391134262\n",
      "iteration 4376, loss value 338.132568359375, l2 loss: 0.11370006948709488\n",
      "iteration 4377, loss value 338.1322021484375, l2 loss: 0.11369852721691132\n",
      "iteration 4378, loss value 338.1326599121094, l2 loss: 0.11369694769382477\n",
      "iteration 4379, loss value 338.1325378417969, l2 loss: 0.11369538307189941\n",
      "iteration 4380, loss value 338.1326599121094, l2 loss: 0.11369378864765167\n",
      "iteration 4381, loss value 338.13232421875, l2 loss: 0.11369220912456512\n",
      "iteration 4382, loss value 338.1322021484375, l2 loss: 0.11369063705205917\n",
      "iteration 4383, loss value 338.13238525390625, l2 loss: 0.11368900537490845\n",
      "iteration 4384, loss value 338.1322021484375, l2 loss: 0.1136874184012413\n",
      "iteration 4385, loss value 338.1322937011719, l2 loss: 0.11368583142757416\n",
      "iteration 4386, loss value 338.1321716308594, l2 loss: 0.11368421465158463\n",
      "iteration 4387, loss value 338.1315002441406, l2 loss: 0.11368259787559509\n",
      "iteration 4388, loss value 338.1315612792969, l2 loss: 0.11368099600076675\n",
      "iteration 4389, loss value 338.1319580078125, l2 loss: 0.11367938667535782\n",
      "iteration 4390, loss value 338.13214111328125, l2 loss: 0.11367776244878769\n",
      "iteration 4391, loss value 338.13214111328125, l2 loss: 0.11367611587047577\n",
      "iteration 4392, loss value 338.1320495605469, l2 loss: 0.11367446929216385\n",
      "iteration 4393, loss value 338.1316833496094, l2 loss: 0.11367284506559372\n",
      "iteration 4394, loss value 338.1321716308594, l2 loss: 0.1136711835861206\n",
      "iteration 4395, loss value 338.13134765625, l2 loss: 0.11366955935955048\n",
      "iteration 4396, loss value 338.13153076171875, l2 loss: 0.11366791278123856\n",
      "iteration 4397, loss value 338.1316833496094, l2 loss: 0.11366628110408783\n",
      "iteration 4398, loss value 338.1317138671875, l2 loss: 0.11366463452577591\n",
      "iteration 4399, loss value 338.13189697265625, l2 loss: 0.11366298794746399\n",
      "iteration 4400, loss value 338.131591796875, l2 loss: 0.11366134136915207\n",
      "iteration 4401, loss value 338.13189697265625, l2 loss: 0.11365965753793716\n",
      "iteration 4402, loss value 338.131591796875, l2 loss: 0.11365801841020584\n",
      "iteration 4403, loss value 338.1315612792969, l2 loss: 0.11365636438131332\n",
      "iteration 4404, loss value 338.1313781738281, l2 loss: 0.1136547103524208\n",
      "iteration 4405, loss value 338.13165283203125, l2 loss: 0.11365304887294769\n",
      "iteration 4406, loss value 338.13165283203125, l2 loss: 0.11365137994289398\n",
      "iteration 4407, loss value 338.1314392089844, l2 loss: 0.11364973336458206\n",
      "iteration 4408, loss value 338.13134765625, l2 loss: 0.11364804953336716\n",
      "iteration 4409, loss value 338.1313171386719, l2 loss: 0.11364637315273285\n",
      "iteration 4410, loss value 338.13116455078125, l2 loss: 0.11364466696977615\n",
      "iteration 4411, loss value 338.13067626953125, l2 loss: 0.11364301294088364\n",
      "iteration 4412, loss value 338.13067626953125, l2 loss: 0.11364131420850754\n",
      "iteration 4413, loss value 338.130859375, l2 loss: 0.11363964527845383\n",
      "iteration 4414, loss value 338.13116455078125, l2 loss: 0.11363795399665833\n",
      "iteration 4415, loss value 338.13116455078125, l2 loss: 0.11363625526428223\n",
      "iteration 4416, loss value 338.1307067871094, l2 loss: 0.11363455653190613\n",
      "iteration 4417, loss value 338.1307373046875, l2 loss: 0.11363285779953003\n",
      "iteration 4418, loss value 338.130859375, l2 loss: 0.11363113671541214\n",
      "iteration 4419, loss value 338.1307373046875, l2 loss: 0.11362943798303604\n",
      "iteration 4420, loss value 338.130615234375, l2 loss: 0.11362772434949875\n",
      "iteration 4421, loss value 338.1310729980469, l2 loss: 0.11362598091363907\n",
      "iteration 4422, loss value 338.130859375, l2 loss: 0.11362425982952118\n",
      "iteration 4423, loss value 338.13092041015625, l2 loss: 0.1136225014925003\n",
      "iteration 4424, loss value 338.1309509277344, l2 loss: 0.11362076550722122\n",
      "iteration 4425, loss value 338.1308288574219, l2 loss: 0.11361902207136154\n",
      "iteration 4426, loss value 338.1310729980469, l2 loss: 0.11361726373434067\n",
      "iteration 4427, loss value 338.1306457519531, l2 loss: 0.11361552029848099\n",
      "iteration 4428, loss value 338.13079833984375, l2 loss: 0.11361373215913773\n",
      "iteration 4429, loss value 338.1305847167969, l2 loss: 0.11361198127269745\n",
      "iteration 4430, loss value 338.1302795410156, l2 loss: 0.11361023038625717\n",
      "iteration 4431, loss value 338.13067626953125, l2 loss: 0.1136084571480751\n",
      "iteration 4432, loss value 338.1304931640625, l2 loss: 0.11360666900873184\n",
      "iteration 4433, loss value 338.1300354003906, l2 loss: 0.11360489577054977\n",
      "iteration 4434, loss value 338.1300964355469, l2 loss: 0.11360311508178711\n",
      "iteration 4435, loss value 338.1295166015625, l2 loss: 0.11360134184360504\n",
      "iteration 4436, loss value 338.130126953125, l2 loss: 0.11359954625368118\n",
      "iteration 4437, loss value 338.12969970703125, l2 loss: 0.11359775066375732\n",
      "iteration 4438, loss value 338.1300964355469, l2 loss: 0.11359598487615585\n",
      "iteration 4439, loss value 338.13031005859375, l2 loss: 0.11359421163797379\n",
      "iteration 4440, loss value 338.130126953125, l2 loss: 0.11359237879514694\n",
      "iteration 4441, loss value 338.1297912597656, l2 loss: 0.11359059810638428\n",
      "iteration 4442, loss value 338.12994384765625, l2 loss: 0.11358878761529922\n",
      "iteration 4443, loss value 338.129638671875, l2 loss: 0.11358700692653656\n",
      "iteration 4444, loss value 338.1300964355469, l2 loss: 0.1135852113366127\n",
      "iteration 4445, loss value 338.13018798828125, l2 loss: 0.11358338594436646\n",
      "iteration 4446, loss value 338.1299743652344, l2 loss: 0.113581582903862\n",
      "iteration 4447, loss value 338.12994384765625, l2 loss: 0.11357977241277695\n",
      "iteration 4448, loss value 338.1298828125, l2 loss: 0.11357796937227249\n",
      "iteration 4449, loss value 338.1300964355469, l2 loss: 0.11357613652944565\n",
      "iteration 4450, loss value 338.1293640136719, l2 loss: 0.11357434093952179\n",
      "iteration 4451, loss value 338.1297302246094, l2 loss: 0.11357250064611435\n",
      "iteration 4452, loss value 338.12982177734375, l2 loss: 0.11357066035270691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4453, loss value 338.1296081542969, l2 loss: 0.11356883496046066\n",
      "iteration 4454, loss value 338.1300354003906, l2 loss: 0.11356697976589203\n",
      "iteration 4455, loss value 338.1292419433594, l2 loss: 0.11356513947248459\n",
      "iteration 4456, loss value 338.12921142578125, l2 loss: 0.11356329917907715\n",
      "iteration 4457, loss value 338.1296691894531, l2 loss: 0.11356145143508911\n",
      "iteration 4458, loss value 338.12969970703125, l2 loss: 0.11355958133935928\n",
      "iteration 4459, loss value 338.129638671875, l2 loss: 0.11355771869421005\n",
      "iteration 4460, loss value 338.12939453125, l2 loss: 0.11355583369731903\n",
      "iteration 4461, loss value 338.1292419433594, l2 loss: 0.1135539636015892\n",
      "iteration 4462, loss value 338.1292419433594, l2 loss: 0.11355205625295639\n",
      "iteration 4463, loss value 338.1291198730469, l2 loss: 0.11355020105838776\n",
      "iteration 4464, loss value 338.1292419433594, l2 loss: 0.11354831606149673\n",
      "iteration 4465, loss value 338.1292724609375, l2 loss: 0.11354640126228333\n",
      "iteration 4466, loss value 338.1289367675781, l2 loss: 0.1135445162653923\n",
      "iteration 4467, loss value 338.12908935546875, l2 loss: 0.11354263126850128\n",
      "iteration 4468, loss value 338.1289978027344, l2 loss: 0.11354077607393265\n",
      "iteration 4469, loss value 338.12890625, l2 loss: 0.11353886872529984\n",
      "iteration 4470, loss value 338.1287536621094, l2 loss: 0.11353696882724762\n",
      "iteration 4471, loss value 338.1288757324219, l2 loss: 0.11353509873151779\n",
      "iteration 4472, loss value 338.1294250488281, l2 loss: 0.11353316158056259\n",
      "iteration 4473, loss value 338.12860107421875, l2 loss: 0.11353126168251038\n",
      "iteration 4474, loss value 338.12884521484375, l2 loss: 0.11352936178445816\n",
      "iteration 4475, loss value 338.1286315917969, l2 loss: 0.11352743953466415\n",
      "iteration 4476, loss value 338.1287536621094, l2 loss: 0.11352552473545074\n",
      "iteration 4477, loss value 338.1286315917969, l2 loss: 0.11352361738681793\n",
      "iteration 4478, loss value 338.128662109375, l2 loss: 0.11352168023586273\n",
      "iteration 4479, loss value 338.1288146972656, l2 loss: 0.11351976543664932\n",
      "iteration 4480, loss value 338.1287536621094, l2 loss: 0.11351783573627472\n",
      "iteration 4481, loss value 338.1285705566406, l2 loss: 0.1135159283876419\n",
      "iteration 4482, loss value 338.1286926269531, l2 loss: 0.1135140061378479\n",
      "iteration 4483, loss value 338.1288757324219, l2 loss: 0.1135120615363121\n",
      "iteration 4484, loss value 338.1285705566406, l2 loss: 0.1135101169347763\n",
      "iteration 4485, loss value 338.1282653808594, l2 loss: 0.1135081872344017\n",
      "iteration 4486, loss value 338.1286926269531, l2 loss: 0.11350623518228531\n",
      "iteration 4487, loss value 338.1285095214844, l2 loss: 0.11350426822900772\n",
      "iteration 4488, loss value 338.1280822753906, l2 loss: 0.11350235342979431\n",
      "iteration 4489, loss value 338.1282958984375, l2 loss: 0.11350037157535553\n",
      "iteration 4490, loss value 338.1279296875, l2 loss: 0.11349842697381973\n",
      "iteration 4491, loss value 338.128662109375, l2 loss: 0.11349643766880035\n",
      "iteration 4492, loss value 338.1279602050781, l2 loss: 0.11349451541900635\n",
      "iteration 4493, loss value 338.1280212402344, l2 loss: 0.11349255591630936\n",
      "iteration 4494, loss value 338.12860107421875, l2 loss: 0.11349058896303177\n",
      "iteration 4495, loss value 338.1283874511719, l2 loss: 0.11348862200975418\n",
      "iteration 4496, loss value 338.12811279296875, l2 loss: 0.113486647605896\n",
      "iteration 4497, loss value 338.1279602050781, l2 loss: 0.11348467320203781\n",
      "iteration 4498, loss value 338.1278991699219, l2 loss: 0.11348270624876022\n",
      "iteration 4499, loss value 338.1279602050781, l2 loss: 0.11348072439432144\n",
      "iteration 4500, loss value 338.12744140625, l2 loss: 0.11347874253988266\n",
      "iteration 4501, loss value 338.12762451171875, l2 loss: 0.11347676068544388\n",
      "iteration 4502, loss value 338.1282653808594, l2 loss: 0.1134747713804245\n",
      "iteration 4503, loss value 338.1280517578125, l2 loss: 0.11347277462482452\n",
      "iteration 4504, loss value 338.12786865234375, l2 loss: 0.11347073316574097\n",
      "iteration 4505, loss value 338.12750244140625, l2 loss: 0.11346876621246338\n",
      "iteration 4506, loss value 338.1278991699219, l2 loss: 0.11346675455570221\n",
      "iteration 4507, loss value 338.1278076171875, l2 loss: 0.11346472054719925\n",
      "iteration 4508, loss value 338.1273193359375, l2 loss: 0.11346269398927689\n",
      "iteration 4509, loss value 338.1275634765625, l2 loss: 0.11346067488193512\n",
      "iteration 4510, loss value 338.1274108886719, l2 loss: 0.11345862597227097\n",
      "iteration 4511, loss value 338.1272888183594, l2 loss: 0.113456591963768\n",
      "iteration 4512, loss value 338.1275329589844, l2 loss: 0.11345455795526505\n",
      "iteration 4513, loss value 338.1274108886719, l2 loss: 0.1134525015950203\n",
      "iteration 4514, loss value 338.1275329589844, l2 loss: 0.11345043033361435\n",
      "iteration 4515, loss value 338.1274108886719, l2 loss: 0.1134483739733696\n",
      "iteration 4516, loss value 338.1272277832031, l2 loss: 0.11344633251428604\n",
      "iteration 4517, loss value 338.1272888183594, l2 loss: 0.11344429105520248\n",
      "iteration 4518, loss value 338.12744140625, l2 loss: 0.11344221979379654\n",
      "iteration 4519, loss value 338.12701416015625, l2 loss: 0.11344014108181\n",
      "iteration 4520, loss value 338.12713623046875, l2 loss: 0.11343809217214584\n",
      "iteration 4521, loss value 338.126953125, l2 loss: 0.11343604326248169\n",
      "iteration 4522, loss value 338.1270446777344, l2 loss: 0.11343397945165634\n",
      "iteration 4523, loss value 338.12701416015625, l2 loss: 0.11343193799257278\n",
      "iteration 4524, loss value 338.127197265625, l2 loss: 0.11342987418174744\n",
      "iteration 4525, loss value 338.12677001953125, l2 loss: 0.11342780292034149\n",
      "iteration 4526, loss value 338.12664794921875, l2 loss: 0.11342577636241913\n",
      "iteration 4527, loss value 338.127197265625, l2 loss: 0.11342368274927139\n",
      "iteration 4528, loss value 338.12677001953125, l2 loss: 0.11342164129018784\n",
      "iteration 4529, loss value 338.12713623046875, l2 loss: 0.1134195402264595\n",
      "iteration 4530, loss value 338.12738037109375, l2 loss: 0.11341747641563416\n",
      "iteration 4531, loss value 338.1269226074219, l2 loss: 0.11341538280248642\n",
      "iteration 4532, loss value 338.1266174316406, l2 loss: 0.11341328918933868\n",
      "iteration 4533, loss value 338.12664794921875, l2 loss: 0.11341120302677155\n",
      "iteration 4534, loss value 338.1267395019531, l2 loss: 0.11340910196304321\n",
      "iteration 4535, loss value 338.12652587890625, l2 loss: 0.11340700834989548\n",
      "iteration 4536, loss value 338.12664794921875, l2 loss: 0.11340492963790894\n",
      "iteration 4537, loss value 338.1265563964844, l2 loss: 0.11340280622243881\n",
      "iteration 4538, loss value 338.1266784667969, l2 loss: 0.11340069025754929\n",
      "iteration 4539, loss value 338.12652587890625, l2 loss: 0.11339857429265976\n",
      "iteration 4540, loss value 338.1265563964844, l2 loss: 0.11339646577835083\n",
      "iteration 4541, loss value 338.1264343261719, l2 loss: 0.1133943423628807\n",
      "iteration 4542, loss value 338.1266784667969, l2 loss: 0.11339221149682999\n",
      "iteration 4543, loss value 338.1266784667969, l2 loss: 0.11339008808135986\n",
      "iteration 4544, loss value 338.12615966796875, l2 loss: 0.11338796466588974\n",
      "iteration 4545, loss value 338.12677001953125, l2 loss: 0.11338581144809723\n",
      "iteration 4546, loss value 338.12640380859375, l2 loss: 0.1133836954832077\n",
      "iteration 4547, loss value 338.1266784667969, l2 loss: 0.113381527364254\n",
      "iteration 4548, loss value 338.12603759765625, l2 loss: 0.11337939649820328\n",
      "iteration 4549, loss value 338.12664794921875, l2 loss: 0.11337722092866898\n",
      "iteration 4550, loss value 338.126220703125, l2 loss: 0.11337506771087646\n",
      "iteration 4551, loss value 338.12603759765625, l2 loss: 0.11337290704250336\n",
      "iteration 4552, loss value 338.1261901855469, l2 loss: 0.11337073892354965\n",
      "iteration 4553, loss value 338.1266174316406, l2 loss: 0.11336857825517654\n",
      "iteration 4554, loss value 338.1260986328125, l2 loss: 0.11336640268564224\n",
      "iteration 4555, loss value 338.12603759765625, l2 loss: 0.11336425691843033\n",
      "iteration 4556, loss value 338.12603759765625, l2 loss: 0.11336205154657364\n",
      "iteration 4557, loss value 338.1258239746094, l2 loss: 0.11335989087820053\n",
      "iteration 4558, loss value 338.1258850097656, l2 loss: 0.11335770785808563\n",
      "iteration 4559, loss value 338.12579345703125, l2 loss: 0.11335553973913193\n",
      "iteration 4560, loss value 338.1255798339844, l2 loss: 0.11335333436727524\n",
      "iteration 4561, loss value 338.1257019042969, l2 loss: 0.11335117369890213\n",
      "iteration 4562, loss value 338.12554931640625, l2 loss: 0.11334893852472305\n",
      "iteration 4563, loss value 338.1257629394531, l2 loss: 0.11334677785634995\n",
      "iteration 4564, loss value 338.12567138671875, l2 loss: 0.11334457993507385\n",
      "iteration 4565, loss value 338.1257019042969, l2 loss: 0.11334237456321716\n",
      "iteration 4566, loss value 338.1257629394531, l2 loss: 0.11334014683961868\n",
      "iteration 4567, loss value 338.1255187988281, l2 loss: 0.1133379340171814\n",
      "iteration 4568, loss value 338.12579345703125, l2 loss: 0.11333572119474411\n",
      "iteration 4569, loss value 338.12591552734375, l2 loss: 0.11333351582288742\n",
      "iteration 4570, loss value 338.1255187988281, l2 loss: 0.11333128809928894\n",
      "iteration 4571, loss value 338.1253662109375, l2 loss: 0.11332908272743225\n",
      "iteration 4572, loss value 338.1256103515625, l2 loss: 0.11332684755325317\n",
      "iteration 4573, loss value 338.1256408691406, l2 loss: 0.1133246123790741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4574, loss value 338.1253356933594, l2 loss: 0.11332239210605621\n",
      "iteration 4575, loss value 338.1248474121094, l2 loss: 0.11332018673419952\n",
      "iteration 4576, loss value 338.125244140625, l2 loss: 0.11331795156002045\n",
      "iteration 4577, loss value 338.1249084472656, l2 loss: 0.11331573128700256\n",
      "iteration 4578, loss value 338.12554931640625, l2 loss: 0.1133134737610817\n",
      "iteration 4579, loss value 338.12530517578125, l2 loss: 0.11331124603748322\n",
      "iteration 4580, loss value 338.1252746582031, l2 loss: 0.11330901086330414\n",
      "iteration 4581, loss value 338.12481689453125, l2 loss: 0.11330673098564148\n",
      "iteration 4582, loss value 338.1247863769531, l2 loss: 0.11330447345972061\n",
      "iteration 4583, loss value 338.1249084472656, l2 loss: 0.11330223083496094\n",
      "iteration 4584, loss value 338.1251220703125, l2 loss: 0.11329999566078186\n",
      "iteration 4585, loss value 338.1255798339844, l2 loss: 0.1132977232336998\n",
      "iteration 4586, loss value 338.1249694824219, l2 loss: 0.11329548805952072\n",
      "iteration 4587, loss value 338.124755859375, l2 loss: 0.11329320818185806\n",
      "iteration 4588, loss value 338.1244812011719, l2 loss: 0.11329097300767899\n",
      "iteration 4589, loss value 338.12493896484375, l2 loss: 0.11328867822885513\n",
      "iteration 4590, loss value 338.1242370605469, l2 loss: 0.11328643560409546\n",
      "iteration 4591, loss value 338.1246032714844, l2 loss: 0.113284170627594\n",
      "iteration 4592, loss value 338.124755859375, l2 loss: 0.11328189820051193\n",
      "iteration 4593, loss value 338.12469482421875, l2 loss: 0.11327961087226868\n",
      "iteration 4594, loss value 338.1245422363281, l2 loss: 0.113277368247509\n",
      "iteration 4595, loss value 338.12481689453125, l2 loss: 0.11327505856752396\n",
      "iteration 4596, loss value 338.124267578125, l2 loss: 0.11327279359102249\n",
      "iteration 4597, loss value 338.12481689453125, l2 loss: 0.11327047646045685\n",
      "iteration 4598, loss value 338.1243591308594, l2 loss: 0.11326818913221359\n",
      "iteration 4599, loss value 338.12457275390625, l2 loss: 0.11326585710048676\n",
      "iteration 4600, loss value 338.1242370605469, l2 loss: 0.11326354742050171\n",
      "iteration 4601, loss value 338.12445068359375, l2 loss: 0.11326122283935547\n",
      "iteration 4602, loss value 338.124755859375, l2 loss: 0.11325888335704803\n",
      "iteration 4603, loss value 338.1246032714844, l2 loss: 0.11325656622648239\n",
      "iteration 4604, loss value 338.1242370605469, l2 loss: 0.11325424909591675\n",
      "iteration 4605, loss value 338.12469482421875, l2 loss: 0.11325189471244812\n",
      "iteration 4606, loss value 338.1242370605469, l2 loss: 0.11324955523014069\n",
      "iteration 4607, loss value 338.12432861328125, l2 loss: 0.11324722319841385\n",
      "iteration 4608, loss value 338.1242370605469, l2 loss: 0.11324489861726761\n",
      "iteration 4609, loss value 338.1238708496094, l2 loss: 0.11324258148670197\n",
      "iteration 4610, loss value 338.1240234375, l2 loss: 0.11324025690555573\n",
      "iteration 4611, loss value 338.1241455078125, l2 loss: 0.11323794722557068\n",
      "iteration 4612, loss value 338.12432861328125, l2 loss: 0.11323558539152145\n",
      "iteration 4613, loss value 338.12384033203125, l2 loss: 0.1132332906126976\n",
      "iteration 4614, loss value 338.12408447265625, l2 loss: 0.11323094367980957\n",
      "iteration 4615, loss value 338.1236267089844, l2 loss: 0.11322861909866333\n",
      "iteration 4616, loss value 338.1240234375, l2 loss: 0.1132262572646141\n",
      "iteration 4617, loss value 338.1239929199219, l2 loss: 0.11322392523288727\n",
      "iteration 4618, loss value 338.1240539550781, l2 loss: 0.11322159320116043\n",
      "iteration 4619, loss value 338.12371826171875, l2 loss: 0.1132192313671112\n",
      "iteration 4620, loss value 338.12384033203125, l2 loss: 0.11321684718132019\n",
      "iteration 4621, loss value 338.12371826171875, l2 loss: 0.11321450024843216\n",
      "iteration 4622, loss value 338.12371826171875, l2 loss: 0.11321215331554413\n",
      "iteration 4623, loss value 338.1236572265625, l2 loss: 0.1132097840309143\n",
      "iteration 4624, loss value 338.123779296875, l2 loss: 0.11320740729570389\n",
      "iteration 4625, loss value 338.1235046386719, l2 loss: 0.11320503801107407\n",
      "iteration 4626, loss value 338.123779296875, l2 loss: 0.11320265382528305\n",
      "iteration 4627, loss value 338.123291015625, l2 loss: 0.11320024728775024\n",
      "iteration 4628, loss value 338.1233215332031, l2 loss: 0.11319786310195923\n",
      "iteration 4629, loss value 338.12310791015625, l2 loss: 0.11319547891616821\n",
      "iteration 4630, loss value 338.1232604980469, l2 loss: 0.11319313198328018\n",
      "iteration 4631, loss value 338.12432861328125, l2 loss: 0.11319068819284439\n",
      "iteration 4632, loss value 338.12322998046875, l2 loss: 0.11318830400705338\n",
      "iteration 4633, loss value 338.12347412109375, l2 loss: 0.11318589001893997\n",
      "iteration 4634, loss value 338.1231384277344, l2 loss: 0.11318348348140717\n",
      "iteration 4635, loss value 338.1227722167969, l2 loss: 0.11318109929561615\n",
      "iteration 4636, loss value 338.1232604980469, l2 loss: 0.11317867785692215\n",
      "iteration 4637, loss value 338.1231689453125, l2 loss: 0.11317627131938934\n",
      "iteration 4638, loss value 338.123046875, l2 loss: 0.11317384988069534\n",
      "iteration 4639, loss value 338.12274169921875, l2 loss: 0.11317143589258194\n",
      "iteration 4640, loss value 338.1229248046875, l2 loss: 0.11316899955272675\n",
      "iteration 4641, loss value 338.1230163574219, l2 loss: 0.11316657066345215\n",
      "iteration 4642, loss value 338.1230163574219, l2 loss: 0.11316415667533875\n",
      "iteration 4643, loss value 338.1233825683594, l2 loss: 0.11316172033548355\n",
      "iteration 4644, loss value 338.1231384277344, l2 loss: 0.11315926909446716\n",
      "iteration 4645, loss value 338.1228942871094, l2 loss: 0.11315684765577316\n",
      "iteration 4646, loss value 338.1226501464844, l2 loss: 0.11315441876649857\n",
      "iteration 4647, loss value 338.12274169921875, l2 loss: 0.11315198242664337\n",
      "iteration 4648, loss value 338.1228332519531, l2 loss: 0.11314955353736877\n",
      "iteration 4649, loss value 338.12274169921875, l2 loss: 0.11314710229635239\n",
      "iteration 4650, loss value 338.1227722167969, l2 loss: 0.1131446436047554\n",
      "iteration 4651, loss value 338.12249755859375, l2 loss: 0.11314217746257782\n",
      "iteration 4652, loss value 338.12261962890625, l2 loss: 0.11313971877098083\n",
      "iteration 4653, loss value 338.12249755859375, l2 loss: 0.11313726752996445\n",
      "iteration 4654, loss value 338.1226501464844, l2 loss: 0.11313480883836746\n",
      "iteration 4655, loss value 338.122314453125, l2 loss: 0.11313235759735107\n",
      "iteration 4656, loss value 338.12249755859375, l2 loss: 0.11312989890575409\n",
      "iteration 4657, loss value 338.1226501464844, l2 loss: 0.11312741786241531\n",
      "iteration 4658, loss value 338.1219482421875, l2 loss: 0.11312495172023773\n",
      "iteration 4659, loss value 338.1224365234375, l2 loss: 0.11312247067689896\n",
      "iteration 4660, loss value 338.12255859375, l2 loss: 0.11312001198530197\n",
      "iteration 4661, loss value 338.12237548828125, l2 loss: 0.113117516040802\n",
      "iteration 4662, loss value 338.1222839355469, l2 loss: 0.11311500519514084\n",
      "iteration 4663, loss value 338.1219177246094, l2 loss: 0.11311253160238266\n",
      "iteration 4664, loss value 338.12237548828125, l2 loss: 0.11311005055904388\n",
      "iteration 4665, loss value 338.12274169921875, l2 loss: 0.11310751736164093\n",
      "iteration 4666, loss value 338.12213134765625, l2 loss: 0.11310501396656036\n",
      "iteration 4667, loss value 338.1218566894531, l2 loss: 0.1131025031208992\n",
      "iteration 4668, loss value 338.1214904785156, l2 loss: 0.11310002207756042\n",
      "iteration 4669, loss value 338.1225280761719, l2 loss: 0.11309750378131866\n",
      "iteration 4670, loss value 338.12213134765625, l2 loss: 0.11309503018856049\n",
      "iteration 4671, loss value 338.12225341796875, l2 loss: 0.11309251189231873\n",
      "iteration 4672, loss value 338.1217346191406, l2 loss: 0.11308998614549637\n",
      "iteration 4673, loss value 338.1217956542969, l2 loss: 0.1130874827504158\n",
      "iteration 4674, loss value 338.1219177246094, l2 loss: 0.11308494955301285\n",
      "iteration 4675, loss value 338.1215515136719, l2 loss: 0.11308246850967407\n",
      "iteration 4676, loss value 338.12213134765625, l2 loss: 0.11307989805936813\n",
      "iteration 4677, loss value 338.1217956542969, l2 loss: 0.11307737976312637\n",
      "iteration 4678, loss value 338.1220397949219, l2 loss: 0.11307486891746521\n",
      "iteration 4679, loss value 338.12188720703125, l2 loss: 0.11307230591773987\n",
      "iteration 4680, loss value 338.1217041015625, l2 loss: 0.11306978017091751\n",
      "iteration 4681, loss value 338.12176513671875, l2 loss: 0.11306726187467575\n",
      "iteration 4682, loss value 338.1219482421875, l2 loss: 0.1130647212266922\n",
      "iteration 4683, loss value 338.12164306640625, l2 loss: 0.11306219547986984\n",
      "iteration 4684, loss value 338.1216125488281, l2 loss: 0.11305966228246689\n",
      "iteration 4685, loss value 338.12164306640625, l2 loss: 0.11305710673332214\n",
      "iteration 4686, loss value 338.1216735839844, l2 loss: 0.11305458098649979\n",
      "iteration 4687, loss value 338.12188720703125, l2 loss: 0.11305203288793564\n",
      "iteration 4688, loss value 338.12164306640625, l2 loss: 0.1130494773387909\n",
      "iteration 4689, loss value 338.12176513671875, l2 loss: 0.11304692924022675\n",
      "iteration 4690, loss value 338.1210632324219, l2 loss: 0.1130443662405014\n",
      "iteration 4691, loss value 338.12152099609375, l2 loss: 0.11304178833961487\n",
      "iteration 4692, loss value 338.1210632324219, l2 loss: 0.11303922533988953\n",
      "iteration 4693, loss value 338.1209716796875, l2 loss: 0.1130366325378418\n",
      "iteration 4694, loss value 338.1210632324219, l2 loss: 0.11303406953811646\n",
      "iteration 4695, loss value 338.121337890625, l2 loss: 0.11303146928548813\n",
      "iteration 4696, loss value 338.12103271484375, l2 loss: 0.1130288690328598\n",
      "iteration 4697, loss value 338.120849609375, l2 loss: 0.11302627623081207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4698, loss value 338.1210632324219, l2 loss: 0.11302370578050613\n",
      "iteration 4699, loss value 338.12115478515625, l2 loss: 0.11302110552787781\n",
      "iteration 4700, loss value 338.12139892578125, l2 loss: 0.11301852017641068\n",
      "iteration 4701, loss value 338.12103271484375, l2 loss: 0.11301591992378235\n",
      "iteration 4702, loss value 338.120849609375, l2 loss: 0.11301334202289581\n",
      "iteration 4703, loss value 338.1206970214844, l2 loss: 0.11301077902317047\n",
      "iteration 4704, loss value 338.121337890625, l2 loss: 0.11300817131996155\n",
      "iteration 4705, loss value 338.1210632324219, l2 loss: 0.11300558596849442\n",
      "iteration 4706, loss value 338.12103271484375, l2 loss: 0.11300303041934967\n",
      "iteration 4707, loss value 338.1213073730469, l2 loss: 0.11300045251846313\n",
      "iteration 4708, loss value 338.12109375, l2 loss: 0.1129978597164154\n",
      "iteration 4709, loss value 338.12078857421875, l2 loss: 0.11299525201320648\n",
      "iteration 4710, loss value 338.12060546875, l2 loss: 0.11299268156290054\n",
      "iteration 4711, loss value 338.1206970214844, l2 loss: 0.11299008876085281\n",
      "iteration 4712, loss value 338.12060546875, l2 loss: 0.1129874661564827\n",
      "iteration 4713, loss value 338.1207275390625, l2 loss: 0.11298487335443497\n",
      "iteration 4714, loss value 338.1209716796875, l2 loss: 0.11298223584890366\n",
      "iteration 4715, loss value 338.1203918457031, l2 loss: 0.11297961324453354\n",
      "iteration 4716, loss value 338.12060546875, l2 loss: 0.11297698318958282\n",
      "iteration 4717, loss value 338.12054443359375, l2 loss: 0.11297432333230972\n",
      "iteration 4718, loss value 338.1206359863281, l2 loss: 0.1129717081785202\n",
      "iteration 4719, loss value 338.12060546875, l2 loss: 0.1129690557718277\n",
      "iteration 4720, loss value 338.1204833984375, l2 loss: 0.11296640336513519\n",
      "iteration 4721, loss value 338.1205749511719, l2 loss: 0.11296374350786209\n",
      "iteration 4722, loss value 338.120361328125, l2 loss: 0.11296107620000839\n",
      "iteration 4723, loss value 338.1204528808594, l2 loss: 0.1129584014415741\n",
      "iteration 4724, loss value 338.1202087402344, l2 loss: 0.112955741584301\n",
      "iteration 4725, loss value 338.12066650390625, l2 loss: 0.1129530519247055\n",
      "iteration 4726, loss value 338.1203308105469, l2 loss: 0.112950399518013\n",
      "iteration 4727, loss value 338.1202392578125, l2 loss: 0.1129477322101593\n",
      "iteration 4728, loss value 338.1200256347656, l2 loss: 0.1129450872540474\n",
      "iteration 4729, loss value 338.1203308105469, l2 loss: 0.1129424050450325\n",
      "iteration 4730, loss value 338.119873046875, l2 loss: 0.1129397451877594\n",
      "iteration 4731, loss value 338.1202392578125, l2 loss: 0.1129370704293251\n",
      "iteration 4732, loss value 338.11920166015625, l2 loss: 0.112934410572052\n",
      "iteration 4733, loss value 338.1201171875, l2 loss: 0.11293172836303711\n",
      "iteration 4734, loss value 338.119873046875, l2 loss: 0.11292903870344162\n",
      "iteration 4735, loss value 338.1194763183594, l2 loss: 0.11292634904384613\n",
      "iteration 4736, loss value 338.1196594238281, l2 loss: 0.11292371153831482\n",
      "iteration 4737, loss value 338.1206359863281, l2 loss: 0.11292098462581635\n",
      "iteration 4738, loss value 338.1201171875, l2 loss: 0.11291828751564026\n",
      "iteration 4739, loss value 338.1197509765625, l2 loss: 0.11291559040546417\n",
      "iteration 4740, loss value 338.1198425292969, l2 loss: 0.11291291564702988\n",
      "iteration 4741, loss value 338.1199645996094, l2 loss: 0.11291022598743439\n",
      "iteration 4742, loss value 338.1194763183594, l2 loss: 0.1129075139760971\n",
      "iteration 4743, loss value 338.119384765625, l2 loss: 0.11290482431650162\n",
      "iteration 4744, loss value 338.1194152832031, l2 loss: 0.11290211975574493\n",
      "iteration 4745, loss value 338.1192932128906, l2 loss: 0.11289942264556885\n",
      "iteration 4746, loss value 338.1195068359375, l2 loss: 0.11289674788713455\n",
      "iteration 4747, loss value 338.1200866699219, l2 loss: 0.11289400607347488\n",
      "iteration 4748, loss value 338.11932373046875, l2 loss: 0.1128913015127182\n",
      "iteration 4749, loss value 338.1194763183594, l2 loss: 0.11288858205080032\n",
      "iteration 4750, loss value 338.119384765625, l2 loss: 0.11288585513830185\n",
      "iteration 4751, loss value 338.1192932128906, l2 loss: 0.11288315057754517\n",
      "iteration 4752, loss value 338.11956787109375, l2 loss: 0.1128804087638855\n",
      "iteration 4753, loss value 338.1192321777344, l2 loss: 0.11287766695022583\n",
      "iteration 4754, loss value 338.11871337890625, l2 loss: 0.11287492513656616\n",
      "iteration 4755, loss value 338.11944580078125, l2 loss: 0.1128721833229065\n",
      "iteration 4756, loss value 338.1197204589844, l2 loss: 0.11286943405866623\n",
      "iteration 4757, loss value 338.11932373046875, l2 loss: 0.11286669969558716\n",
      "iteration 4758, loss value 338.1191711425781, l2 loss: 0.11286398023366928\n",
      "iteration 4759, loss value 338.119384765625, l2 loss: 0.11286121606826782\n",
      "iteration 4760, loss value 338.1191711425781, l2 loss: 0.11285848170518875\n",
      "iteration 4761, loss value 338.11907958984375, l2 loss: 0.11285574734210968\n",
      "iteration 4762, loss value 338.1194152832031, l2 loss: 0.11285299807786942\n",
      "iteration 4763, loss value 338.1187744140625, l2 loss: 0.11285024136304855\n",
      "iteration 4764, loss value 338.119140625, l2 loss: 0.11284750699996948\n",
      "iteration 4765, loss value 338.1191101074219, l2 loss: 0.11284473538398743\n",
      "iteration 4766, loss value 338.1186218261719, l2 loss: 0.11284199357032776\n",
      "iteration 4767, loss value 338.1190490722656, l2 loss: 0.1128392368555069\n",
      "iteration 4768, loss value 338.11907958984375, l2 loss: 0.11283647269010544\n",
      "iteration 4769, loss value 338.1188659667969, l2 loss: 0.11283367872238159\n",
      "iteration 4770, loss value 338.1186218261719, l2 loss: 0.11283091455698013\n",
      "iteration 4771, loss value 338.11895751953125, l2 loss: 0.11282810568809509\n",
      "iteration 4772, loss value 338.1185607910156, l2 loss: 0.11282532662153244\n",
      "iteration 4773, loss value 338.11920166015625, l2 loss: 0.11282255500555038\n",
      "iteration 4774, loss value 338.1189880371094, l2 loss: 0.11281976103782654\n",
      "iteration 4775, loss value 338.11907958984375, l2 loss: 0.11281698197126389\n",
      "iteration 4776, loss value 338.1190185546875, l2 loss: 0.11281420290470123\n",
      "iteration 4777, loss value 338.11895751953125, l2 loss: 0.11281140148639679\n",
      "iteration 4778, loss value 338.118896484375, l2 loss: 0.11280861496925354\n",
      "iteration 4779, loss value 338.11907958984375, l2 loss: 0.1128057986497879\n",
      "iteration 4780, loss value 338.1184997558594, l2 loss: 0.11280300468206406\n",
      "iteration 4781, loss value 338.1185302734375, l2 loss: 0.112800233066082\n",
      "iteration 4782, loss value 338.1184997558594, l2 loss: 0.11279741674661636\n",
      "iteration 4783, loss value 338.1187438964844, l2 loss: 0.11279460042715073\n",
      "iteration 4784, loss value 338.1185302734375, l2 loss: 0.11279179900884628\n",
      "iteration 4785, loss value 338.1182556152344, l2 loss: 0.11278896033763885\n",
      "iteration 4786, loss value 338.1184387207031, l2 loss: 0.11278616636991501\n",
      "iteration 4787, loss value 338.1181335449219, l2 loss: 0.11278335005044937\n",
      "iteration 4788, loss value 338.11822509765625, l2 loss: 0.11278054863214493\n",
      "iteration 4789, loss value 338.1183776855469, l2 loss: 0.11277773231267929\n",
      "iteration 4790, loss value 338.1185302734375, l2 loss: 0.11277492344379425\n",
      "iteration 4791, loss value 338.11859130859375, l2 loss: 0.11277207732200623\n",
      "iteration 4792, loss value 338.1180725097656, l2 loss: 0.11276929080486298\n",
      "iteration 4793, loss value 338.1182861328125, l2 loss: 0.11276643723249435\n",
      "iteration 4794, loss value 338.11834716796875, l2 loss: 0.11276360601186752\n",
      "iteration 4795, loss value 338.1180419921875, l2 loss: 0.1127607449889183\n",
      "iteration 4796, loss value 338.1180725097656, l2 loss: 0.11275791376829147\n",
      "iteration 4797, loss value 338.1180419921875, l2 loss: 0.11275505274534225\n",
      "iteration 4798, loss value 338.1178894042969, l2 loss: 0.11275220662355423\n",
      "iteration 4799, loss value 338.1178894042969, l2 loss: 0.1127493679523468\n",
      "iteration 4800, loss value 338.1180419921875, l2 loss: 0.11274652928113937\n",
      "iteration 4801, loss value 338.11798095703125, l2 loss: 0.11274363845586777\n",
      "iteration 4802, loss value 338.11773681640625, l2 loss: 0.11274080723524094\n",
      "iteration 4803, loss value 338.11810302734375, l2 loss: 0.11273795366287231\n",
      "iteration 4804, loss value 338.1180114746094, l2 loss: 0.1127350926399231\n",
      "iteration 4805, loss value 338.117919921875, l2 loss: 0.11273223161697388\n",
      "iteration 4806, loss value 338.11810302734375, l2 loss: 0.11272936314344406\n",
      "iteration 4807, loss value 338.1177673339844, l2 loss: 0.11272652447223663\n",
      "iteration 4808, loss value 338.11767578125, l2 loss: 0.11272364109754562\n",
      "iteration 4809, loss value 338.1175537109375, l2 loss: 0.11272075027227402\n",
      "iteration 4810, loss value 338.1175537109375, l2 loss: 0.1127178743481636\n",
      "iteration 4811, loss value 338.11749267578125, l2 loss: 0.1127149686217308\n",
      "iteration 4812, loss value 338.11749267578125, l2 loss: 0.11271210759878159\n",
      "iteration 4813, loss value 338.1174621582031, l2 loss: 0.11270920187234879\n",
      "iteration 4814, loss value 338.11767578125, l2 loss: 0.11270628124475479\n",
      "iteration 4815, loss value 338.1172790527344, l2 loss: 0.11270340532064438\n",
      "iteration 4816, loss value 338.117431640625, l2 loss: 0.11270052939653397\n",
      "iteration 4817, loss value 338.1177673339844, l2 loss: 0.11269760876893997\n",
      "iteration 4818, loss value 338.1175231933594, l2 loss: 0.11269471049308777\n",
      "iteration 4819, loss value 338.1174621582031, l2 loss: 0.11269181221723557\n",
      "iteration 4820, loss value 338.1174621582031, l2 loss: 0.11268893629312515\n",
      "iteration 4821, loss value 338.11737060546875, l2 loss: 0.11268603056669235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4822, loss value 338.1173095703125, l2 loss: 0.11268311738967896\n",
      "iteration 4823, loss value 338.11712646484375, l2 loss: 0.11268020421266556\n",
      "iteration 4824, loss value 338.1173095703125, l2 loss: 0.11267729848623276\n",
      "iteration 4825, loss value 338.117431640625, l2 loss: 0.11267438530921936\n",
      "iteration 4826, loss value 338.11712646484375, l2 loss: 0.11267141997814178\n",
      "iteration 4827, loss value 338.1165771484375, l2 loss: 0.11266850680112839\n",
      "iteration 4828, loss value 338.1172790527344, l2 loss: 0.112665556371212\n",
      "iteration 4829, loss value 338.11669921875, l2 loss: 0.11266263574361801\n",
      "iteration 4830, loss value 338.1167907714844, l2 loss: 0.11265967786312103\n",
      "iteration 4831, loss value 338.1169128417969, l2 loss: 0.11265671998262405\n",
      "iteration 4832, loss value 338.1166076660156, l2 loss: 0.11265379935503006\n",
      "iteration 4833, loss value 338.1168212890625, l2 loss: 0.11265084892511368\n",
      "iteration 4834, loss value 338.11669921875, l2 loss: 0.1126478910446167\n",
      "iteration 4835, loss value 338.11663818359375, l2 loss: 0.11264494806528091\n",
      "iteration 4836, loss value 338.1165771484375, l2 loss: 0.11264201253652573\n",
      "iteration 4837, loss value 338.11700439453125, l2 loss: 0.11263906955718994\n",
      "iteration 4838, loss value 338.11700439453125, l2 loss: 0.11263611912727356\n",
      "iteration 4839, loss value 338.11627197265625, l2 loss: 0.11263320595026016\n",
      "iteration 4840, loss value 338.1168212890625, l2 loss: 0.11263023316860199\n",
      "iteration 4841, loss value 338.1169128417969, l2 loss: 0.1126272976398468\n",
      "iteration 4842, loss value 338.1163330078125, l2 loss: 0.11262435466051102\n",
      "iteration 4843, loss value 338.1168212890625, l2 loss: 0.11262141913175583\n",
      "iteration 4844, loss value 338.1164245605469, l2 loss: 0.11261843889951706\n",
      "iteration 4845, loss value 338.1163635253906, l2 loss: 0.11261550337076187\n",
      "iteration 4846, loss value 338.1167297363281, l2 loss: 0.11261256039142609\n",
      "iteration 4847, loss value 338.11663818359375, l2 loss: 0.11260958760976791\n",
      "iteration 4848, loss value 338.1165771484375, l2 loss: 0.11260664463043213\n",
      "iteration 4849, loss value 338.1165466308594, l2 loss: 0.11260369420051575\n",
      "iteration 4850, loss value 338.11676025390625, l2 loss: 0.11260071396827698\n",
      "iteration 4851, loss value 338.1163635253906, l2 loss: 0.1125977635383606\n",
      "iteration 4852, loss value 338.1161193847656, l2 loss: 0.11259476840496063\n",
      "iteration 4853, loss value 338.1164245605469, l2 loss: 0.11259178817272186\n",
      "iteration 4854, loss value 338.11627197265625, l2 loss: 0.11258882284164429\n",
      "iteration 4855, loss value 338.11639404296875, l2 loss: 0.11258582770824432\n",
      "iteration 4856, loss value 338.1162414550781, l2 loss: 0.11258284002542496\n",
      "iteration 4857, loss value 338.1163024902344, l2 loss: 0.11257985234260559\n",
      "iteration 4858, loss value 338.11676025390625, l2 loss: 0.11257684230804443\n",
      "iteration 4859, loss value 338.11627197265625, l2 loss: 0.11257384717464447\n",
      "iteration 4860, loss value 338.11627197265625, l2 loss: 0.1125708520412445\n",
      "iteration 4861, loss value 338.1158752441406, l2 loss: 0.11256784945726395\n",
      "iteration 4862, loss value 338.115966796875, l2 loss: 0.11256483942270279\n",
      "iteration 4863, loss value 338.1164245605469, l2 loss: 0.11256185919046402\n",
      "iteration 4864, loss value 338.1162109375, l2 loss: 0.11255886405706406\n",
      "iteration 4865, loss value 338.11627197265625, l2 loss: 0.1125558465719223\n",
      "iteration 4866, loss value 338.1155090332031, l2 loss: 0.11255283653736115\n",
      "iteration 4867, loss value 338.1159362792969, l2 loss: 0.11254981905221939\n",
      "iteration 4868, loss value 338.11566162109375, l2 loss: 0.11254683136940002\n",
      "iteration 4869, loss value 338.1160583496094, l2 loss: 0.11254380643367767\n",
      "iteration 4870, loss value 338.11572265625, l2 loss: 0.11254078149795532\n",
      "iteration 4871, loss value 338.1155700683594, l2 loss: 0.11253778636455536\n",
      "iteration 4872, loss value 338.1156311035156, l2 loss: 0.1125347688794136\n",
      "iteration 4873, loss value 338.11602783203125, l2 loss: 0.11253173649311066\n",
      "iteration 4874, loss value 338.1155700683594, l2 loss: 0.1125287339091301\n",
      "iteration 4875, loss value 338.1156311035156, l2 loss: 0.11252569407224655\n",
      "iteration 4876, loss value 338.1154479980469, l2 loss: 0.1125226765871048\n",
      "iteration 4877, loss value 338.1158142089844, l2 loss: 0.11251963675022125\n",
      "iteration 4878, loss value 338.1156921386719, l2 loss: 0.11251658946275711\n",
      "iteration 4879, loss value 338.1151123046875, l2 loss: 0.11251356452703476\n",
      "iteration 4880, loss value 338.115478515625, l2 loss: 0.11251050978899002\n",
      "iteration 4881, loss value 338.115478515625, l2 loss: 0.11250750720500946\n",
      "iteration 4882, loss value 338.11590576171875, l2 loss: 0.11250446736812592\n",
      "iteration 4883, loss value 338.1156005859375, l2 loss: 0.11250140517950058\n",
      "iteration 4884, loss value 338.1151123046875, l2 loss: 0.11249838769435883\n",
      "iteration 4885, loss value 338.11541748046875, l2 loss: 0.11249534040689468\n",
      "iteration 4886, loss value 338.1154479980469, l2 loss: 0.11249228566884995\n",
      "iteration 4887, loss value 338.1156005859375, l2 loss: 0.11248921602964401\n",
      "iteration 4888, loss value 338.11480712890625, l2 loss: 0.11248613893985748\n",
      "iteration 4889, loss value 338.11505126953125, l2 loss: 0.11248309910297394\n",
      "iteration 4890, loss value 338.11517333984375, l2 loss: 0.1124800443649292\n",
      "iteration 4891, loss value 338.11505126953125, l2 loss: 0.11247695982456207\n",
      "iteration 4892, loss value 338.1151428222656, l2 loss: 0.11247391253709793\n",
      "iteration 4893, loss value 338.115234375, l2 loss: 0.11247081309556961\n",
      "iteration 4894, loss value 338.1148376464844, l2 loss: 0.11246776580810547\n",
      "iteration 4895, loss value 338.1147155761719, l2 loss: 0.11246468871831894\n",
      "iteration 4896, loss value 338.1149597167969, l2 loss: 0.11246161162853241\n",
      "iteration 4897, loss value 338.1148986816406, l2 loss: 0.11245854943990707\n",
      "iteration 4898, loss value 338.11444091796875, l2 loss: 0.11245547235012054\n",
      "iteration 4899, loss value 338.1150817871094, l2 loss: 0.11245238035917282\n",
      "iteration 4900, loss value 338.1152038574219, l2 loss: 0.11244932562112808\n",
      "iteration 4901, loss value 338.11480712890625, l2 loss: 0.11244624853134155\n",
      "iteration 4902, loss value 338.1146240234375, l2 loss: 0.11244316399097443\n",
      "iteration 4903, loss value 338.1145935058594, l2 loss: 0.1124400794506073\n",
      "iteration 4904, loss value 338.1148986816406, l2 loss: 0.11243700236082077\n",
      "iteration 4905, loss value 338.11456298828125, l2 loss: 0.11243392527103424\n",
      "iteration 4906, loss value 338.11480712890625, l2 loss: 0.11243085563182831\n",
      "iteration 4907, loss value 338.11492919921875, l2 loss: 0.11242777854204178\n",
      "iteration 4908, loss value 338.1146240234375, l2 loss: 0.11242472380399704\n",
      "iteration 4909, loss value 338.1144104003906, l2 loss: 0.11242163181304932\n",
      "iteration 4910, loss value 338.1146240234375, l2 loss: 0.112418532371521\n",
      "iteration 4911, loss value 338.11444091796875, l2 loss: 0.11241544783115387\n",
      "iteration 4912, loss value 338.1145935058594, l2 loss: 0.11241237074136734\n",
      "iteration 4913, loss value 338.1147155761719, l2 loss: 0.11240924894809723\n",
      "iteration 4914, loss value 338.1147155761719, l2 loss: 0.11240614950656891\n",
      "iteration 4915, loss value 338.1147766113281, l2 loss: 0.1124030202627182\n",
      "iteration 4916, loss value 338.1144714355469, l2 loss: 0.11239989101886749\n",
      "iteration 4917, loss value 338.1142883300781, l2 loss: 0.11239678412675858\n",
      "iteration 4918, loss value 338.11419677734375, l2 loss: 0.11239362508058548\n",
      "iteration 4919, loss value 338.1143493652344, l2 loss: 0.11239053308963776\n",
      "iteration 4920, loss value 338.1145935058594, l2 loss: 0.11238739639520645\n",
      "iteration 4921, loss value 338.11444091796875, l2 loss: 0.11238427460193634\n",
      "iteration 4922, loss value 338.1145935058594, l2 loss: 0.11238114535808563\n",
      "iteration 4923, loss value 338.11456298828125, l2 loss: 0.11237802356481552\n",
      "iteration 4924, loss value 338.114501953125, l2 loss: 0.11237488687038422\n",
      "iteration 4925, loss value 338.1142272949219, l2 loss: 0.11237174272537231\n",
      "iteration 4926, loss value 338.1136779785156, l2 loss: 0.1123686209321022\n",
      "iteration 4927, loss value 338.1142272949219, l2 loss: 0.1123654767870903\n",
      "iteration 4928, loss value 338.1142272949219, l2 loss: 0.11236235499382019\n",
      "iteration 4929, loss value 338.1142272949219, l2 loss: 0.1123591959476471\n",
      "iteration 4930, loss value 338.1136779785156, l2 loss: 0.11235608160495758\n",
      "iteration 4931, loss value 338.1142272949219, l2 loss: 0.11235295236110687\n",
      "iteration 4932, loss value 338.1138916015625, l2 loss: 0.11234981566667557\n",
      "iteration 4933, loss value 338.1138610839844, l2 loss: 0.11234669387340546\n",
      "iteration 4934, loss value 338.11407470703125, l2 loss: 0.11234354227781296\n",
      "iteration 4935, loss value 338.1136474609375, l2 loss: 0.11234042048454285\n",
      "iteration 4936, loss value 338.11419677734375, l2 loss: 0.11233725398778915\n",
      "iteration 4937, loss value 338.1141662597656, l2 loss: 0.11233410239219666\n",
      "iteration 4938, loss value 338.1137390136719, l2 loss: 0.11233095079660416\n",
      "iteration 4939, loss value 338.11395263671875, l2 loss: 0.11232779175043106\n",
      "iteration 4940, loss value 338.1137390136719, l2 loss: 0.11232466995716095\n",
      "iteration 4941, loss value 338.114013671875, l2 loss: 0.11232149600982666\n",
      "iteration 4942, loss value 338.1134033203125, l2 loss: 0.11231834441423416\n",
      "iteration 4943, loss value 338.11395263671875, l2 loss: 0.11231517046689987\n",
      "iteration 4944, loss value 338.1132507324219, l2 loss: 0.11231203377246857\n",
      "iteration 4945, loss value 338.1136169433594, l2 loss: 0.11230885982513428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4946, loss value 338.1138000488281, l2 loss: 0.11230568587779999\n",
      "iteration 4947, loss value 338.11334228515625, l2 loss: 0.11230254173278809\n",
      "iteration 4948, loss value 338.1138916015625, l2 loss: 0.112299345433712\n",
      "iteration 4949, loss value 338.1134338378906, l2 loss: 0.11229617893695831\n",
      "iteration 4950, loss value 338.1133728027344, l2 loss: 0.11229298263788223\n",
      "iteration 4951, loss value 338.1132507324219, l2 loss: 0.11228981614112854\n",
      "iteration 4952, loss value 338.1136169433594, l2 loss: 0.11228661984205246\n",
      "iteration 4953, loss value 338.11334228515625, l2 loss: 0.11228342354297638\n",
      "iteration 4954, loss value 338.11309814453125, l2 loss: 0.1122802197933197\n",
      "iteration 4955, loss value 338.1132507324219, l2 loss: 0.11227701604366302\n",
      "iteration 4956, loss value 338.113037109375, l2 loss: 0.11227383464574814\n",
      "iteration 4957, loss value 338.1130065917969, l2 loss: 0.11227062344551086\n",
      "iteration 4958, loss value 338.1132507324219, l2 loss: 0.11226741969585419\n",
      "iteration 4959, loss value 338.113037109375, l2 loss: 0.11226420849561691\n",
      "iteration 4960, loss value 338.1130065917969, l2 loss: 0.11226101964712143\n",
      "iteration 4961, loss value 338.1133728027344, l2 loss: 0.11225780099630356\n",
      "iteration 4962, loss value 338.11328125, l2 loss: 0.11225458979606628\n",
      "iteration 4963, loss value 338.1126403808594, l2 loss: 0.1122514083981514\n",
      "iteration 4964, loss value 338.11297607421875, l2 loss: 0.11224818974733353\n",
      "iteration 4965, loss value 338.11322021484375, l2 loss: 0.11224497854709625\n",
      "iteration 4966, loss value 338.1127624511719, l2 loss: 0.11224177479743958\n",
      "iteration 4967, loss value 338.1133728027344, l2 loss: 0.11223853379487991\n",
      "iteration 4968, loss value 338.112548828125, l2 loss: 0.11223537474870682\n",
      "iteration 4969, loss value 338.1127624511719, l2 loss: 0.11223214119672775\n",
      "iteration 4970, loss value 338.1126403808594, l2 loss: 0.11222894489765167\n",
      "iteration 4971, loss value 338.1128845214844, l2 loss: 0.11222571134567261\n",
      "iteration 4972, loss value 338.11260986328125, l2 loss: 0.11222250759601593\n",
      "iteration 4973, loss value 338.1127014160156, l2 loss: 0.11221930384635925\n",
      "iteration 4974, loss value 338.11297607421875, l2 loss: 0.112216055393219\n",
      "iteration 4975, loss value 338.1123962402344, l2 loss: 0.11221287399530411\n",
      "iteration 4976, loss value 338.1130065917969, l2 loss: 0.11220963299274445\n",
      "iteration 4977, loss value 338.1123352050781, l2 loss: 0.11220639944076538\n",
      "iteration 4978, loss value 338.1127624511719, l2 loss: 0.1122031956911087\n",
      "iteration 4979, loss value 338.11285400390625, l2 loss: 0.11219993233680725\n",
      "iteration 4980, loss value 338.1119079589844, l2 loss: 0.11219672858715057\n",
      "iteration 4981, loss value 338.1125793457031, l2 loss: 0.11219348013401031\n",
      "iteration 4982, loss value 338.11285400390625, l2 loss: 0.11219022423028946\n",
      "iteration 4983, loss value 338.1121826171875, l2 loss: 0.1121869906783104\n",
      "iteration 4984, loss value 338.1123962402344, l2 loss: 0.11218375712633133\n",
      "iteration 4985, loss value 338.1121520996094, l2 loss: 0.11218049377202988\n",
      "iteration 4986, loss value 338.1121520996094, l2 loss: 0.11217724531888962\n",
      "iteration 4987, loss value 338.11212158203125, l2 loss: 0.11217400431632996\n",
      "iteration 4988, loss value 338.1123962402344, l2 loss: 0.1121707558631897\n",
      "iteration 4989, loss value 338.11199951171875, l2 loss: 0.11216753721237183\n",
      "iteration 4990, loss value 338.1120300292969, l2 loss: 0.11216426640748978\n",
      "iteration 4991, loss value 338.11224365234375, l2 loss: 0.11216100305318832\n",
      "iteration 4992, loss value 338.1119384765625, l2 loss: 0.11215777695178986\n",
      "iteration 4993, loss value 338.1121826171875, l2 loss: 0.11215449869632721\n",
      "iteration 4994, loss value 338.11175537109375, l2 loss: 0.11215128004550934\n",
      "iteration 4995, loss value 338.1123352050781, l2 loss: 0.1121479943394661\n",
      "iteration 4996, loss value 338.1120300292969, l2 loss: 0.11214473098516464\n",
      "iteration 4997, loss value 338.1119079589844, l2 loss: 0.11214147508144379\n",
      "iteration 4998, loss value 338.1120300292969, l2 loss: 0.11213819682598114\n",
      "iteration 4999, loss value 338.112060546875, l2 loss: 0.1121349185705185\n"
     ]
    }
   ],
   "source": [
    "# initialize a random array\n",
    "learned = learn_marginal_logits(mu_true, X, learn_with_w2=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "aec7f38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learned_mu_true = jax.nn.softmax(learned)\n",
    "plt.bar(X, learned_mu_true)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "a362ca3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(3285.7407, dtype=float32)"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize a random array\n",
    "key = jax.random.PRNGKey(42)\n",
    "key, subkey = jax.random.split(key)\n",
    "theta = jax.random.normal(key=subkey, shape=X.shape)\n",
    "X_reshaped = X.reshape(X.shape[0], 1).astype(float)\n",
    "\n",
    "logit_loss(theta, mu_true, X_reshaped)\n",
    "#learn_marginal_logits(mu_true)\n",
    "\n",
    "#jax.grad(logit_loss)(theta, mu_true, X=X_reshaped)\n",
    "\n",
    "#mu = gaussian_kernel(0, 1, X)\n",
    "#X_reshaped = X.reshape(X.shape[0], 1).astype(float)\n",
    "#jax.grad(ot_marginal_loss)(mu, mu_true, X_reshaped)\n",
    "#jax.grad(loss)(jnp.array(0.1), mu_true, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f69a3a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0, 0],\n",
       "       [0, 1],\n",
       "       [0, 2],\n",
       "       [0, 3],\n",
       "       [0, 4],\n",
       "       [1, 0],\n",
       "       [1, 1],\n",
       "       [1, 2],\n",
       "       [1, 3],\n",
       "       [1, 4],\n",
       "       [2, 0],\n",
       "       [2, 1],\n",
       "       [2, 2],\n",
       "       [2, 3],\n",
       "       [2, 4],\n",
       "       [3, 0],\n",
       "       [3, 1],\n",
       "       [3, 2],\n",
       "       [3, 3],\n",
       "       [3, 4]], dtype=int32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = jnp.arange(4)\n",
    "b = jnp.arange(5)\n",
    "X, Y = jnp.meshgrid(a, b, indexing=\"ij\")\n",
    "XY = jnp.stack([X, Y], axis=-1).reshape((20, 2))\n",
    "XY\n",
    "#ot_marginal_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
