{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "8482cd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax  # https://github.com/deepmind/optax\n",
    "import torch  # https://pytorch.org\n",
    "import torchvision  # https://pytorch.org\n",
    "from jaxtyping import Array, Float, Int, PyTree  # https://github.com/google/jaxtyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "1f94a302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 3e-4\n",
    "STEPS = 300\n",
    "PRINT_EVERY = 30\n",
    "SEED = 5678\n",
    "\n",
    "key = jax.random.PRNGKey(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "23e6cb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "normalise_data = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)),\n",
    "    ]\n",
    ")\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    \"CIFAR10\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=normalise_data,\n",
    ")\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    \"CIFAR10\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=normalise_data,\n",
    ")\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d7f18d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 3, 32, 32)\n",
      "(64,)\n",
      "[9 9 0 9 8 1 7 1 2 5 0 4 7 4 9 7 9 8 7 6 0 7 7 5 9 7 3 9 6 5 9 7 2 6 9 2 0\n",
      " 9 9 8 6 6 6 5 3 0 8 9 7 9 6 9 5 1 0 4 4 5 3 8 7 2 9 6]\n"
     ]
    }
   ],
   "source": [
    "# Checking our data a bit (by now, everyone knows what the MNIST dataset looks like)\n",
    "dummy_x, dummy_y = next(iter(trainloader))\n",
    "dummy_x = dummy_x.numpy()\n",
    "dummy_y = dummy_y.numpy()\n",
    "print(dummy_x.shape)  # 64x1x28x28\n",
    "print(dummy_y.shape)  # 64\n",
    "print(dummy_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "474021ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(eqx.Module):\n",
    "    layers: list\n",
    "\n",
    "    def __init__(self, key):\n",
    "        key1, key2, key3, key4 = jax.random.split(key, 4)\n",
    "        # Standard CNN setup: convolutional layer, followed by flattening,\n",
    "        # with a small MLP on top.\n",
    "        self.layers = [\n",
    "            jnp.ravel,\n",
    "            eqx.nn.Linear(3072, 1500, key=key1),\n",
    "            jax.nn.relu,\n",
    "            eqx.nn.Linear(1500, 200, key=key2),\n",
    "            jax.nn.relu,\n",
    "            eqx.nn.Linear(200, 10, key=key3),\n",
    "            jax.nn.softmax\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x: Float[Array, \"3 32 32\"]) -> Float[Array, \"10\"]:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "key, subkey = jax.random.split(key, 2)\n",
    "model = MLP(subkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "61779660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  layers=[\n",
      "    <wrapped function ravel>,\n",
      "    Linear(\n",
      "      weight=f32[1500,3072],\n",
      "      bias=f32[1500],\n",
      "      in_features=3072,\n",
      "      out_features=1500,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    <wrapped function relu>,\n",
      "    Linear(\n",
      "      weight=f32[200,1500],\n",
      "      bias=f32[200],\n",
      "      in_features=1500,\n",
      "      out_features=200,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    <wrapped function relu>,\n",
      "    Linear(\n",
      "      weight=f32[10,200],\n",
      "      bias=f32[10],\n",
      "      in_features=200,\n",
      "      out_features=10,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    <function softmax>\n",
      "  ]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "fc5bca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_jit\n",
    "def loss(\n",
    "    model: MLP, x: Float[Array, \"batch 3 32 32\"], y: Int[Array, \" batch\"], lam: Int[Array, \"\"]) -> Float[Array, \"\"]:\n",
    "    # Our input has the shape (BATCH_SIZE, 1, 28, 28), but our model operations on\n",
    "    # a single input input image of shape (1, 28, 28).\n",
    "    #\n",
    "    # Therefore, we have to use jax.vmap, which in this case maps our model over the\n",
    "    # leading (batch) axis.\n",
    "    pred_y = jax.vmap(model)(x)\n",
    "    reg = 0\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, eqx.nn.Linear):\n",
    "            reg += lam * (jnp.sum(layer.weight ** 2) + jnp.sum(layer.bias ** 2))\n",
    "    return cross_entropy(y, pred_y) + reg\n",
    "\n",
    "@eqx.filter_jit\n",
    "def cross_entropy(\n",
    "    y: Int[Array, \" batch\"], pred_y: Float[Array, \"batch 10\"]\n",
    ") -> Float[Array, \"\"]:\n",
    "    # y are the true targets, and should be integers 0-9.\n",
    "    # pred_y are the log-softmax'd predictions.\n",
    "    pred_y = jnp.take_along_axis(pred_y, jnp.expand_dims(y, 1), axis=1)\n",
    "    return -jnp.mean(pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "284372ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = eqx.filter_jit(loss)  # JIT our loss function from earlier!\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def compute_accuracy(\n",
    "    model: MLP, x: Float[Array, \"batch 3 32 32\"], y: Int[Array, \" batch\"]\n",
    ") -> Float[Array, \"\"]:\n",
    "    \"\"\"This function takes as input the current model\n",
    "    and computes the average accuracy on a batch.\n",
    "    \"\"\"\n",
    "    pred_y = jax.vmap(model)(x)\n",
    "    pred_y = jnp.argmax(pred_y, axis=1)\n",
    "    return jnp.mean(y == pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "1064a2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: MLP, testloader: torch.utils.data.DataLoader, lam: int):\n",
    "    \"\"\"This function evaluates the model on the test dataset,\n",
    "    computing both the average loss and the average accuracy.\n",
    "    \"\"\"\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    for x, y in testloader:\n",
    "        x = x.numpy()\n",
    "        y = y.numpy()\n",
    "        # Note that all the JAX operations happen inside `loss` and `compute_accuracy`,\n",
    "        # and both have JIT wrappers, so this is fast.\n",
    "        avg_loss += loss(model, x, y, lam)\n",
    "        avg_acc += compute_accuracy(model, x, y)\n",
    "    return avg_loss / len(testloader), avg_acc / len(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "df8b60a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = optax.adamw(LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "bb71996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: CNN,\n",
    "    lam: int,\n",
    "    trainloader: torch.utils.data.DataLoader,\n",
    "    optim: optax.GradientTransformation,\n",
    "    steps: int,\n",
    ") -> MLP:\n",
    "    # Just like earlier: It only makes sense to train the arrays in our model,\n",
    "    # so filter out everything else.\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "    # Always wrap everything -- computing gradients, running the optimiser, updating\n",
    "    # the model -- into a single JIT region. This ensures things run as fast as\n",
    "    # possible.\n",
    "    @eqx.filter_jit\n",
    "    def make_step(\n",
    "        model: MLP,\n",
    "        opt_state: PyTree,\n",
    "        x: Float[Array, \"batch 1 28 28\"],\n",
    "        y: Int[Array, \" batch\"],\n",
    "    ):\n",
    "        loss_value, grads = eqx.filter_value_and_grad(loss)(model, x, y, lam)\n",
    "        updates, opt_state = optim.update(\n",
    "            grads, opt_state, eqx.filter(model, eqx.is_array)\n",
    "        )\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return model, opt_state, loss_value\n",
    "\n",
    "    # Loop over our training dataset as many times as we need.\n",
    "    def infinite_trainloader():\n",
    "        while True:\n",
    "            yield from trainloader\n",
    "\n",
    "    for step, (x, y) in zip(range(steps), infinite_trainloader()):\n",
    "        # PyTorch dataloaders give PyTorch tensors by default,\n",
    "        # so convert them to NumPy arrays.\n",
    "        x = x.numpy()\n",
    "        y = y.numpy()\n",
    "        model, opt_state, train_loss = make_step(model, opt_state, x, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "89445d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda: 1e-06, test_acc: 0.35250794887542725 [trial [0] of 5]\n",
      "lambda: 1e-06, test_acc: 0.36634156107902527 [trial [1] of 5]\n",
      "lambda: 1e-06, test_acc: 0.35798168182373047 [trial [2] of 5]\n",
      "lambda: 1e-06, test_acc: 0.3364848792552948 [trial [3] of 5]\n",
      "lambda: 1e-06, test_acc: 0.35280653834342957 [trial [4] of 5]\n",
      "lambda: 1e-07, test_acc: 0.36514729261398315 [trial [0] of 5]\n",
      "lambda: 1e-07, test_acc: 0.3498208522796631 [trial [1] of 5]\n",
      "lambda: 1e-07, test_acc: 0.3543988764286041 [trial [2] of 5]\n",
      "lambda: 1e-07, test_acc: 0.34016719460487366 [trial [3] of 5]\n",
      "lambda: 1e-07, test_acc: 0.35061705112457275 [trial [4] of 5]\n",
      "lambda: 1e-08, test_acc: 0.33399680256843567 [trial [0] of 5]\n",
      "lambda: 1e-08, test_acc: 0.3418590724468231 [trial [1] of 5]\n",
      "lambda: 1e-08, test_acc: 0.36813294887542725 [trial [2] of 5]\n",
      "lambda: 1e-08, test_acc: 0.35867834091186523 [trial [3] of 5]\n",
      "lambda: 1e-08, test_acc: 0.3280254900455475 [trial [4] of 5]\n",
      "lambda: 1e-09, test_acc: 0.3460390269756317 [trial [0] of 5]\n",
      "lambda: 1e-09, test_acc: 0.36096736788749695 [trial [1] of 5]\n",
      "lambda: 1e-09, test_acc: 0.3645501732826233 [trial [2] of 5]\n",
      "lambda: 1e-09, test_acc: 0.35479697585105896 [trial [3] of 5]\n",
      "lambda: 1e-09, test_acc: 0.36425158381462097 [trial [4] of 5]\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for lam in [10**(-k) for k in range(6, 10)]:\n",
    "    results[lam] = []\n",
    "    for trial in range(5):\n",
    "        # create new model\n",
    "        key, subkey = jax.random.split(key, 2)\n",
    "        model = MLP(subkey)\n",
    "        \n",
    "        # train & evaluate model\n",
    "        model = train(model, lam, trainloader, optim, STEPS)\n",
    "        test_loss, test_acc = evaluate(model, testloader, lam)\n",
    "        results[lam].append(test_acc)\n",
    "        \n",
    "        print(f'lambda: {lam}, test_acc: {test_acc} [trial [{trial}] of 5]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b8a09a6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() takes 5 positional arguments but 6 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[175], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPRINT_EVERY\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: train() takes 5 positional arguments but 6 were given"
     ]
    }
   ],
   "source": [
    "model = train(model, trainloader, testloader, optim, STEPS, PRINT_EVERY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
